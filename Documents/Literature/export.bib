@article{Valle2020,
   abstract = {In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at https://github.com/NVIDIA/flowtron},
   author = {Rafael Valle and Kevin Shih and Ryan Prenger and Bryan Catanzaro},
   month = {5},
   title = {Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},
   url = {http://arxiv.org/abs/2005.05957},
   year = {2020},
}
@inproceedings{Novick2018,
   abstract = {For embodied conversational agents (ECAs) the relationship between gesture and rapport is an open question. To enable us to learn whether adding breathing behaviors to an agent similar to SimSensei would lead users interacting to perceive the agent as more natural, we built an application, called Paola Chat, in which the ECA could display naturalistic breathing animations. Our study had two phases. In the first phase, we determined the most natural amplitude for the agent’s breathing. In the second phase, we assessed the effect of breathing on the users’ perceptions of rapport and naturalness. The study had a within-subjects design, with breathing/not-breathing as the independent variable. Despite our expectation that increased naturalness from breathing would lead users to report greater rapport in the breathing condition than in the not-breathing condition, the study’s results suggest that the animation of breathing appears to neither increase nor decrease these perceptions.},
   author = {David Novick and Mahdokht Afravi and Adriana Camacho},
   doi = {10.1007/978-3-319-91581-4_26},
   isbn = {9783319915807},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Dialog system,Embodied conversational agents,Human-agent dialog},
   pages = {351-360},
   publisher = {Springer Verlag},
   title = {Paolachat: A virtual agent with naturalistic breathing},
   volume = {10909 LNCS},
   year = {2018},
}
@inproceedings{Berthold1999,
   abstract = {Users of computing devices are increasingly likely to be subject to situationally determined distractions that produce exceptionally high cognitive load. The question arises of how a system can automatically interpret symptoms of such cognitive load in the user’s behavior. This paper examines this question with respect to systems that process speech input. First, we synthesize results of previous experimental studies of the ways in which a speaker’s cognitive load is reflected in features of speech. Then we present a conceptualization of these relationships in terms of Bayesian networks. For two examples of such symptoms-sentence fragments and articulation rate-we present results concerning the distribution of the symptoms in realistic assistance dialogs. Finally. using artificial data generated in accordance with the preceding analyses. we examine the ability of a Bayesian network to assess a user’s cognitive load on the basis of limited observations involving these two symptoms.},
   author = {Andre Berthold and Anthony Jameson},
   doi = {10.1007/978-3-7091-2490-1_23},
   isbn = {9783211831519},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {235-244},
   publisher = {Springer Verlag},
   title = {Interpreting symptoms of cognitive load in speech input},
   volume = {407},
   year = {1999},
}
@report{Heim1968,
   author = {Edgar Heim and Peter H Knapp and Louis Vachon and Gordon G Globus and S Joseph Nemetz},
   journal = {Jourmti of Psychosomatic Reaearch},
   title = {Emotion, breathing and speech},
   volume = {12},
   year = {1968},
}
@inproceedings{Bernardet2017,
   abstract = {Human speech production requires the dynamic regulation of air through the vocal system. While virtual character systems commonly are capable of speech output, they rarely take breathing during speaking – speech breathing – into account. We believe that integrating dynamic speech breathing systems in virtual characters can significantly contribute to augmenting their realism. Here, we present a novel control architecture aimed at generating speech breathing in virtual characters. This architecture is informed by behavioral, linguistic and anatomical knowledge of human speech breathing. Based on textual input and controlled by a set of low- and high-level parameters, the system produces dynamic signals in real-time that control the virtual character’s anatomy (thorax, abdomen, head, nostrils, and mouth) and sound production (speech and breathing). The system is implemented in Python, offers a graphical user interface for easy parameter control, and simultaneously controls the visual and auditory aspects of speech breathing through the integration of the character animation system SmartBody [16] and the audio synthesis platform SuperCollider [12]. Beyond contributing to realism, the presented system allows for a flexible generation of a wide range of speech breathing behaviors that can convey information about the speaker such as mood, age, and health.},
   author = {Ulysses Bernardet and Sin hwa Kang and Andrew Feng and Steve DiPaola and Ari Shapiro},
   doi = {10.1007/978-3-319-67401-8_5},
   isbn = {9783319674001},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Animation,Breathing,Speaking,Speech breathing,Virtual character},
   pages = {43-52},
   publisher = {Springer Verlag},
   title = {A dynamic speech breathing system for virtual characters},
   volume = {10498 LNAI},
   year = {2017},
}
@article{Paiva2017,
   abstract = {This article surveys the area of computational empathy, analysing different ways by which artificial agents can simulate and trigger empathy in their interactions with humans. Empathic agents can be seen as agents that have the capacity to place themselves into the position of a user's or another agent's emotional situation and respond appropriately. We also survey artificial agents that, by their design and behaviour, can lead users to respond emotionally as if they were experiencing the agent's situation. In the course of this survey, we present the research conducted to date on empathic agents in light of the principles and mechanisms of empathy found in humans. We end by discussing some of the main challenges that this exciting area will be facing in the future. Copyright is held by the owner/author(s).},
   author = {Ana Paiva and Iolanda Leite and Hana Boukricha and Ipke Wachsmuth},
   doi = {10.1145/2912150},
   issn = {21606463},
   issue = {3},
   journal = {ACM Transactions on Interactive Intelligent Systems},
   keywords = {Affective computing,Empathy,Human-computer interaction,Human-robot interaction,Social robots,Virtual agents},
   month = {9},
   publisher = {Association for Computing Machinery},
   title = {Empathy in virtual agents and robots: A survey},
   volume = {7},
   year = {2017},
}
@inproceedings{Roes2022,
   author = {Rozemarijn Hannah Roes and Francisca Pessanha and Almila Akdag Salah},
   doi = {10.1145/3536220.3558803},
   isbn = {9781450393898},
   month = {11},
   pages = {70-78},
   publisher = {Association for Computing Machinery (ACM)},
   title = {An Emotional Respiration Speech Dataset},
   year = {2022},
}
@article{Le2023,
   abstract = {In recent years, speech synthesis systems have allowed for the production of very high-quality voices. Therefore, research in this domain is now turning to the problem of integrating emotions into speech. However, the method of constructing a speech synthesizer for each emotion has some limitations. First, this method often requires an emotional-speech data set with many sentences. Such data sets are very time-intensive and labor-intensive to complete. Second, training each of these models requires computers with large computational capabilities and a lot of effort and time for model tuning. In addition, each model for each emotion failed to take advantage of data sets of other emotions. In this paper, we propose a new method to synthesize emotional speech in which the latent expressions of emotions are learned from a small data set of professional actors through a Flowtron model. In addition, we provide a new method to build a speech corpus that is scalable and whose quality is easy to control. Next, to produce a high-quality speech synthesis model, we used this data set to train the Tacotron 2 model. We used it as a pre-trained model to train the Flowtron model. We applied this method to synthesize Vietnamese speech with sadness and happiness. Mean opinion score (MOS) assessment results show that MOS is 3.61 for sadness and 3.95 for happiness. In conclusion, the proposed method proves to be more effective for a high degree of automation and fast emotional sentence generation, using a small emotional-speech data set.},
   author = {Thanh X. Le and An T. Le and Quang H. Nguyen},
   doi = {10.32604/csse.2023.026234},
   issn = {02676192},
   issue = {2},
   journal = {Computer Systems Science and Engineering},
   keywords = {Emotional speech synthesis,flowtron,speech synthesis,style transfer,vietnamese speech},
   pages = {1263-1278},
   publisher = {Tech Science Press},
   title = {Emotional Vietnamese Speech Synthesis Using Style-Transfer Learning},
   volume = {44},
   year = {2023},
}
@article{Liu2021,
   abstract = {Emotional text-to-speech synthesis (ETTS) has seen much progress in recent years. However, the generated voice is often not perceptually identifiable by its intended emotion category. To address this problem, we propose a new interactive training paradigm for ETTS, denoted as i-ETTS, which seeks to directly improve the emotion discriminability by interacting with a speech emotion recognition (SER) model. Moreover, we formulate an iterative training strategy with reinforcement learning to ensure the quality of i-ETTS optimization. Experimental results demonstrate that the proposed i-ETTS outperforms the state-of-the-art baselines by rendering speech with more accurate emotion style. To our best knowledge, this is the first study of reinforcement learning in emotional text-to-speech synthesis.},
   author = {Rui Liu and Berrak Sisman and Haizhou Li},
   month = {4},
   title = {Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability},
   url = {http://arxiv.org/abs/2104.01408},
   year = {2021},
}
@article{Salah2021,
   abstract = {Many people experience a traumatic event during their lifetime. In some extraordinary situations, such as natural disasters, war, massacres, terrorism, or mass migration, the traumatic event is shared by a community and the effects go beyond those directly affected. Today, thanks to recorded interviews and testimonials, many archives and collections exist that are open to researchers of trauma studies, holocaust studies, and historians, among others. These archives act as vital testimonials for oral history, politics, and human rights. As such, they are usually either transcribed or meticulously indexed. In this work, we propose to look at the nonverbal signals emitted by victims of various traumatic events when they describe the trauma and we seek to render these for novel representations without taking into account the explicit verbal content. Our preliminary paralinguistic analysis on a manually annotated collection of testimonials from different archives, as well as on a corpus prepared for depression and post-traumatic stress disorder detection indicates a tentative connection between breathing and emotional states of speakers, which opens up new possibilities of exploring oral history archives.},
   author = {Almila Akdag Salah and Albert Ali Salah and Heysem Kaya and Metehan Doyran and Evrim Kavcar},
   doi = {10.1093/llc/fqaa056},
   issn = {2055-7671},
   issue = {Supplement_2},
   journal = {Digital Scholarship in the Humanities},
   month = {11},
   pages = {ii2-ii8},
   publisher = {Oxford University Press (OUP)},
   title = {The sound of silence: Breathing analysis for finding traces of trauma and depression in oral history archives},
   volume = {36},
   year = {2021},
}
@article{Livingstone2018,
   abstract = {The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/ zenodo.1188976.},
   author = {Steven R Livingstone and Frank A Russo},
   doi = {10.5281/zenodo.1188976},
   isbn = {1111111111},
   title = {The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
   url = {https://www.},
   year = {2018},
}
@article{Lancucki2020,
   abstract = {We present FastPitch, a fully-parallel text-to-speech model based on FastSpeech, conditioned on fundamental frequency contours. The model predicts pitch contours during inference. By altering these predictions, the generated speech can be more expressive, better match the semantic of the utterance, and in the end more engaging to the listener. Uniformly increasing or decreasing pitch with FastPitch generates speech that resembles the voluntary modulation of voice. Conditioning on frequency contours improves the overall quality of synthesized speech, making it comparable to state-of-the-art. It does not introduce an overhead, and FastPitch retains the favorable, fully-parallel Transformer architecture, with over 900x real-time factor for mel-spectrogram synthesis of a typical utterance.},
   author = {Adrian Łańcucki},
   month = {6},
   title = {FastPitch: Parallel Text-to-speech with Pitch Prediction},
   url = {http://arxiv.org/abs/2006.06873},
   year = {2020},
}
@book{Szekely2020,
   abstract = {Title from content provider.},
   author = {Éva Székely and Gustav Eje Henter and Jonas Beskow and Joakim Gustafson},
   isbn = {9781509066315},
   publisher = {IEEE},
   title = {Breathing and speech planning in spontaneous speech synthesis},
   year = {2020},
}
@article{Iannizzotto2018,
   author = {Giancarlo Iannizzotto and Lucia Lo Bello and Andrea Nucita and Giorgio Mario Grasso},
   doi = {10.1109/HSI.2018.8431232},
   isbn = {978-1-5386-5024-0},
   journal = {2018 11th International Conference on Human System Interaction (HSI)},
   month = {7},
   pages = {50-56},
   publisher = {IEEE},
   title = {A Vision and Speech Enabled, Customizable, Virtual Assistant for Smart Environments},
   url = {https://ieeexplore.ieee.org/document/8431232/},
   year = {2018},
}
@article{Urgen2018,
   abstract = {Uncanny valley refers to humans' negative reaction to almost-but-not-quite-human agents. Theoretical work proposes prediction violation as an explanation for uncanny valley but no empirical work has directly tested it. Here, we provide evidence that supports this theory using event-related brain potential recordings from the human scalp. Human subjects were presented images and videos of three agents as EEG was recorded: a real human, a mechanical robot, and a realistic robot in between. The real human and the mechanical robot had congruent appearance and motion whereas the realistic robot had incongruent appearance and motion. We hypothesize that the appearance of the agent would provide a context to predict her movement, and accordingly the perception of the realistic robot would elicit an N400 effect indicating the violation of predictions, whereas the human and the mechanical robot would not. Our data confirmed this hypothesis suggesting that uncanny valley could be explained by violation of one's predictions about human norms when encountered with realistic but artificial human forms. Importantly, our results implicate that the mechanisms underlying perception of other individuals in our environment are predictive in nature.},
   author = {Burcu A. Urgen and Marta Kutas and Ayse P. Saygin},
   doi = {10.1016/j.neuropsychologia.2018.04.027},
   issn = {18733514},
   journal = {Neuropsychologia},
   keywords = {Action perception,N400,Predictive processing,Social neuroscience,Uncanny valley},
   month = {6},
   pages = {181-185},
   pmid = {29704523},
   publisher = {Elsevier Ltd},
   title = {Uncanny valley as a window into predictive processing in the social brain},
   volume = {114},
   year = {2018},
}
@inproceedings{Weis2017,
   abstract = {In social robotics, the term Uncanny Valley describes the phenomenon that linear increases in human-likeness of an agent do not entail an equally linear increase in favorable reactions towards that agent. Instead, a pronounced dip or 'valley' at around 70% human-likeness emerges. One currently popular view to explain this drop in favorable reactions is delivered by the Categorical Perception Hypothesis. It is suggested that categorization of agents with mixed human and nonhuman features is associated with additional cognitive costs and that these costs are the cause of the Uncanny Valley. However, the nature of the cognitive costs is still matter of debate. The current study explores whether the cognitive costs associated with stimulus categorization around the Uncanny Valley could be due to cognitive conflict as evoked by simultaneous activation of two categories. Using the mouse tracking technique, we show that cognitive conflict indeed peaks around the Uncanny Valley region of human-likeness. Our findings lay the foundation for investigating the effects of cognitive conflict on positive affect towards agents of around 70% human-likeness, possibly leading to the unraveling of the origins of the Uncanny Valley.},
   author = {Patrick P. Weis and Eva Wiese},
   doi = {10.1177/1541931213601763},
   isbn = {9780945289531},
   issn = {10711813},
   journal = {Proceedings of the Human Factors and Ergonomics Society},
   pages = {1599-1603},
   publisher = {Human Factors an Ergonomics Society Inc.},
   title = {Cognitive conflict as possible origin of the uncanny valley},
   volume = {2017-October},
   year = {2017},
}
@inproceedings{Wang2019,
   abstract = {Prior work has shown that embodiment can benefit virtual agents, such as increasing rapport and conveying nonverbal information. However, it is unclear if users prefer an embodied to a speech-only agent for augmented reality (AR) headsets that are designed to assist users in completing real-world tasks. We conducted a study to examine users’ perceptions and behaviors when interacting with virtual agents in AR. We asked 24 adults to wear the Microsoft HoloLens and find objects in a hidden object game while interacting with an agent that would offer assistance. We presented participants with four different agents: voice-only, non-human, full-size embodied, and a miniature embodied agent. Overall, users preferred the miniature embodied agent due to the novelty of his size and reduced uncanniness as opposed to the larger agent. From our results, we draw conclusions about how agent representation matters and derive guidelines on designing agents for AR headsets.},
   author = {Isaac Wang and Jesse Smith and Jaime Ruiz},
   doi = {10.1145/3290605.3300511},
   isbn = {9781450359702},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {Augmented reality,Embodied conversational agents},
   month = {5},
   publisher = {Association for Computing Machinery},
   title = {Exploring virtual agents for augmented reality},
   year = {2019},
}
@report{Bailenson2005,
   abstract = {The current study examined how assessments of copresence in an immersive virtual environment are influenced by variations in how much an embodied agent resembles a human being in appearance and behavior. We measured the extent to which virtual representations were both perceived and treated as if they were human via self-report, behavioral, and cognitive dependent measures. Distinctive patterns of findings emerged with respect to the behavior and appearance of embodied agents depending on the definition and operationalization of copresence. Independent and interactive effects for appearance and behavior were found suggesting that assessing the impact of behavioral realism on copresence without taking into account the appearance of the embodied agent (and vice versa) can lead to misleading conclusions. Consistent with the results of previous research, copresence was lowest when there was a large mismatch between the appearance and behav-ioral realism of an embodied agent.},
   author = {Jeremy N Bailenson and Kim Swinth and Crystal Hoyt and Susan Persky and Alex Dimov and Jim Blascovich},
   issue = {4},
   journal = {Presence},
   pages = {379-393},
   title = {The Independent and Interactive Effects of Embodied-Agent Appearance and Behavior on Self-Report, Cognitive, and Behavioral Markers of Copresence in Immersive Virtual Environments},
   volume = {14},
   year = {2005},
}
@report{Haake2009,
   author = {Magnus Haake and Agneta Gulz},
   journal = {Article in International Journal of Artificial Intelligence},
   title = {A Look at the Roles of Look & Roles in Embodied Pedagogical Agents-A User Preference Perspective. LINEAGE View project Maximizing informativeness and minimizing neglect-the next step in feedback research View project},
   url = {https://www.researchgate.net/publication/220049803},
   year = {2009},
}
@inproceedings{Klausen2022,
   abstract = {A novel abstract non-humanoid soft robot with four pneumatically actuated chambers was developed with the aim to signal specific emotions by altering its shape, movements, and breathing rates. Through a user study we investigated how observers perceived the robot's emotional state at different breathing rates. An online questionnaire utilizing the Self-Assessment Manikin scale was used to evaluate pleasure, arousal, and dominance. Our findings show that a slow breathing rate between 7-12.5 breaths per minute corresponds to a high level of pleasure, whereas a high breathing rate of 40 breaths per minute corresponds to a high level of arousal. Participants' gender, in addition, influences the perception of pleasure and arousal at different breathing rates. The findings demonstrate the possibility of signalling emotions through breathing patterns with a non-humanoid soft robot.},
   author = {Troels Aske Klausen and Ulrich Farhadi and Evgenios Vlachos and Jonas Jorgensen},
   doi = {10.1109/RoboSoft54090.2022.9762140},
   isbn = {9781665408288},
   journal = {2022 IEEE 5th International Conference on Soft Robotics, RoboSoft 2022},
   pages = {194-200},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Signalling Emotions with a Breathing Soft Robot},
   year = {2022},
}
@inproceedings{Terzioglu2020,
   abstract = {In this paper, we investigate how collaborative robots, or cobots, typically composed of a robotic arm and a gripper carrying out manipulation tasks alongside human coworkers, can be enhanced with HRI capabilities by applying ideas and principles from character animation. To this end, we modified the appearance and behaviors of a cobot, with minimal impact on its functionality and performance, and studied the extent to which these modifications improved its communication with and perceptions by human collaborators. Specifically, we aimed to improve the Appeal of the robot by manipulating its physical appearance, posture, and gaze, creating an animal-like character with a head-on-neck morphology; to utilize Arcs by generating smooth trajectories for the robot arm; and to increase the lifelikeness of the robot through Secondary Action by adding breathing motions to the robot. In two user studies, we investigated the effects of these cues on collaborator perceptions of the robot. Findings from our first study showed breathing to have a positive effect on most measures of robot perception and reveal nuanced interactions among the other factors. Data from our second study showed that, using gaze cues alone, a robot arm can improve metrics such as likeability and perceived sociability.},
   author = {Yunus Terzioglu and Bilge Mutlu and Erol Sahin},
   doi = {10.1145/3319502.3374829},
   isbn = {9781450367462},
   issn = {21672148},
   journal = {ACM/IEEE International Conference on Human-Robot Interaction},
   keywords = {Animation principles,Character design,Collaborative robots,Human-robot collaboration,Robot motion,Social cues},
   month = {3},
   pages = {343-357},
   publisher = {IEEE Computer Society},
   title = {Designing social cues for collaborative robots: The role of gaze and breathing in human-robot collaboration},
   year = {2020},
}
@inproceedings{Szekely2019,
   abstract = {Using spontaneous conversational speech for TTS raises questions on how disfluencies such as filled pauses (FPs) should be approached. Detailed annotation of FPs in training data enables precise control at synthesis time; coarse or nonexistent FP annotation, when combined with stochastic attention-based neural TTS, leads to synthesisers that insert these phenomena into fluent prompts on their own accord. In this study we investigate, objectively and subjectively, the effects of FP annotation and the impact of relinquishing control over FPs in a Tacotron TTS system. The training corpus comprised 9 hours of singlespeaker breath groups extracted from a conversational podcast. Systems trained with no or location-only FP annotation were found to reproduce FP locations and types (uh/um) in a pattern broadly similar to that of the corpus. We also studied the effect of FPs on natural and synthetic speech rate and the interchangeability of FP types. Interestingly, subjective tests indicate that synthesiser-predicted FP types from location-only annotation often were preferred over specifying the ground-truth type. In contrast, a more precise annotation, allowing us to focus training on the most fluent parts of the corpus, improved rated naturalness when synthesising fluent speech.},
   author = {Éva Székely and Gustav Eje Henter and Jonas Beskow and Joakim Gustafson},
   doi = {10.21437/ssw.2019-44},
   month = {9},
   pages = {245-250},
   publisher = {International Speech Communication Association},
   title = {How to train your fillers: uh and um in spontaneous speech synthesis},
   year = {2019},
}
@report{Liu2022,
   abstract = {Fig. 1. Overview. BEAT is a large-scale, multi-modal mo-cap human gestures dataset with semantic, emotional annotations, diverse speakers and multiple languages. Abstract. Achieving realistic, vivid, and human-like synthesized conversational gestures conditioned on multi-modal data is still an unsolved problem, due to the lack of available datasets, models and standard evaluation metrics. To address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i) 76 hours, high-quality, multi-modal data captured from 30 speakers talking with eight different emotions and in four different languages, ii) 32 millions frame-level emotion and semantic relevance annotations. Our statistical analysis on BEAT demonstrates the correlation of conversational gestures with facial expressions, emotions, and semantics, in addition to the known correlation with audio , text, and speaker identity. Based on this observation, we propose a baseline model, Cascaded Motion Network (CaMN), which consists of above six modalities modeled in a cascaded architecture for gesture synthesis. To evaluate the diversity of synthesized gestures, we introduce a metric, Semantic Relevance Gesture Recall (SRGR). Qualitative and quantitative experiments demonstrate metrics' validness, ground truth data quality, and baseline's state-of-the-art performance. To the best of our knowledge, BEAT is the largest motion capture dataset for investigating human gestures, which may contribute to a number of different research fields including controllable gesture synthesis, cross-modality analysis, emotional gesture recognition. The data, code and model will be released for research. 2 H. Liu et al.},
   author = {Haiyang Liu and Zihao Zhu and Naoya Iwamoto and Yichen Peng and Zhengqing Li and You Zhou and Elif Bozkurt and Bo Zheng},
   title = {BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis},
   year = {2022},
}
@report{Pfeifer2009,
   abstract = {We describe the design and evaluation of an agent that uses the fillers um and uh in its speech. We describe an empirical study of human-human dialogue , analyzing gaze behavior during the production of fillers and use this data to develop a model of agent-based gaze behavior. We find that speakers are significantly more likely to gaze away from their dialogue partner while uttering fillers, especially if the filler occurs at the beginning of a speaking turn. This model is evaluated in a preliminary experiment. Results indicate mixed attitudes towards an agent that uses conversational fillers in its speech.},
   author = {Laura M Pfeifer and Timothy Bickmore},
   journal = {LNAI},
   keywords = {embodied conversational agent,filled pause,fillers,gaze},
   pages = {460-466},
   title = {Should Agents Speak Like, um, Humans? The Use of Conversational Fillers by Virtual Agents},
   volume = {5773},
   year = {2009},
}
@report{Cassell2004,
   abstract = {The Behavior Expression Animation Toolkit (BEAT) allows animators to input typed text that they wish to be spoken by an animated human figure, and to obtain as output appropriate and synchronized nonverbal behaviors and synthesized speech in a form that can be sent to a number of different animation systems. The nonverbal behaviors are assigned on the basis of actual linguistic and contextual analysis of the typed text, relying on rules derived from extensive research into human conversational behavior. The toolkit is extensible, so that new rules can be quickly added. It is designed to plug into larger systems that may also assign personality profiles, motion characteristics, scene constraints, or the animation styles of particular animators.},
   author = {Justine Cassell and Hannes Högni Vilhjálmsson and Timothy Bickmore},
   keywords = {Animation Systems,Facial Animation,Gesture,Speech Synthesis},
   title = {BEAT: the Behavior Expression Animation Toolkit},
   year = {2004},
}
@report{Bergmann2009,
   abstract = {Embodied conversational agents are required to be able to express themselves convincingly and autonomously. Based on an empirial study on spatial descriptions of landmarks in direction-giving, we present a model that allows virtual agents to automatically generate, i.e., select the content and derive the form of coordinated language and iconic gestures. Our model simulates the interplay between these two modes of expressiveness on two levels. First, two kinds of knowledge representation (propositional and imagistic) are utilized to capture the modality-specific contents and processes of content planning. Second, specific planners are integrated to carry out the formulation of concrete verbal and gestural behavior. A probabilistic approach to gesture formulation is presented that incorporates multiple contextual factors as well as idiosyncratic patterns in the mapping of visuo-spatial referent properties onto gesture morphology. Results from a prototype implementation are described.},
   author = {Kirsten Bergmann and Stefan Kopp},
   keywords = {D22 [Software Engineering]: Design Tools and Techniques-User Interfaces General Terms Design, Experimentation, Theory Keywords Gesture, language, expressiveness, multimodal output, em-bodied conversational agents,I20 [Artificial Intelligence]: General-Cognitive Simula-tion,I21 [Artificial Intelligence]: Applications and Ex-pert Systems-Natural Language Interfaces,I211 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelli-gent Agents},
   title = {Increasing the Expressiveness of Virtual Agents-Autonomous Generation of Speech and Gesture for Spatial Description Tasks},
   url = {www.ifaamas.org},
   year = {2009},
}
@report{Batliner2004,
   abstract = {This paper deals with databases that combine different aspects: children's speech, emotional speech, human-robot communication, cross-linguistics, and read vs. spontaneous speech: in a Wizard-of-Oz scenario, German and English children had to instruct Sony's AIBO robot to fulfil specific tasks. In one experimental condition, strictly parallel for German and English, the AIBO behaved 'disobedient' by following it's own script irrespective of the child's commands. By that, reactions of different children to the same sequence of AIBO's actions could be obtained. In addition, both the German and the English children were recorded reading texts. The data are transliterated orthographically; emotional user states and some other phenomena will be annotated. We report preliminary word recognition rates and classification results.},
   author = {A Batliner and C Hacker and S Steidl and E Nöth and S D'arcy and M Russell and M Wong},
   title = {"You stupid tin box"-children interacting with the AIBO robot: A cross-linguistic emotional speech corpus},
   url = {http://pfstar.itc.it/},
   year = {2004},
}
@report{Stiefelhagen2004,
   abstract = {Absfmcf-In this paper we present our ongoing work in building technologis for natural multimodal human-mbot interaction. We present our systems for spontmeous speech r d t i o n , multimodal dialogue processing and visual pcr-ception of a user, which Includes the recognition of pointing gestures as well as the recognition ora person's head orients-tion. Each of the components are described in the paper and experimental resultr are presented. In order to demonstrate and measure the usefulness of such technologies for human-robot interaction, all components have been integrated on B mobile mho1 platform and have been used for real-time human-robot interaction in a kitchen scenario. 1. ~NTRODUCTION bring certain objects or to obtain suggested recipes from the robot. The current components of our system include. a speech recognizer,. 3D face-and hand-tracking, pointing gesture recognition,. recognition of head pose,. a dialogue component, speech synthesis, .' a mobile platfom,. a stereo camera system, including pan-tilt, unit mnmred on the olatform. In the upcoming field of humanoid and buman-friendb robots, the ability of the robot for simple, unconstrained and natural interaction with its users is of central impor-lance [I], [21. The basis for appropriate action ofthe robot must be a comprehensive model of the Curtent SUrro~nding Figure 1.a) shows a picture of our system and a person interacting with it. Part of the risual tracking components have aheady heen integrated in A R M , 131, a humanoid robot with two arms and 23 degrees of freedom. This robot is depicted in Figure lb).-and in panicular of the humans involved in interaction. To facilitate natural interaction, robots should be able to perceive and understand all the modalities used by humans during face-to-face interaction. Besides speech, as the probably most prominent modality used by humans, these modalities also include pointing gestures, facial expressions , head pose, gaze, eye-contact and body language for example. In our rehearch labs 31 thc U I ~ I V & ~ I Kxlzruhe ITHJ and at Camegie hlellon Uruseriity. uc are de\clopinp technologies for the undcr4anding o l 1hr.w human ~nlcrartion mdalities. In particular in the frmeu,ork of 3 German research project on humanoid robots (Sonderforschungs-bereich Humanoide Roboter, SFB 588) we have been working using and improving such technologies to provide for natural interaction between a humanoid robot and its users. In this paper we present our work in this area. We have developed components for speech recognition, multi-modal dialogue processing, visual detection and modeling of users, including head pose estimation and pointing gesture recognition. All components have been integrated on a mobile robot platform and can he used for real-time multimodal interaction with a robot. The target scenario we addressed is a household situation , in which a human can ask the robot questions related to the kitchen (such as 'What's in the fridge ?"), ask the robot to set the table, to switch certain lights on or off, to a) b) Fig. I FIG.1 A) INTERACTION WITH OUR DEVELOPMENT SYSTEM.},
   author = {R Stiefelhagen and C Fugen and P Gieselmann and H Holzapfel and K Nickel and A Waibel},
   title = {Natural Human-Robot Interaction using Speech, Head Pose and Gestures},
   year = {2004},
}
