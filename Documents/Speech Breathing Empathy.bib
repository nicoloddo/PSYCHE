
@article{Valle2020,
	title = {Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},
	url = {http://arxiv.org/abs/2005.05957},
	abstract = {In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from {IAF} and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores ({MOS}) show that Flowtron matches state-of-the-art {TTS} models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at https://github.com/{NVIDIA}/flowtron},
	author = {Valle, Rafael and Shih, Kevin and Prenger, Ryan and Catanzaro, Bryan},
	date = {2020-05-12},
	eprinttype = {arxiv},
	eprint = {2005.05957},
	keywords = {★, synthesis},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\P6WNQVD7\\flowtron.pdf:application/pdf},
}

@inproceedings{novick_paolachat_2018,
	title = {Paolachat: A virtual agent with naturalistic breathing},
	volume = {10909 {LNCS}},
	isbn = {978-3-319-91580-7},
	doi = {10.1007/978-3-319-91581-4_26},
	abstract = {For embodied conversational agents ({ECAs}) the relationship between gesture and rapport is an open question. To enable us to learn whether adding breathing behaviors to an agent similar to {SimSensei} would lead users interacting to perceive the agent as more natural, we built an application, called Paola Chat, in which the {ECA} could display naturalistic breathing animations. Our study had two phases. In the first phase, we determined the most natural amplitude for the agent’s breathing. In the second phase, we assessed the effect of breathing on the users’ perceptions of rapport and naturalness. The study had a within-subjects design, with breathing/not-breathing as the independent variable. Despite our expectation that increased naturalness from breathing would lead users to report greater rapport in the breathing condition than in the not-breathing condition, the study’s results suggest that the animation of breathing appears to neither increase nor decrease these perceptions.},
	pages = {351--360},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	publisher = {Springer Verlag},
	author = {Novick, David and Afravi, Mahdokht and Camacho, Adriana},
	date = {2018},
	note = {{ISSN}: 16113349},
	keywords = {Dialog system, Embodied conversational agents, Human-agent dialog},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\BEUI6SP9\\PaolaChat.pdf:application/pdf},
}

@inproceedings{berthold_interpreting_1999,
	title = {Interpreting symptoms of cognitive load in speech input},
	volume = {407},
	isbn = {978-3-211-83151-9},
	doi = {10.1007/978-3-7091-2490-1_23},
	abstract = {Users of computing devices are increasingly likely to be subject to situationally determined distractions that produce exceptionally high cognitive load. The question arises of how a system can automatically interpret symptoms of such cognitive load in the user’s behavior. This paper examines this question with respect to systems that process speech input. First, we synthesize results of previous experimental studies of the ways in which a speaker’s cognitive load is reflected in features of speech. Then we present a conceptualization of these relationships in terms of Bayesian networks. For two examples of such symptoms-sentence fragments and articulation rate-we present results concerning the distribution of the symptoms in realistic assistance dialogs. Finally. using artificial data generated in accordance with the preceding analyses. we examine the ability of a Bayesian network to assess a user’s cognitive load on the basis of limited observations involving these two symptoms.},
	pages = {235--244},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	publisher = {Springer Verlag},
	author = {Berthold, Andre and Jameson, Anthony},
	date = {1999},
	note = {{ISSN}: 16113349},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\4USE2X75\\cognitive_load_speech.pdf:application/pdf},
}

@report{heim_emotion_1968,
	title = {Emotion, breathing and speech},
	author = {Heim, Edgar and Knapp, Peter H and Vachon, Louis and Globus, Gordon G and Nemetz, S Joseph},
	date = {1968},
	note = {Publication Title: Jourmti of Psychosomatic Reaearch
Volume: 12},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\M887QJRH\\emotion_breathing_speech.pdf:application/pdf},
}

@inproceedings{bernardet_dynamic_2017,
	title = {A dynamic speech breathing system for virtual characters},
	volume = {10498 {LNAI}},
	isbn = {978-3-319-67400-1},
	doi = {10.1007/978-3-319-67401-8_5},
	abstract = {Human speech production requires the dynamic regulation of air through the vocal system. While virtual character systems commonly are capable of speech output, they rarely take breathing during speaking – speech breathing – into account. We believe that integrating dynamic speech breathing systems in virtual characters can significantly contribute to augmenting their realism. Here, we present a novel control architecture aimed at generating speech breathing in virtual characters. This architecture is informed by behavioral, linguistic and anatomical knowledge of human speech breathing. Based on textual input and controlled by a set of low- and high-level parameters, the system produces dynamic signals in real-time that control the virtual character’s anatomy (thorax, abdomen, head, nostrils, and mouth) and sound production (speech and breathing). The system is implemented in Python, offers a graphical user interface for easy parameter control, and simultaneously controls the visual and auditory aspects of speech breathing through the integration of the character animation system {SmartBody} [16] and the audio synthesis platform {SuperCollider} [12]. Beyond contributing to realism, the presented system allows for a flexible generation of a wide range of speech breathing behaviors that can convey information about the speaker such as mood, age, and health.},
	pages = {43--52},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	publisher = {Springer Verlag},
	author = {Bernardet, Ulysses and Kang, Sin hwa and Feng, Andrew and {DiPaola}, Steve and Shapiro, Ari},
	date = {2017},
	note = {{ISSN}: 16113349},
	keywords = {synthesis, Animation, Breathing, Speaking, Speech breathing, Virtual character},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\YQ4GBL8T\\A_Dynamic_Speech_Breathing_System_for_Virtual_Char(1).pdf:application/pdf},
}

@thesis{kroes_empathizing_2022,
	title = {Empathizing with virtual agents: the effect of personification and general empathic tendencies},
	type = {phdthesis},
	author = {Kroes, Kim and Saccardi, Isabellla and Masthoff, Judith},
	date = {2022},
	keywords = {empathy, thesis},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\YK94HNN6\\Empathizing with virtual agents.pdf:application/pdf},
}

@article{paiva_empathy_2017,
	title = {Empathy in virtual agents and robots: A survey},
	volume = {7},
	issn = {21606463},
	doi = {10.1145/2912150},
	abstract = {This article surveys the area of computational empathy, analysing different ways by which artificial agents can simulate and trigger empathy in their interactions with humans. Empathic agents can be seen as agents that have the capacity to place themselves into the position of a user's or another agent's emotional situation and respond appropriately. We also survey artificial agents that, by their design and behaviour, can lead users to respond emotionally as if they were experiencing the agent's situation. In the course of this survey, we present the research conducted to date on empathic agents in light of the principles and mechanisms of empathy found in humans. We end by discussing some of the main challenges that this exciting area will be facing in the future. Copyright is held by the owner/author(s).},
	number = {3},
	journaltitle = {{ACM} Transactions on Interactive Intelligent Systems},
	author = {Paiva, Ana and Leite, Iolanda and Boukricha, Hana and Wachsmuth, Ipke},
	date = {2017-09-01},
	note = {Publisher: Association for Computing Machinery},
	keywords = {★, Affective computing, Empathy, Human-computer interaction, Human-robot interaction, Social robots, Virtual agents},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\IE9MJCHX\\empathy_in_agents.pdf:application/pdf},
}

@inproceedings{roes_emotional_2022,
	title = {An Emotional Respiration Speech Dataset},
	isbn = {978-1-4503-9389-8},
	doi = {10.1145/3536220.3558803},
	pages = {70--78},
	publisher = {Association for Computing Machinery ({ACM})},
	author = {Roes, Rozemarijn Hannah and Pessanha, Francisca and Akdag Salah, Almila},
	date = {2022-11-07},
	keywords = {★},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\GPNYJ8CY\\Revised_GENEA_Emotional_Respiration_Dataset(2).pdf:application/pdf},
}

@thesis{wiersema_perception_2022,
	title = {Perception study: The difference in lighting perception on overall mood in rendered video compared to Virtual Reality environment},
	type = {phdthesis},
	author = {Wiersema, L J},
	date = {2022},
	keywords = {thesis},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\GFRD99MI\\Thesis_LauraWiersema.pdf:application/pdf},
}

@thesis{francois_blom_automated_2022,
	title = {Automated detection of depression},
	type = {phdthesis},
	author = {{François Blom}},
	date = {2022},
	keywords = {thesis},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\QW9HQVAR\\MSc_Thesis__Fran_ois___Phase_2__Thesis_Report(1).pdf:application/pdf},
}

@article{le_emotional_2023,
	title = {Emotional Vietnamese Speech Synthesis Using Style-Transfer Learning},
	volume = {44},
	issn = {02676192},
	doi = {10.32604/csse.2023.026234},
	abstract = {In recent years, speech synthesis systems have allowed for the production of very high-quality voices. Therefore, research in this domain is now turning to the problem of integrating emotions into speech. However, the method of constructing a speech synthesizer for each emotion has some limitations. First, this method often requires an emotional-speech data set with many sentences. Such data sets are very time-intensive and labor-intensive to complete. Second, training each of these models requires computers with large computational capabilities and a lot of effort and time for model tuning. In addition, each model for each emotion failed to take advantage of data sets of other emotions. In this paper, we propose a new method to synthesize emotional speech in which the latent expressions of emotions are learned from a small data set of professional actors through a Flowtron model. In addition, we provide a new method to build a speech corpus that is scalable and whose quality is easy to control. Next, to produce a high-quality speech synthesis model, we used this data set to train the Tacotron 2 model. We used it as a pre-trained model to train the Flowtron model. We applied this method to synthesize Vietnamese speech with sadness and happiness. Mean opinion score ({MOS}) assessment results show that {MOS} is 3.61 for sadness and 3.95 for happiness. In conclusion, the proposed method proves to be more effective for a high degree of automation and fast emotional sentence generation, using a small emotional-speech data set.},
	pages = {1263--1278},
	number = {2},
	journaltitle = {Computer Systems Science and Engineering},
	author = {Le, Thanh X. and Le, An T. and Nguyen, Quang H.},
	date = {2023},
	note = {Publisher: Tech Science Press},
	keywords = {Emotional speech synthesis, flowtron, speech synthesis, style transfer, vietnamese speech},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\XNL5TDI9\\vietnamese_flowtron.pdf:application/pdf},
}

@article{liu_reinforcement_2021,
	title = {Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability},
	url = {http://arxiv.org/abs/2104.01408},
	abstract = {Emotional text-to-speech synthesis ({ETTS}) has seen much progress in recent years. However, the generated voice is often not perceptually identifiable by its intended emotion category. To address this problem, we propose a new interactive training paradigm for {ETTS}, denoted as i-{ETTS}, which seeks to directly improve the emotion discriminability by interacting with a speech emotion recognition ({SER}) model. Moreover, we formulate an iterative training strategy with reinforcement learning to ensure the quality of i-{ETTS} optimization. Experimental results demonstrate that the proposed i-{ETTS} outperforms the state-of-the-art baselines by rendering speech with more accurate emotion style. To our best knowledge, this is the first study of reinforcement learning in emotional text-to-speech synthesis.},
	author = {Liu, Rui and Sisman, Berrak and Li, Haizhou},
	date = {2021-04-03},
	eprinttype = {arxiv},
	eprint = {2104.01408},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\TRGX8W5L\\RL_speech_synth.pdf:application/pdf},
}

@article{salah_sound_2021,
	title = {The sound of silence: Breathing analysis for finding traces of trauma and depression in oral history archives},
	volume = {36},
	issn = {2055-7671},
	doi = {10.1093/llc/fqaa056},
	abstract = {Many people experience a traumatic event during their lifetime. In some extraordinary situations, such as natural disasters, war, massacres, terrorism, or mass migration, the traumatic event is shared by a community and the effects go beyond those directly affected. Today, thanks to recorded interviews and testimonials, many archives and collections exist that are open to researchers of trauma studies, holocaust studies, and historians, among others. These archives act as vital testimonials for oral history, politics, and human rights. As such, they are usually either transcribed or meticulously indexed. In this work, we propose to look at the nonverbal signals emitted by victims of various traumatic events when they describe the trauma and we seek to render these for novel representations without taking into account the explicit verbal content. Our preliminary paralinguistic analysis on a manually annotated collection of testimonials from different archives, as well as on a corpus prepared for depression and post-traumatic stress disorder detection indicates a tentative connection between breathing and emotional states of speakers, which opens up new possibilities of exploring oral history archives.},
	pages = {ii2--ii8},
	issue = {Supplement\_2},
	journaltitle = {Digital Scholarship in the Humanities},
	author = {Salah, Almila Akdag and Salah, Albert Ali and Kaya, Heysem and Doyran, Metehan and Kavcar, Evrim},
	date = {2021-11-05},
	note = {Publisher: Oxford University Press ({OUP})},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\G66X3MAU\\the_sound_of_silence.pdf:application/pdf},
}

@article{livingstone_ryerson_2018,
	title = {The Ryerson Audio-Visual Database of Emotional Speech and Song ({RAVDESS}): A dynamic, multimodal set of facial and vocal expressions in North American English},
	url = {https://www.},
	doi = {10.5281/zenodo.1188976},
	abstract = {The {RAVDESS} is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/ zenodo.1188976.},
	author = {Livingstone, Steven R and Russo, Frank A},
	urldate = {2022-12-30},
	date = {2018},
	note = {{ISBN}: 1111111111},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\H8EJDY3N\\full-text.pdf:application/pdf},
}

@article{Lancucki2020,
	title = {{FastPitch}: Parallel Text-to-speech with Pitch Prediction},
	url = {http://arxiv.org/abs/2006.06873},
	abstract = {We present {FastPitch}, a fully-parallel text-to-speech model based on {FastSpeech}, conditioned on fundamental frequency contours. The model predicts pitch contours during inference. By altering these predictions, the generated speech can be more expressive, better match the semantic of the utterance, and in the end more engaging to the listener. Uniformly increasing or decreasing pitch with {FastPitch} generates speech that resembles the voluntary modulation of voice. Conditioning on frequency contours improves the overall quality of synthesized speech, making it comparable to state-of-the-art. It does not introduce an overhead, and {FastPitch} retains the favorable, fully-parallel Transformer architecture, with over 900x real-time factor for mel-spectrogram synthesis of a typical utterance.},
	author = {Łańcucki, Adrian},
	date = {2020-06-11},
	eprinttype = {arxiv},
	eprint = {2006.06873},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\8ZSNDBD5\\fastpitch.pdf:application/pdf},
}

@incollection{Szekely2020,
	title = {Breathing and speech planning in spontaneous speech synthesis},
	isbn = {978-1-5090-6631-5},
	abstract = {Breathing and speech planning in spontaneous speech are coordinated processes, often exhibiting disfluent patterns. While synthetic speech is not subject to respiratory needs, integrating breath into synthesis has advantages for naturalness and recall. At the same time, a synthetic voice reproducing disfluent breathing patterns learned from the data can be problematic. To address this, we first propose training stochastic {TTS} on a corpus of overlapping breath-group bigrams, to take context into account. Next, we introduce an unsupervised automatic annotation of likely-disfluent breath events, through a product-of-experts model that combines the output of two breathevent predictors, each using complementary information and operating in opposite directions. This annotation enables creating an automatically-breathing spontaneous speech synthesiser with a more fluent breathing style. A subjective evaluation on two spoken genres (impromptu and rehearsed) found the proposed system to be preferred over the baseline approach treating all breath events the same.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {{Éva Székely} and {Gustav Eje Henter} and {Jonas Beskow} and {Joakim Gustafson}},
	date = {2020},
	keywords = {★},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\BMJ9P3WU\\stochastic_breath_synthesis.pdf:application/pdf},
}

@article{iannizzotto_vision_2018,
	title = {A Vision and Speech Enabled, Customizable, Virtual Assistant for Smart Environments},
	url = {https://ieeexplore.ieee.org/document/8431232/},
	doi = {10.1109/HSI.2018.8431232},
	pages = {50--56},
	journaltitle = {2018 11th International Conference on Human System Interaction ({HSI})},
	author = {Iannizzotto, Giancarlo and Bello, Lucia Lo and Nucita, Andrea and Grasso, Giorgio Mario},
	urldate = {2023-01-13},
	date = {2018-07},
	note = {Publisher: {IEEE}
{ISBN}: 978-1-5386-5024-0},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\D5TRUHYV\\non_human_VA.pdf:application/pdf},
}

@article{urgen_uncanny_2018,
	title = {Uncanny valley as a window into predictive processing in the social brain},
	volume = {114},
	issn = {18733514},
	doi = {10.1016/j.neuropsychologia.2018.04.027},
	abstract = {Uncanny valley refers to humans' negative reaction to almost-but-not-quite-human agents. Theoretical work proposes prediction violation as an explanation for uncanny valley but no empirical work has directly tested it. Here, we provide evidence that supports this theory using event-related brain potential recordings from the human scalp. Human subjects were presented images and videos of three agents as {EEG} was recorded: a real human, a mechanical robot, and a realistic robot in between. The real human and the mechanical robot had congruent appearance and motion whereas the realistic robot had incongruent appearance and motion. We hypothesize that the appearance of the agent would provide a context to predict her movement, and accordingly the perception of the realistic robot would elicit an N400 effect indicating the violation of predictions, whereas the human and the mechanical robot would not. Our data confirmed this hypothesis suggesting that uncanny valley could be explained by violation of one's predictions about human norms when encountered with realistic but artificial human forms. Importantly, our results implicate that the mechanisms underlying perception of other individuals in our environment are predictive in nature.},
	pages = {181--185},
	journaltitle = {Neuropsychologia},
	author = {Urgen, Burcu A. and Kutas, Marta and Saygin, Ayse P.},
	date = {2018-06-01},
	pmid = {29704523},
	note = {Publisher: Elsevier Ltd},
	keywords = {Action perception, N400, Predictive processing, Social neuroscience, Uncanny valley},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\YPWSGKCH\\urgen_uncanny_valley.pdf:application/pdf},
}

@inproceedings{weis_cognitive_2017,
	title = {Cognitive conflict as possible origin of the uncanny valley},
	volume = {2017-October},
	isbn = {978-0-945289-53-1},
	doi = {10.1177/1541931213601763},
	abstract = {In social robotics, the term Uncanny Valley describes the phenomenon that linear increases in human-likeness of an agent do not entail an equally linear increase in favorable reactions towards that agent. Instead, a pronounced dip or 'valley' at around 70\% human-likeness emerges. One currently popular view to explain this drop in favorable reactions is delivered by the Categorical Perception Hypothesis. It is suggested that categorization of agents with mixed human and nonhuman features is associated with additional cognitive costs and that these costs are the cause of the Uncanny Valley. However, the nature of the cognitive costs is still matter of debate. The current study explores whether the cognitive costs associated with stimulus categorization around the Uncanny Valley could be due to cognitive conflict as evoked by simultaneous activation of two categories. Using the mouse tracking technique, we show that cognitive conflict indeed peaks around the Uncanny Valley region of human-likeness. Our findings lay the foundation for investigating the effects of cognitive conflict on positive affect towards agents of around 70\% human-likeness, possibly leading to the unraveling of the origins of the Uncanny Valley.},
	pages = {1599--1603},
	booktitle = {Proceedings of the Human Factors and Ergonomics Society},
	publisher = {Human Factors an Ergonomics Society Inc.},
	author = {Weis, Patrick P. and Wiese, Eva},
	date = {2017},
	note = {{ISSN}: 10711813},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\CYTJG4DR\\uncanny_valley.pdf:application/pdf},
}

@inproceedings{wang_exploring_2019,
	title = {Exploring virtual agents for augmented reality},
	isbn = {978-1-4503-5970-2},
	doi = {10.1145/3290605.3300511},
	abstract = {Prior work has shown that embodiment can benefit virtual agents, such as increasing rapport and conveying nonverbal information. However, it is unclear if users prefer an embodied to a speech-only agent for augmented reality ({AR}) headsets that are designed to assist users in completing real-world tasks. We conducted a study to examine users’ perceptions and behaviors when interacting with virtual agents in {AR}. We asked 24 adults to wear the Microsoft {HoloLens} and find objects in a hidden object game while interacting with an agent that would offer assistance. We presented participants with four different agents: voice-only, non-human, full-size embodied, and a miniature embodied agent. Overall, users preferred the miniature embodied agent due to the novelty of his size and reduced uncanniness as opposed to the larger agent. From our results, we draw conclusions about how agent representation matters and derive guidelines on designing agents for {AR} headsets.},
	booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
	publisher = {Association for Computing Machinery},
	author = {Wang, Isaac and Smith, Jesse and Ruiz, Jaime},
	date = {2019-05-02},
	keywords = {Embodied conversational agents, Augmented reality},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\8ETTXCTI\\AR_virtual_agents_comparison.pdf:application/pdf},
}

@report{bailenson_independent_2005,
	title = {The Independent and Interactive Effects of Embodied-Agent Appearance and Behavior on Self-Report, Cognitive, and Behavioral Markers of Copresence in Immersive Virtual Environments},
	abstract = {The current study examined how assessments of copresence in an immersive virtual environment are influenced by variations in how much an embodied agent resembles a human being in appearance and behavior. We measured the extent to which virtual representations were both perceived and treated as if they were human via self-report, behavioral, and cognitive dependent measures. Distinctive patterns of findings emerged with respect to the behavior and appearance of embodied agents depending on the definition and operationalization of copresence. Independent and interactive effects for appearance and behavior were found suggesting that assessing the impact of behavioral realism on copresence without taking into account the appearance of the embodied agent (and vice versa) can lead to misleading conclusions. Consistent with the results of previous research, copresence was lowest when there was a large mismatch between the appearance and behav-ioral realism of an embodied agent.},
	pages = {379--393},
	author = {Bailenson, Jeremy N and Swinth, Kim and Hoyt, Crystal and Persky, Susan and Dimov, Alex and Blascovich, Jim},
	date = {2005},
	note = {Publication Title: Presence
Volume: 14
Issue: 4},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\K8W3BU52\\bailenson2005.pdf:application/pdf},
}

@report{haake_look_2009,
	title = {A Look at the Roles of Look \& Roles in Embodied Pedagogical Agents-A User Preference Perspective. {LINEAGE} View project Maximizing informativeness and minimizing neglect-the next step in feedback research View project},
	url = {https://www.researchgate.net/publication/220049803},
	author = {Haake, Magnus and Gulz, Agneta},
	date = {2009},
	note = {Publication Title: Article in International Journal of Artificial Intelligence},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\ALFMNXU4\\IJAIED707Haakev7.pdf:application/pdf},
}

@inproceedings{klausen_signalling_2022,
	title = {Signalling Emotions with a Breathing Soft Robot},
	isbn = {978-1-66540-828-8},
	doi = {10.1109/RoboSoft54090.2022.9762140},
	abstract = {A novel abstract non-humanoid soft robot with four pneumatically actuated chambers was developed with the aim to signal specific emotions by altering its shape, movements, and breathing rates. Through a user study we investigated how observers perceived the robot's emotional state at different breathing rates. An online questionnaire utilizing the Self-Assessment Manikin scale was used to evaluate pleasure, arousal, and dominance. Our findings show that a slow breathing rate between 7-12.5 breaths per minute corresponds to a high level of pleasure, whereas a high breathing rate of 40 breaths per minute corresponds to a high level of arousal. Participants' gender, in addition, influences the perception of pleasure and arousal at different breathing rates. The findings demonstrate the possibility of signalling emotions through breathing patterns with a non-humanoid soft robot.},
	pages = {194--200},
	booktitle = {2022 {IEEE} 5th International Conference on Soft Robotics, {RoboSoft} 2022},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Klausen, Troels Aske and Farhadi, Ulrich and Vlachos, Evgenios and Jorgensen, Jonas},
	date = {2022},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\8D3P2QHH\\Signalling_Emotions_with_a_Breathing_Soft_Robot_correct_version_.pdf:application/pdf},
}

@inproceedings{terzioglu_designing_2020,
	title = {Designing social cues for collaborative robots: The role of gaze and breathing in human-robot collaboration},
	isbn = {978-1-4503-6746-2},
	doi = {10.1145/3319502.3374829},
	abstract = {In this paper, we investigate how collaborative robots, or cobots, typically composed of a robotic arm and a gripper carrying out manipulation tasks alongside human coworkers, can be enhanced with {HRI} capabilities by applying ideas and principles from character animation. To this end, we modified the appearance and behaviors of a cobot, with minimal impact on its functionality and performance, and studied the extent to which these modifications improved its communication with and perceptions by human collaborators. Specifically, we aimed to improve the Appeal of the robot by manipulating its physical appearance, posture, and gaze, creating an animal-like character with a head-on-neck morphology; to utilize Arcs by generating smooth trajectories for the robot arm; and to increase the lifelikeness of the robot through Secondary Action by adding breathing motions to the robot. In two user studies, we investigated the effects of these cues on collaborator perceptions of the robot. Findings from our first study showed breathing to have a positive effect on most measures of robot perception and reveal nuanced interactions among the other factors. Data from our second study showed that, using gaze cues alone, a robot arm can improve metrics such as likeability and perceived sociability.},
	pages = {343--357},
	booktitle = {{ACM}/{IEEE} International Conference on Human-Robot Interaction},
	publisher = {{IEEE} Computer Society},
	author = {Terzioglu, Yunus and Mutlu, Bilge and Sahin, Erol},
	date = {2020-03-09},
	note = {{ISSN}: 21672148},
	keywords = {Animation principles, Character design, Collaborative robots, Human-robot collaboration, Robot motion, Social cues},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\4EL7AVYV\\breathing_robot_human_collaboration.pdf:application/pdf},
}

@inproceedings{Szekely2019,
	title = {How to train your fillers: uh and um in spontaneous speech synthesis},
	doi = {10.21437/ssw.2019-44},
	abstract = {Using spontaneous conversational speech for {TTS} raises questions on how disfluencies such as filled pauses ({FPs}) should be approached. Detailed annotation of {FPs} in training data enables precise control at synthesis time; coarse or nonexistent {FP} annotation, when combined with stochastic attention-based neural {TTS}, leads to synthesisers that insert these phenomena into fluent prompts on their own accord. In this study we investigate, objectively and subjectively, the effects of {FP} annotation and the impact of relinquishing control over {FPs} in a Tacotron {TTS} system. The training corpus comprised 9 hours of singlespeaker breath groups extracted from a conversational podcast. Systems trained with no or location-only {FP} annotation were found to reproduce {FP} locations and types (uh/um) in a pattern broadly similar to that of the corpus. We also studied the effect of {FPs} on natural and synthetic speech rate and the interchangeability of {FP} types. Interestingly, subjective tests indicate that synthesiser-predicted {FP} types from location-only annotation often were preferred over specifying the ground-truth type. In contrast, a more precise annotation, allowing us to focus training on the most fluent parts of the corpus, improved rated naturalness when synthesising fluent speech.},
	pages = {245--250},
	publisher = {International Speech Communication Association},
	author = {Székely, Éva and Eje Henter, Gustav and Beskow, Jonas and Gustafson, Joakim},
	date = {2019-09-14},
	keywords = {★},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\BCTKFAS6\\how_to_train_your_fillers.pdf:application/pdf},
}

@report{liu_beat_2022,
	title = {{BEAT}: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis},
	abstract = {Fig. 1. Overview. {BEAT} is a large-scale, multi-modal mo-cap human gestures dataset with semantic, emotional annotations, diverse speakers and multiple languages. Abstract. Achieving realistic, vivid, and human-like synthesized conversational gestures conditioned on multi-modal data is still an unsolved problem, due to the lack of available datasets, models and standard evaluation metrics. To address this, we build Body-Expression-Audio-Text dataset, {BEAT}, which has i) 76 hours, high-quality, multi-modal data captured from 30 speakers talking with eight different emotions and in four different languages, ii) 32 millions frame-level emotion and semantic relevance annotations. Our statistical analysis on {BEAT} demonstrates the correlation of conversational gestures with facial expressions, emotions, and semantics, in addition to the known correlation with audio , text, and speaker identity. Based on this observation, we propose a baseline model, Cascaded Motion Network ({CaMN}), which consists of above six modalities modeled in a cascaded architecture for gesture synthesis. To evaluate the diversity of synthesized gestures, we introduce a metric, Semantic Relevance Gesture Recall ({SRGR}). Qualitative and quantitative experiments demonstrate metrics' validness, ground truth data quality, and baseline's state-of-the-art performance. To the best of our knowledge, {BEAT} is the largest motion capture dataset for investigating human gestures, which may contribute to a number of different research fields including controllable gesture synthesis, cross-modality analysis, emotional gesture recognition. The data, code and model will be released for research. 2 H. Liu et al.},
	author = {Liu, Haiyang and Zhu, Zihao and Iwamoto, Naoya and Peng, Yichen and Li, Zhengqing and Zhou, You and Bozkurt, Elif and Zheng, Bo},
	date = {2022},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\HFCUJIC8\\BEAT_A_Large-Scale_Semantic_and_Emotional_Multi-Mo.pdf:application/pdf},
}

@report{pfeifer_should_2009,
	title = {Should Agents Speak Like, um, Humans? The Use of Conversational Fillers by Virtual Agents},
	abstract = {We describe the design and evaluation of an agent that uses the fillers um and uh in its speech. We describe an empirical study of human-human dialogue , analyzing gaze behavior during the production of fillers and use this data to develop a model of agent-based gaze behavior. We find that speakers are significantly more likely to gaze away from their dialogue partner while uttering fillers, especially if the filler occurs at the beginning of a speaking turn. This model is evaluated in a preliminary experiment. Results indicate mixed attitudes towards an agent that uses conversational fillers in its speech.},
	pages = {460--466},
	author = {Pfeifer, Laura M and Bickmore, Timothy},
	date = {2009},
	note = {Publication Title: {LNAI}
Volume: 5773},
	keywords = {embodied conversational agent, filled pause, fillers, gaze},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\5ZWIVAMS\\should_agents_speak_with_fillers.pdf:application/pdf},
}

@report{cassell_beat_2004,
	title = {{BEAT}: the Behavior Expression Animation Toolkit},
	abstract = {The Behavior Expression Animation Toolkit ({BEAT}) allows animators to input typed text that they wish to be spoken by an animated human figure, and to obtain as output appropriate and synchronized nonverbal behaviors and synthesized speech in a form that can be sent to a number of different animation systems. The nonverbal behaviors are assigned on the basis of actual linguistic and contextual analysis of the typed text, relying on rules derived from extensive research into human conversational behavior. The toolkit is extensible, so that new rules can be quickly added. It is designed to plug into larger systems that may also assign personality profiles, motion characteristics, scene constraints, or the animation styles of particular animators.},
	author = {Cassell, Justine and Vilhjálmsson, Hannes Högni and Bickmore, Timothy},
	date = {2004},
	keywords = {Animation Systems, Facial Animation, Gesture, Speech Synthesis},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\HPNXAK4P\\beat-behavorial-animation-toolkit.pdf:application/pdf},
}

@report{bergmann_increasing_2009,
	title = {Increasing the Expressiveness of Virtual Agents-Autonomous Generation of Speech and Gesture for Spatial Description Tasks},
	url = {www.ifaamas.org},
	abstract = {Embodied conversational agents are required to be able to express themselves convincingly and autonomously. Based on an empirial study on spatial descriptions of landmarks in direction-giving, we present a model that allows virtual agents to automatically generate, i.e., select the content and derive the form of coordinated language and iconic gestures. Our model simulates the interplay between these two modes of expressiveness on two levels. First, two kinds of knowledge representation (propositional and imagistic) are utilized to capture the modality-specific contents and processes of content planning. Second, specific planners are integrated to carry out the formulation of concrete verbal and gestural behavior. A probabilistic approach to gesture formulation is presented that incorporates multiple contextual factors as well as idiosyncratic patterns in the mapping of visuo-spatial referent properties onto gesture morphology. Results from a prototype implementation are described.},
	author = {Bergmann, Kirsten and Kopp, Stefan},
	date = {2009},
	keywords = {D22 [Software Engineering]: Design Tools and Techniques-User Interfaces General Terms Design, Experimentation, Theory Keywords Gesture, language, expressiveness, multimodal output, em-bodied conversational agents, I20 [Artificial Intelligence]: General-Cognitive Simula-tion, I21 [Artificial Intelligence]: Applications and Ex-pert Systems-Natural Language Interfaces, I211 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelli-gent Agents},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\P7RUMPIS\\Increasing_the_expressiveness_of_virtual.pdf:application/pdf},
}

@report{batliner_you_2004,
	title = {"You stupid tin box"-children interacting with the {AIBO} robot: A cross-linguistic emotional speech corpus},
	url = {http://pfstar.itc.it/},
	abstract = {This paper deals with databases that combine different aspects: children's speech, emotional speech, human-robot communication, cross-linguistics, and read vs. spontaneous speech: in a Wizard-of-Oz scenario, German and English children had to instruct Sony's {AIBO} robot to fulfil specific tasks. In one experimental condition, strictly parallel for German and English, the {AIBO} behaved 'disobedient' by following it's own script irrespective of the child's commands. By that, reactions of different children to the same sequence of {AIBO}'s actions could be obtained. In addition, both the German and the English children were recorded reading texts. The data are transliterated orthographically; emotional user states and some other phenomena will be annotated. We report preliminary word recognition rates and classification results.},
	author = {Batliner, A and Hacker, C and Steidl, S and Nöth, E and D'arcy, S and Russell, M and Wong, M},
	date = {2004},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\I8HBCAMY\\Batliner04-YST.pdf:application/pdf},
}

@report{stiefelhagen_natural_2004,
	title = {Natural Human-Robot Interaction using Speech, Head Pose and Gestures},
	abstract = {Absfmcf-In this paper we present our ongoing work in building technologis for natural multimodal human-mbot interaction. We present our systems for spontmeous speech r d t i o n , multimodal dialogue processing and visual pcr-ception of a user, which Includes the recognition of pointing gestures as well as the recognition ora person's head orients-tion. Each of the components are described in the paper and experimental resultr are presented. In order to demonstrate and measure the usefulness of such technologies for human-robot interaction, all components have been integrated on B mobile mho1 platform and have been used for real-time human-robot interaction in a kitchen scenario. 1. {\textasciitilde}{NTRODUCTION} bring certain objects or to obtain suggested recipes from the robot. The current components of our system include. a speech recognizer,. 3D face-and hand-tracking, pointing gesture recognition,. recognition of head pose,. a dialogue component, speech synthesis, .' a mobile platfom,. a stereo camera system, including pan-tilt, unit mnmred on the olatform. In the upcoming field of humanoid and buman-friendb robots, the ability of the robot for simple, unconstrained and natural interaction with its users is of central impor-lance [I], [21. The basis for appropriate action ofthe robot must be a comprehensive model of the Curtent {SUrro}{\textasciitilde}nding Figure 1.a) shows a picture of our system and a person interacting with it. Part of the risual tracking components have aheady heen integrated in A R M , 131, a humanoid robot with two arms and 23 degrees of freedom. This robot is depicted in Figure lb).-and in panicular of the humans involved in interaction. To facilitate natural interaction, robots should be able to perceive and understand all the modalities used by humans during face-to-face interaction. Besides speech, as the probably most prominent modality used by humans, these modalities also include pointing gestures, facial expressions , head pose, gaze, eye-contact and body language for example. In our rehearch labs 31 thc U I {\textasciitilde} I V \& {\textasciitilde} I Kxlzruhe {ITHJ} and at Camegie hlellon Uruseriity. uc are de{\textbackslash}clopinp technologies for the undcr4anding o l 1hr.w human {\textasciitilde}nlcrartion mdalities. In particular in the frmeu,ork of 3 German research project on humanoid robots (Sonderforschungs-bereich Humanoide Roboter, {SFB} 588) we have been working using and improving such technologies to provide for natural interaction between a humanoid robot and its users. In this paper we present our work in this area. We have developed components for speech recognition, multi-modal dialogue processing, visual detection and modeling of users, including head pose estimation and pointing gesture recognition. All components have been integrated on a mobile robot platform and can he used for real-time multimodal interaction with a robot. The target scenario we addressed is a household situation , in which a human can ask the robot questions related to the kitchen (such as 'What's in the fridge ?"), ask the robot to set the table, to switch certain lights on or off, to a) b) Fig. I {FIG}.1 A) {INTERACTION} {WITH} {OUR} {DEVELOPMENT} {SYSTEM}.},
	author = {Stiefelhagen, R and Fugen, C and Gieselmann, P and Holzapfel, H and Nickel, K and Waibel, A},
	date = {2004},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\Z7KZJSIX\\iros.2004.1389771.pdf:application/pdf},
}

@report{bernardet_speech_2019,
	title = {Speech Breathing in Virtual Humans: An Interactive Model and Empirical Study},
	abstract = {Figure 1: A model for simulating speech breathing. Breaths are added to synthetic speech based on lung capacity, speaking loudness, mouth or nose breathing and other factors. {ABSTRACT} Human speech production requires the dynamic regulation of air through the vocal system. While virtual character systems commonly are capable of speech output, they rarely take breathing during speaking-speech breathing-into account. We believe that integrating dynamic speech breathing systems in virtual characters can significantly contribute to augmenting their realism. Here, we present a novel control architecture aimed at generating speech breathing in virtual characters. This architecture is informed by behavioral, linguistic and anatomical knowledge of human speech breathing. Based on textual input and controlled by a set of low-and high-level parameters, the system produces dynamic signals in real-time that control the virtual character's anatomy (thorax, abdomen , head, nostrils, and mouth) and sound production (speech and breathing). In addition, we perform a study to determine the effects of including breathing-motivated speech movements, such as head tilts and chest expansions during dialogue on a virtual character, as well as breathing sounds. This study includes speech that is generated both from a text-to-speech engine as well as from recorded voice.},
	author = {Bernardet, Ulysses and Feng, Andrew and Dipaola, Steve and Shapiro, Ari},
	date = {2019},
	keywords = {Index Terms, Human-centered computing, Virtual reality, Procedural animation},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\DDENUGSA\\bernardet2019.pdf:application/pdf},
}

@article{yan_adaspeech_2021,
	title = {{AdaSpeech} 3: Adaptive Text to Speech for Spontaneous Style},
	url = {http://arxiv.org/abs/2107.02530},
	abstract = {While recent text to speech ({TTS}) models perform very well in synthesizing reading-style (e.g., audiobook) speech, it is still challenging to synthesize spontaneous-style speech (e.g., podcast or conversation), mainly because of two reasons: 1) the lack of training data for spontaneous speech; 2) the difficulty in modeling the filled pauses (um and uh) and diverse rhythms in spontaneous speech. In this paper, we develop {AdaSpeech} 3, an adaptive {TTS} system that fine-tunes a well-trained reading-style {TTS} model for spontaneous-style speech. Specifically, 1) to insert filled pauses ({FP}) in the text sequence appropriately, we introduce an {FP} predictor to the {TTS} model; 2) to model the varying rhythms, we introduce a duration predictor based on mixture of experts ({MoE}), which contains three experts responsible for the generation of fast, medium and slow speech respectively, and fine-tune it as well as the pitch predictor for rhythm adaptation; 3) to adapt to other speaker timbre, we fine-tune some parameters in the decoder with few speech data. To address the challenge of lack of training data, we mine a spontaneous speech dataset to support our research this work and facilitate future research on spontaneous {TTS}. Experiments show that {AdaSpeech} 3 synthesizes speech with natural {FP} and rhythms in spontaneous styles, and achieves much better {MOS} and {SMOS} scores than previous adaptive {TTS} systems.},
	author = {Yan, Yuzi and Tan, Xu and Li, Bohan and Zhang, Guangyan and Qin, Tao and Zhao, Sheng and Shen, Yuan and Zhang, Wei-Qiang and Liu, Tie-Yan},
	date = {2021-07-06},
	eprinttype = {arxiv},
	eprint = {2107.02530},
	keywords = {★},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\67FUBFJ7\\AdaSpeech_3_Adaptive_Text_to_Speech_for_Spontaneou.pdf:application/pdf},
}

@inproceedings{kirkland_wheres_2022,
	title = {Where's the uh, hesitation? The interplay between filled pause location, speech rate and fundamental frequency in perception of confidence},
	volume = {2022-September},
	doi = {10.21437/Interspeech.2022-10973},
	abstract = {Much of the research investigating the perception of speaker certainty has relied on either attempting to elicit prosodic features in read speech, or artificial manipulation of recorded audio. Our novel method of controlling prosody in synthesized spontaneous speech provides a powerful tool for studying speech perception and can provide better insight into the interacting effects of prosodic features on perception while also paving the way for conversational systems which are more effectively able to engage in and respond to social behaviors. Here we have used this method to examine the combined impact of filled pause location, speech rate and f0 on the perception of speaker confidence. We found an additive effect of all three features. The most confident-sounding utterances had no filler, low f0 and high speech rate, while the least confident-sounding utterances had a medial filled pause, high f0 and low speech rate. Insertion of filled pauses had the strongest influence, but pitch and speaking rate could be used to more finely control the uncertainty cues in spontaneous speech synthesis.},
	pages = {4990--4994},
	booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, {INTERSPEECH}},
	publisher = {International Speech Communication Association},
	author = {Kirkland, Ambika and Lameris, Harm and Székely, Éva and Gustafson, Joakim},
	date = {2022},
	note = {{ISSN}: 19909772},
	keywords = {speech synthesis, expressive speech synthesis, paralinguistics, speech perception},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\WQUFFBJM\\kirkland22_interspeech.pdf:application/pdf},
}

@article{drolet_authenticity_2012,
	title = {Authenticity affects the recognition of emotions in speech: Behavioral and {fMRI} evidence},
	volume = {12},
	issn = {15307026},
	doi = {10.3758/s13415-011-0069-3},
	abstract = {The aim of the present study was to determine how authenticity of emotion expression in speech modulates activity in the neuronal substrates involved in emotion recognition. Within an {fMRI} paradigm, participants judged either the authenticity (authentic or play acted) or emotional content (anger, fear, joy, or sadness) of recordings of spontaneous emotions and reenactments by professional actors. When contrasting between task types, active judgment of authenticity, more than active judgment of emotion, indicated potential involvement of the theory of mind ({ToM}) network (medial prefrontal cortex, temporoparietal cortex, retrosplenium) as well as areas involved in working memory and decision making ({BA} 47). Subsequently, trials with authentic recordings were contrasted with those of reenactments to determine the modulatory effects of authenticity. Authentic recordings were found to enhance activity in part of the {ToM} network (medial prefrontal cortex). This effect of authenticity suggests that individuals integrate recollections of their own experiences more for judgments involving authentic stimuli than for those involving play-acted stimuli. The behavioral and functional results show that authenticity of emotional prosody is an important property influencing human responses to such stimuli, with implications for studies using play-acted emotions. © The Author(s) 2011.},
	pages = {140--150},
	number = {1},
	journaltitle = {Cognitive, Affective and Behavioral Neuroscience},
	author = {Drolet, Matthis and Schubotz, Ricarda I. and Fischer, Julia},
	date = {2012-03},
	pmid = {22038706},
	keywords = {Affect, Episodic memory, Intonation, Prosodym mentalizing, Theory of mind},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\5K5CYKM2\\s13415-011-0069-3.pdf:application/pdf},
}

@inproceedings{kirkland_perception_2021,
	title = {Perception of smiling voice in spontaneous speech synthesis},
	doi = {10.21437/ssw.2021-19},
	pages = {108--112},
	publisher = {International Speech Communication Association},
	author = {Kirkland, Ambika and Włodarczak, Marcin and Gustafson, Joakim and Szekely, Eva},
	date = {2021-08-24},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\IG4NNNKX\\kirkland21_ssw.pdf:application/pdf},
}

@article{moullec_multi-sensory_2022,
	title = {Multi-sensory display of self-avatar's physiological state: virtual breathing and heart beating can increase sensation of effort in {VR}},
	volume = {2022},
	issn = {1077-2626},
	url = {https://www.ieee.org/publications/rights/index.html},
	doi = {10.1109/TVCG.2022.3203120ï},
	abstract = {sensory display of self-avatar's physiological state: virtual breathing and heart beating can increase sensation of effort in {VR}. elanie {CognéCogn}´Cogné and Anatole L ´ ecuyer Fig. 1. We propose a novel approach to increase the connection with a self-avatar in virtual reality (A), by displaying its physiological state and physical exertion. It is based on a multi-sensory setup (B) involving visual, auditory and haptic displays. It includes visual effects such as a periphery overlay (C) simulating heart beating ; or haptic stimulation delivered with a piezoelectric actuator (D) and a novel compression belt (E) which exerts pressure on the abdomen to simulate a virtual breathing. Abstract-In this paper we explore the multi-sensory display of self-avatars' physiological state in Virtual Reality ({VR}), as a means to enhance the connection between the users and their avatar. Our approach consists in designing and combining a coherent set of visual, auditory and haptic cues to represent the avatar's cardiac and respiratory activity. These sensory cues are modulated depending on the avatar's simulated physical exertion. We notably introduce a novel haptic technique to represent respiratory activity using a compression belt simulating abdominal movements that occur during a breathing cycle. A series of experiments was conducted to evaluate the influence of our multi-sensory rendering techniques on various aspects of the {VR} user experience, including the sense of virtual embodiment and the sensation of effort during a walking simulation. A first study (N=30) that focused on displaying cardiac activity showed that combining sensory modalities significantly enhances the sensation of effort. A second study (N=20) that focused on respiratory activity showed that combining sensory modalities significantly enhances the sensation of effort as well as two sub-components of the sense of embodiment. Interestingly, the user's actual breathing tended to synchronize with the simulated breathing, especially with the multi-sensory and haptic displays. A third study (N=18) that focused on the combination of cardiac and respiratory activity showed that combining both rendering techniques significantly enhances the sensation of effort. Taken together, our results promote the use of our novel breathing display technique and multi-sensory rendering of physiological parameters in {VR} applications where effort sensations are prominent, such as for rehabilitation, sport training, or exergames.},
	pages = {3596},
	number = {11},
	author = {Moullec, Yann and Saint-Aubert, Justine and Manson, Julien and Cogne, Melanie and Lécuyer, Anatole and Lécuyer Multi-, Anatole},
	date = {2022},
	keywords = {Avatar, cardiac, effort sensation, embodiment, haptic, multi-sensory display, physiological computing, respiration},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\U9HHY49U\\Multi-sensory_display_of_self-avatars_physiological_state_virtual_breathing_and_heart_beating_can_increase_sensation_of_effort_in_VR-3.pdf:application/pdf},
}

@report{guo_research_2011,
	title = {Research on Lhasa Tibetan Prosodic Model of Journalese Based on Respiratory Signal},
	abstract = {In accordance with the actual development for Tibetan speech synthesis, the paper has taken news text as training corpora, analyzed the speech and prosodic features of Tibetan Lhasa dialect and confirmed the respiratory signal parameters with prosodic features. It has confirmed 6 classes of 39 dimensions context feature parameters in terms of previous prosodic structure analysis results. It uses {RBF} neural network to establish prosodic model and output 10 dimensions prosodic control parameters, and testing to know the predictable nature of the established model.},
	author = {Guo, Shuni and Gao, Lu and Yu, Hongzhi},
	date = {2011},
	keywords = {Prosodic model, {RBF}, respiratory signal, Tibetan Lhasa dialect},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\UPJ659TU\\iasp.2011.6108991.pdf:application/pdf},
}

@inproceedings{szekely_casting_2019,
	title = {Casting to Corpus: Segmenting and Selecting Spontaneous Dialogue for Tts with a Cnn-lstm Speaker-dependent Breath Detector},
	volume = {2019-May},
	isbn = {978-1-4799-8131-1},
	doi = {10.1109/ICASSP.2019.8683846},
	abstract = {This paper considers utilising breaths to create improved spontaneous-speech corpora for conversational text-to-speech from found audio recordings such as dialogue podcasts. Breaths are of interest since they relate to prosody and speech planning and are independent of language and transcription. Specifically, we propose a semi-supervised approach where a fraction of coarsely annotated data is used to train a convolutional and recurrent speaker-specific breath detector operating on spectrograms and zero-crossing rate. The classifier output is used to find target-speaker breath groups (audio segments delineated by breaths) and subsequently select those that constitute clean utterances appropriate for a synthesis corpus. An application to 11 hours of raw podcast audio extracts 1969 utterances (106 minutes), 87\% of which are clean and correctly segmented. This outperforms a baseline that performs integrated {VAD} and speaker attribution without accounting for breaths.},
	pages = {6925--6929},
	booktitle = {{ICASSP}, {IEEE} International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Székely, Éva and Henter, Gustav Eje and Gustafson, Joakim},
	date = {2019-05-01},
	note = {{ISSN}: 15206149},
	keywords = {Speech Synthesis, Spontaneous speech, Computational Paralinguistics, Breath detection, Found Data},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\RXXV5A6X\\szekely2019casting.pdf:application/pdf},
}

@report{szekely_spontaneous_2019,
	title = {Spontaneous conversational speech synthesis from found data},
	url = {https://prolific.ac/},
	abstract = {Synthesising spontaneous speech is a difficult task due to disflu-encies, high variability and syntactic conventions different from those of written language. Using found data, as opposed to lab-recorded conversations, for speech synthesis adds to these challenges because of overlapping speech and the lack of control over recording conditions. In this paper we address these challenges by using a speaker-dependent {CNN}-{LSTM} breath detector to separate continuous recordings into utterances, which we here apply to extract nine hours of clean single-speaker breath groups from a conversational podcast. The resulting corpus is transcribed automatically (both lexical items and filler tokens) and used to build several voices on a Tacotron 2 architecture. Listening tests show: i) pronunciation accuracy improved with phonetic input and transfer learning; ii) it is possible to create a more fluent conversational voice by training on data without filled pauses; and iii) the presence of filled pauses improved perceived speaker authenticity. Another listening test showed the found podcast voice to be more appropriate for prompts from both public speeches and casual conversations , compared to synthesis from found read speech and from a manually transcribed lab-recorded spontaneous conversation.},
	author = {Székely, Éva and Henter, Gustav Eje and Beskow, Jonas and Gustafson, Joakim},
	date = {2019},
	keywords = {found data, conversational speech, disfluencies, hesitations, Index Terms: Speech synthesis, spon-taneous speech},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\IWDBZAGS\\szekely2019spontaneous.pdf:application/pdf},
}

@report{mostaani_breathing_2022,
	title = {On Breathing Pattern Information in Synthetic Speech},
	abstract = {The respiratory system is an integral part of human speech production. As a consequence, there is a close relation between respiration and speech signal, and the produced speech signal carries breathing pattern related information. Speech can also be generated using speech synthesis systems. In this paper , we investigate whether synthetic speech carries breathing pattern related information in the same way as natural human speech. We address this research question in the framework of logical-access presentation attack detection using embeddings extracted from neural networks pre-trained for speech breathing pattern estimation. Our studies on {ASVSpoof} 2019 challenge data show that there is a clear distinction between the extracted breathing pattern embedding of natural human speech and synthesized speech, indicating that speech synthesis systems tend to not carry breathing pattern related information in the same way as human speech. Whilst, this is not the case with voice conversion of natural human speech.},
	author = {Mostaani, Zohreh and Doss, Mathew Magimai -},
	date = {2022},
	keywords = {Index Terms: Breathing pattern estimation, Neural network, Presentation attack detection, Synthetic speech},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\K5DGYUTC\\Mostaani_INTERSPEECH_2022.pdf:application/pdf},
}

@article{mehta_neural_2021,
	title = {Neural {HMMs} are all you need (for high-quality attention-free {TTS})},
	url = {http://arxiv.org/abs/2108.13320},
	doi = {10.1109/ICASSP43922.2022.9746686},
	abstract = {Neural sequence-to-sequence {TTS} has achieved significantly better output quality than statistical speech synthesis using {HMMs}. However, neural {TTS} is generally not probabilistic and uses non-monotonic attention. Attention failures increase training time and can make synthesis babble incoherently. This paper describes how the old and new paradigms can be combined to obtain the advantages of both worlds, by replacing attention in neural {TTS} with an autoregressive left-right no-skip hidden Markov model defined by a neural network. Based on this proposal, we modify Tacotron 2 to obtain an {HMM}-based neural {TTS} model with monotonic alignment, trained to maximise the full sequence likelihood without approximation. We also describe how to combine ideas from classical and contemporary {TTS} for best results. The resulting example system is smaller and simpler than Tacotron 2, and learns to speak with fewer iterations and less data, whilst achieving comparable naturalness prior to the post-net. Our approach also allows easy control over speaking rate.},
	author = {Mehta, Shivam and Székely, Éva and Beskow, Jonas and Henter, Gustav Eje},
	date = {2021-08-30},
	eprinttype = {arxiv},
	eprint = {2108.13320},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\S3P3FE6V\\2108.13320.pdf:application/pdf},
}

@article{kong_hifi-gan_2020,
	title = {{HiFi}-{GAN}: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis},
	url = {http://arxiv.org/abs/2010.05646},
	abstract = {Several recent work on speech synthesis have employed generative adversarial networks ({GANs}) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose {HiFi}-{GAN}, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, {MOS}) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 {kHz} high-fidelity audio 167.9 times faster than real-time on a single V100 {GPU}. We further show the generality of {HiFi}-{GAN} to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of {HiFi}-{GAN} generates samples 13.4 times faster than real-time on {CPU} with comparable quality to an autoregressive counterpart.},
	author = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
	date = {2020-10-12},
	eprinttype = {arxiv},
	eprint = {2010.05646},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\S5TCL3W4\\2010.05646.pdf:application/pdf},
}

@article{tan_naturalspeech_2022,
	title = {{NaturalSpeech}: End-to-End Text to Speech Synthesis with Human-Level Quality},
	url = {http://arxiv.org/abs/2205.04421},
	abstract = {Text to speech ({TTS}) has made rapid progress in both academia and industry in recent years. Some questions naturally arise that whether a {TTS} system can achieve human-level quality, how to define/judge that quality and how to achieve it. In this paper, we answer these questions by first defining the human-level quality based on the statistical significance of subjective measure and introducing appropriate guidelines to judge it, and then developing a {TTS} system called {NaturalSpeech} that achieves human-level quality on a benchmark dataset. Specifically, we leverage a variational autoencoder ({VAE}) for end-to-end text to waveform generation, with several key modules to enhance the capacity of the prior from text and reduce the complexity of the posterior from speech, including phoneme pre-training, differentiable duration modeling, bidirectional prior/posterior modeling, and a memory mechanism in {VAE}. Experiment evaluations on popular {LJSpeech} dataset show that our proposed {NaturalSpeech} achieves -0.01 {CMOS} (comparative mean opinion score) to human recordings at the sentence level, with Wilcoxon signed rank test at p-level p {\textgreater}{\textgreater} 0.05, which demonstrates no statistically significant difference from human recordings for the first time on this dataset.},
	author = {Tan, Xu and Chen, Jiawei and Liu, Haohe and Cong, Jian and Zhang, Chen and Liu, Yanqing and Wang, Xi and Leng, Yichong and Yi, Yuanhao and He, Lei and Soong, Frank and Qin, Tao and Zhao, Sheng and Liu, Tie-Yan},
	date = {2022-05-09},
	eprinttype = {arxiv},
	eprint = {2205.04421},
	keywords = {★},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\LG874ZI2\\2205.04421.pdf:application/pdf},
}

@article{tan_survey_2021,
	title = {A Survey on Neural Speech Synthesis},
	url = {http://arxiv.org/abs/2106.15561},
	abstract = {Text to speech ({TTS}), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based {TTS} has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural {TTS}, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural {TTS}, including text analysis, acoustic models and vocoders, and several advanced topics, including fast {TTS}, low-resource {TTS}, robust {TTS}, expressive {TTS}, and adaptive {TTS}, etc. We further summarize resources related to {TTS} (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on {TTS}.},
	author = {Tan, Xu and Qin, Tao and Soong, Frank and Liu, Tie-Yan},
	date = {2021-06-29},
	eprinttype = {arxiv},
	eprint = {2106.15561},
	keywords = {★},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\DHCXV2G6\\TTS_survey.pdf:application/pdf},
}

@article{binkowski_high_2019,
	title = {High Fidelity Speech Synthesis with Adversarial Networks},
	url = {http://arxiv.org/abs/1909.11646},
	abstract = {Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as {WaveNet}, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce {GAN}-{TTS}, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced. To measure the performance of {GAN}-{TTS}, we employ both subjective human evaluation ({MOS} - Mean Opinion Score), as well as novel quantitative metrics (Fr{\textbackslash}'echet {DeepSpeech} Distance and Kernel {DeepSpeech} Distance), which we find to be well correlated with {MOS}. We show that {GAN}-{TTS} is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to {GAN}-{TTS} reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav.},
	author = {Bińkowski, Mikołaj and Donahue, Jeff and Dieleman, Sander and Clark, Aidan and Elsen, Erich and Casagrande, Norman and Cobo, Luis C. and Simonyan, Karen},
	date = {2019-09-25},
	eprinttype = {arxiv},
	eprint = {1909.11646},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\S98CGIU6\\gan-tts.pdf:application/pdf},
}

@article{ren_fastspeech_2020,
	title = {{FastSpeech} 2: Fast and High-Quality End-to-End Text to Speech},
	url = {http://arxiv.org/abs/2006.04558},
	abstract = {Non-autoregressive text to speech ({TTS}) models such as {FastSpeech} can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of {FastSpeech} model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in {TTS}. However, {FastSpeech} has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose {FastSpeech} 2, which addresses the issues in {FastSpeech} and better solves the one-to-many mapping problem in {TTS} by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design {FastSpeech} 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) {FastSpeech} 2 achieves a 3x training speed-up over {FastSpeech}, and {FastSpeech} 2s enjoys even faster inference speed; 2) {FastSpeech} 2 and 2s outperform {FastSpeech} in voice quality, and {FastSpeech} 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.},
	author = {Ren, Yi and Hu, Chenxu and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
	date = {2020-06-08},
	eprinttype = {arxiv},
	eprint = {2006.04558},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\XU7Z777W\\fastspeech2.pdf:application/pdf},
}

@article{shen_natural_2017,
	title = {Natural {TTS} Synthesis by Conditioning {WaveNet} on Mel Spectrogram Predictions},
	url = {http://arxiv.org/abs/1712.05884},
	abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified {WaveNet} model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score ({MOS}) of \$4.53\$ comparable to a {MOS} of \$4.58\$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to {WaveNet} instead of linguistic, duration, and \$F\_0\$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the {WaveNet} architecture.},
	author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerry-Ryan, {RJ} and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
	date = {2017-12-15},
	eprinttype = {arxiv},
	eprint = {1712.05884},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\FF6N399B\\tacotron2.pdf:application/pdf},
}

@article{chen_adaspeech_2021,
	title = {{AdaSpeech}: Adaptive Text to Speech for Custom Voice},
	url = {http://arxiv.org/abs/2103.00993},
	abstract = {Custom voice, a specific text to speech ({TTS}) service in commercial speech platforms, aims to adapt a source {TTS} model to synthesize personal voice for a target speaker using few speech data. Custom voice presents two unique challenges for {TTS} adaptation: 1) to support diverse customers, the adaptation model needs to handle diverse acoustic conditions that could be very different from source speech data, and 2) to support a large number of customers, the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality. In this work, we propose {AdaSpeech}, an adaptive {TTS} system for high-quality and efficient customization of new voices. We design several techniques in {AdaSpeech} to address the two challenges in custom voice: 1) To handle different acoustic conditions, we use two acoustic encoders to extract an utterance-level vector and a sequence of phoneme-level vectors from the target speech during training; in inference, we extract the utterance-level vector from a reference speech and use an acoustic predictor to predict the phoneme-level vectors. 2) To better trade off the adaptation parameters and voice quality, we introduce conditional layer normalization in the mel-spectrogram decoder of {AdaSpeech}, and fine-tune this part in addition to speaker embedding for adaptation. We pre-train the source {TTS} model on {LibriTTS} datasets and fine-tune it on {VCTK} and {LJSpeech} datasets (with different acoustic conditions from {LibriTTS}) with few adaptation data, e.g., 20 sentences, about 1 minute speech. Experiment results show that {AdaSpeech} achieves much better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice. Audio samples are available at https://speechresearch.github.io/adaspeech/.},
	author = {Chen, Mingjian and Tan, Xu and Li, Bohan and Liu, Yanqing and Qin, Tao and Zhao, Sheng and Liu, Tie-Yan},
	date = {2021-03-01},
	eprinttype = {arxiv},
	eprint = {2103.00993},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\I2MJXZNI\\ada.pdf:application/pdf},
}

@article{nallanthighal_deep_2021,
	title = {Deep learning architectures for estimating breathing signal and respiratory parameters from speech recordings},
	volume = {141},
	issn = {18792782},
	doi = {10.1016/j.neunet.2021.03.029},
	abstract = {Respiration is an essential and primary mechanism for speech production. We first inhale and then produce speech while exhaling. When we run out of breath, we stop speaking and inhale. Though this process is involuntary, speech production involves a systematic outflow of air during exhalation characterized by linguistic content and prosodic factors of the utterance. Thus speech and respiration are closely related, and modeling this relationship makes sensing respiratory dynamics directly from the speech plausible, however is not well explored. In this article, we conduct a comprehensive study to explore techniques for sensing breathing signal and breathing parameters from speech using deep learning architectures and address the challenges involved in establishing the practical purpose of this technology. Estimating the breathing pattern from the speech would give us information about the respiratory parameters, thus enabling us to understand the respiratory health using one's speech.},
	pages = {211--224},
	journaltitle = {Neural Networks},
	author = {Nallanthighal, Venkata Srikanth and Mostaani, Zohreh and Härmä, Aki and Strik, Helmer and Magimai-Doss, Mathew},
	date = {2021-09-01},
	pmid = {33915446},
	note = {Publisher: Elsevier Ltd},
	keywords = {Speech breathing, Deep neural networks, Respiratory parameters, Signal processing, Speech technology},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\22A5ZR66\\1-s2.0-S0893608021001179-main.pdf:application/pdf},
}

@article{elias_parallel_2020,
	title = {Parallel Tacotron: Non-Autoregressive and Controllable {TTS}},
	url = {http://arxiv.org/abs/2010.11439},
	abstract = {Although neural end-to-end text-to-speech models can synthesize highly natural speech, there is still room for improvements to its efficiency and naturalness. This paper proposes a non-autoregressive neural text-to-speech model augmented with a variational autoencoder-based residual encoder. This model, called {\textbackslash}emph\{Parallel Tacotron\}, is highly parallelizable during both training and inference, allowing efficient synthesis on modern parallel hardware. The use of the variational autoencoder relaxes the one-to-many mapping nature of the text-to-speech problem and improves naturalness. To further improve the naturalness, we use lightweight convolutions, which can efficiently capture local contexts, and introduce an iterative spectrogram loss inspired by iterative refinement. Experimental results show that Parallel Tacotron matches a strong autoregressive baseline in subjective evaluations with significantly decreased inference time.},
	author = {Elias, Isaac and Zen, Heiga and Shen, Jonathan and Zhang, Yu and Jia, Ye and Weiss, Ron and Wu, Yonghui},
	date = {2020-10-22},
	eprinttype = {arxiv},
	eprint = {2010.11439},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\RDT4H6ME\\parallel_tacotron.pdf:application/pdf},
}

@article{zhang_learning_2018,
	title = {Learning latent representations for style control and transfer in end-to-end speech synthesis},
	url = {http://arxiv.org/abs/1812.04342},
	abstract = {In this paper, we introduce the Variational Autoencoder ({VAE}) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through {VAE} shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of {VAE}, then feeding it into {TTS} network to guide the style in synthesizing speech. To avoid Kullback-Leibler ({KL}) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token ({GST}) model in {ABX} preference tests on style transfer.},
	author = {Zhang, Ya-Jie and Pan, Shifeng and He, Lei and Ling, Zhen-Hua},
	date = {2018-12-11},
	eprinttype = {arxiv},
	eprint = {1812.04342},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\SVHE9SF3\\1812.04342.pdf:application/pdf},
}

@inproceedings{akuzawa_expressive_2018,
	title = {Expressive speech synthesis via modeling expressions with variational autoencoder},
	volume = {2018-September},
	doi = {10.21437/Interspeech.2018-1113},
	abstract = {Recent advances in neural autoregressive models have improve the performance of speech synthesis ({SS}). However, as they lack the ability to model global characteristics of speech (such as speaker individualities or speaking styles), particularly when these characteristics have not been labeled, making neural autoregressive {SS} systems more expressive is still an open issue. In this paper, we propose to combine {VoiceLoop}, an autoregressive {SS} model, with Variational Autoencoder ({VAE}). This approach, unlike traditional autoregressive {SS} systems, uses {VAE} to model the global characteristics explicitly, enabling the expressiveness of the synthesized speech to be controlled in an unsupervised manner. Experiments using the {VCTK} and Bliz-zard2012 datasets show the {VAE} helps {VoiceLoop} to generate higher quality speech and to control the experssions in its synthesized speech by incorporating global characteristics into the speech generating process.},
	pages = {3067--3071},
	booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, {INTERSPEECH}},
	publisher = {International Speech Communication Association},
	author = {Akuzawa, Kei and Iwasawa, Yusuke and Matsuo, Yutaka},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1804.02135},
	note = {{ISSN}: 19909772},
	keywords = {Autoregressive model, Expressive speech synthesis, Variational autoencoder},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\74TLNG9I\\1113.pdf:application/pdf},
}

@inproceedings{markitantov_ensembling_2020,
	title = {Ensembling end-to-end deep models for computational paralinguistics tasks: {ComParE} 2020 mask and breathing sub-challenges},
	volume = {2020-October},
	doi = {10.21437/Interspeech.2020-2666},
	abstract = {This paper describes deep learning approaches for the Mask and Breathing Sub-Challenges ({SCs}), which are addressed by the {INTERSPEECH} 2020 Computational Paralinguistics Challenge. Motivated by outstanding performance of state-of-the-art end-to-end (E2E) approaches, we explore and compare effectiveness of different deep Convolutional Neural Network ({CNN}) architectures on raw data, log Mel-spectrograms, and Mel-Frequency Cepstral Coefficients. We apply a transfer learning approach to improve model's efficiency and convergence speed. In the Mask {SC}, we conduct experiments with several pretrained {CNN} architectures on log-Mel spectrograms, as well as Support Vector Machines on baseline features. For the Breathing {SC}, we propose an ensemble deep learning system that exploits E2E learning and sequence prediction. The E2E model is based on 1D {CNN} operating on raw speech signals and is coupled with Long Short-Term Memory layers for sequence modeling. The second model works with log-Mel features and is based on a pretrained 2D {CNN} model stacked to Gated Recurrent Unit layers. To increase performance of our models in both {SCs}, we use ensembles of the best deep neural models obtained from N-fold cross-validation on combined challenge training and development datasets. Our results markedly outperform the challenge test set baselines in both {SCs}.},
	pages = {2072--2076},
	booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, {INTERSPEECH}},
	publisher = {International Speech Communication Association},
	author = {Markitantov, Maxim and Dresvyanskiy, Denis and Mamontov, Danila and Kaya, Heysem and Minker, Wolfgang and Karpov, Alexey},
	date = {2020},
	note = {{ISSN}: 19909772},
	keywords = {Computational paralinguistics, End-to-end models, Information fusion, Neural networks, Transfer learning},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\NAUKV5CZ\\markitantov20_interspeech.pdf:application/pdf},
}

@article{yu_durian_2019,
	title = {{DurIAN}: Duration Informed Attention Network For Multimodal Synthesis},
	url = {http://arxiv.org/abs/1909.01700},
	abstract = {In this paper, we present a generic and robust multimodal synthesis system that produces highly natural speech and facial expression simultaneously. The key component of this system is the Duration Informed Attention Network ({DurIAN}), an autoregressive model in which the alignments between the input text and the output acoustic features are inferred from a duration model. This is different from the end-to-end attention mechanism used, and accounts for various unavoidable artifacts, in existing end-to-end speech synthesis systems such as Tacotron. Furthermore, {DurIAN} can be used to generate high quality facial expression which can be synchronized with generated speech with/without parallel speech and face data. To improve the efficiency of speech generation, we also propose a multi-band parallel generation strategy on top of the {WaveRNN} model. The proposed Multi-band {WaveRNN} effectively reduces the total computational complexity from 9.8 to 5.5 {GFLOPS}, and is able to generate audio that is 6 times faster than real time on a single {CPU} core. We show that {DurIAN} could generate highly natural speech that is on par with current state of the art end-to-end systems, while at the same time avoid word skipping/repeating errors in those systems. Finally, a simple yet effective approach for fine-grained control of expressiveness of speech and facial expression is introduced.},
	author = {Yu, Chengzhu and Lu, Heng and Hu, Na and Yu, Meng and Weng, Chao and Xu, Kun and Liu, Peng and Tuo, Deyi and Kang, Shiyin and Lei, Guangzhi and Su, Dan and Yu, Dong},
	date = {2019-09-04},
	eprinttype = {arxiv},
	eprint = {1909.01700},
	keywords = {to-read},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\7W3P3G4Z\\durian.pdf:application/pdf},
}

@report{karlapati_prosodic_2020,
	title = {Prosodic representation learning and contextual sampling for neural text-to-speech},
	abstract = {In this paper, we introduce Kathaka, a model trained with a novel two-stage training process for neural speech synthesis with contextually appropriate prosody. In Stage I, we learn a prosodic distribution at the sentence level from mel-spectrograms available during training. In Stage {II}, we propose a novel method to sample from this learnt prosodic distribution using the contextual information available in text. To do this, we use {BERT} on text, and graph-attention networks on parse trees extracted from text. We show a statistically significant relative improvement of 13.2\% in naturalness over a strong baseline when compared to recordings. We also conduct an ablation study on variations of our sampling technique , and show a statistically significant improvement over the baseline in each case.},
	author = {Karlapati, Sri and Abbas, Ammar and Hodari, Zack and Moinet, Alexis and Joly, Arnaud and Karanasou, Penny and Drugman, Thomas},
	date = {2020},
	keywords = {★, contextual prosody, Index Terms-{TTS}, prosody modelling},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\VXIDMSU5\\prosodic-representation-learning-and-contextual-sampling-for-neural-text-to-speech.pdf:application/pdf},
}

@misc{lameris_prosody-controllable_2022,
	title = {Prosody-controllable spontaneous {TTS} with neural {HMMs}},
	url = {http://arxiv.org/abs/2211.13533},
	abstract = {Spontaneous speech has many affective and pragmatic functions that are interesting and challenging to model in {TTS} (text-to-speech). However, the presence of reduced articulation, ﬁllers, repetitions, and other disﬂuencies mean that text and acoustics are less well aligned than in read speech. This is problematic for attention-based {TTS}. We propose a {TTS} architecture that is particularly suited for rapidly learning to speak from irregular and small datasets while also reproducing the diversity of expressive phenomena present in spontaneous speech. Speciﬁcally, we modify an existing neural {HMM}-based {TTS} system, which is capable of stable, monotonic alignments for spontaneous speech, and add utterancelevel prosody control, so that the system can represent the wide range of natural variability in a spontaneous speech corpus. We objectively evaluate control accuracy and perform a subjective listening test to compare to a system without prosody control. To exemplify the power of combining midlevel prosody control and ecologically valid data for reproducing intricate spontaneous speech phenomena, we evaluate the system’s capability of synthesizing two types of creaky phonation.},
	number = {{arXiv}:2211.13533},
	publisher = {{arXiv}},
	author = {Lameris, Harm and Mehta, Shivam and Henter, Gustav Eje and Gustafson, Joakim and Székely, Éva},
	urldate = {2023-02-17},
	date = {2022-11-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2211.13533 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Human-Computer Interaction, Machine Learning, Sound},
	file = {Lameris et al. - 2022 - Prosody-controllable spontaneous TTS with neural H.pdf:C\:\\Users\\nicol\\Zotero\\storage\\BA9K9U9W\\Lameris et al. - 2022 - Prosody-controllable spontaneous TTS with neural H.pdf:application/pdf},
}

@misc{raitio_hierarchical_2022,
	title = {Hierarchical prosody modeling and control in non-autoregressive parallel neural {TTS}},
	url = {http://arxiv.org/abs/2110.02952},
	abstract = {Neural text-to-speech ({TTS}) synthesis can generate speech that is indistinguishable from natural speech. However, the synthetic speech often represents the average prosodic style of the database instead of having more versatile prosodic variation. Moreover, many models lack the ability to control the output prosody, which does not allow for different styles for the same text input. In this work, we train a non-autoregressive parallel neural {TTS} front-end model hierarchically conditioned on both coarse and ﬁne-grained acoustic speech features to learn a latent prosody space with intuitive and meaningful dimensions. Experiments show that a non-autoregressive {TTS} model hierarchically conditioned on utterance-wise pitch, pitch range, duration, energy, and spectral tilt can effectively control each prosodic dimension, generate a wide variety of speaking styles, and provide word-wise emphasis control, while maintaining equal or better quality to the baseline model.},
	number = {{arXiv}:2110.02952},
	publisher = {{arXiv}},
	author = {Raitio, Tuomo and Li, Jiangchuan and Seshadri, Shreyas},
	urldate = {2023-02-17},
	date = {2022-03-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2110.02952 [cs, eess]},
	keywords = {★, Electrical Engineering and Systems Science - Audio and Speech Processing, Computation and Language, Apple},
	file = {Raitio et al. - 2022 - Hierarchical prosody modeling and control in non-a.pdf:C\:\\Users\\nicol\\Zotero\\storage\\QPU73IEK\\Raitio et al. - 2022 - Hierarchical prosody modeling and control in non-a.pdf:application/pdf},
}

@article{riede_vocal_2005,
	title = {Vocal production mechanisms in a non-human primate: morphological data and a model},
	volume = {48},
	issn = {00472484},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047248404001435},
	doi = {10.1016/j.jhevol.2004.10.002},
	shorttitle = {Vocal production mechanisms in a non-human primate},
	abstract = {Human beings are thought to be unique amongst the primates in their capacity to produce rapid changes in the shape of their vocal tracts during speech production. Acoustically, vocal tracts act as resonance chambers, whose geometry determines the position and bandwidth of the formants. Formants provide the acoustic basis for vowels, which enable speakers to refer to external events and to produce other kinds of meaningful communication. Formantbased referential communication is also present in non-human primates, most prominently in Diana monkey alarm calls. Previous work has suggested that the acoustic structure of these calls is the product of a non-uniform vocal tract capable of some degree of articulation. In this study we test this hypothesis by providing morphological measurements of the vocal tract of three adult Diana monkeys, using both radiography and dissection. We use these data to generate a vocal tract computational model capable of simulating the formant structures produced by wild individuals. The model performed best when it combined a non-uniform vocal tract consisting of three diﬀerent tubes with a number of articulatory manoeuvres. We discuss the implications of these ﬁndings for evolutionary theories of human and nonhuman vocal production.},
	pages = {85--96},
	number = {1},
	journaltitle = {Journal of Human Evolution},
	author = {Riede, Tobias and Bronson, Ellen and Hatzikirou, Haralambos and Zuberbühler, Klaus},
	urldate = {2023-03-05},
	date = {2005-01},
	langid = {english},
	file = {Riede et al. - 2005 - Vocal production mechanisms in a non-human primate.pdf:C\:\\Users\\nicol\\Zotero\\storage\\YU52ZU54\\Riede et al. - 2005 - Vocal production mechanisms in a non-human primate.pdf:application/pdf},
}

@book{giles_contexts_1991,
	location = {Paris,  France},
	title = {Contexts of accommodation:  Developments in applied sociolinguistics.},
	isbn = {0-521-36151-6 (Hardcover); 2-7351-0409-5 (Hardcover)},
	series = {Contexts of accommodation:  Developments in applied sociolinguistics.},
	abstract = {The theory of accommodation is concerned with motivations underlying and consequences arising from ways in which we adapt our language and communication patterns toward others. Since accommodation theory's emergence in the early 1970s, it has attracted empirical attention across many disciplines and has been elaborated and expanded many times.  Here accommodation theory is presented as a basis for sociolinguistic explanation and is the applied perspective that predominates in this edited collection. The book as a whole seeks to demonstrate how the core concepts and relationships involved by accommodation theory are available for addressing altogether pragmatic concerns. Accommodative processes can, for example, facilitate or impede language learners' proficiency in a second language as well as immigrants' acceptance into certain host communities; affect audience ratings and thereby the life of a television program; affect reaction to defendants in court and hence the nature of the judicial outcome; and be an enabling or detrimental force in allowing handicapped people to fulfill their communicative potential. "Contexts of Accommodation" will be of interest to researchers and advanced students in language and communication sciences. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pagetotal = {viii, 321},
	publisher = {Editions de la Maison des Sciences de l'Homme},
	editorb = {Giles, Howard and Coupland, Justine and Coupland, Nikolas},
	editorbtype = {redactor},
	date = {1991},
	doi = {10.1017/CBO9780511663673},
	note = {Pages: viii, 321},
	keywords = {*Adjustment, Sociolinguistics},
}

@article{kuhlmann_effects_2021,
	title = {Effects of Speaking Rate on Breathing and Voice Behavior},
	issn = {08921997},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0892199721003052},
	doi = {10.1016/j.jvoice.2021.09.005},
	abstract = {Methods. Ten women and seven men were included as subjects. Lung volume and breathing behaviors were measured by respiratory inductance plethysmography and {fO} was measured from audio recordings by the Praat software. Statistical signiﬁcance was tested by analysis of variance.
Results. For both reading and spontaneous speech, the group increased mean breath group size and breath group duration signiﬁcantly in the fast speaking rate condition. The group signiﬁcantly decreased lung volume excursion per syllable in fast speech. Females also showed a signiﬁcant increase of {fO} in fast speech. The lung volume levels for initiation and termination of breath groups, as well as lung volume excursions in \% vital capacity, showed great individual variations and no signiﬁcant effects of rate. Signiﬁcant effects of speech task were found for breath group size and lung volume excursion per syllable, where reading induced more syllables produced per breath group and less \% {VC} spend per syllable as compared to spontaneous speech. Interaction effects showed that the increases in breath group size and breath group duration associated with fast rate were signiﬁcantly larger in reading than in spontaneous speech.
Conclusion. Our data from 17 vocally untrained, healthy subjects showed great individual variations but still signiﬁcant group effects regarding increased speaking rate, where the subjects seemed to spend less air per syllable and inhaled less often as a consequence of greater breath group sizes in fast speech. Subjects showed greater changes in breath group patterns as a consequence of fast speech in reading than in spontaneous speech, indicating that effects of speaking rate are dependent on the speech task.},
	pages = {S0892199721003052},
	journaltitle = {Journal of Voice},
	author = {Kuhlmann, Laura Lund and Iwarsson, Jenny},
	urldate = {2023-03-07},
	date = {2021-10},
	langid = {english},
	file = {Kuhlmann and Iwarsson - 2021 - Effects of Speaking Rate on Breathing and Voice Be.pdf:C\:\\Users\\nicol\\Zotero\\storage\\EVR6CCNA\\Kuhlmann and Iwarsson - 2021 - Effects of Speaking Rate on Breathing and Voice Be.pdf:application/pdf},
}

@software{robert_pydub_2023,
	title = {Pydub},
	rights = {{MIT}},
	url = {https://github.com/jiaaro/pydub},
	abstract = {Manipulate audio with a simple and easy high level interface},
	author = {Robert, James},
	urldate = {2023-03-07},
	date = {2023-03-07},
	note = {original-date: 2011-05-02T18:42:38Z},
}

@article{hoit_age_1987,
	title = {Age and Speech Breathing},
	volume = {30},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.3003.351},
	doi = {10.1044/jshr.3003.351},
	abstract = {Thirty healthy men representing three widely different age groups (25, 50, and 75 years) were studied with respect to general respiratory function and speech breathing. Subdivisions of the lung volume were found to differ with age and most markedly so for measures of vital capacity and residual volume. Speech breathing also was found to differ with age and was characterized by differences in lung volume excursion, rib cage volume initiation, number of syllables per breath group; and lung volume expended per syllable: Age-related differences in general respiratory function and speech breathing are discussed in relation to possible underlying mechanisms. In addition, implications are drawn regarding the evaluation and management of individuals with speech breathing disorders.},
	pages = {351--366},
	number = {3},
	journaltitle = {J Speech Lang Hear Res},
	author = {Hoit, Jeannette D. and Hixon, Thomas J.},
	urldate = {2023-03-07},
	date = {1987-09},
	langid = {english},
	file = {Hoit and Hixon - 1987 - Age and Speech Breathing.pdf:C\:\\Users\\nicol\\Zotero\\storage\\HQP6NUVE\\Hoit and Hixon - 1987 - Age and Speech Breathing.pdf:application/pdf},
}

@article{fuchs_respiratory_2021,
	title = {The Respiratory Foundations of Spoken Language},
	volume = {7},
	issn = {2333-9683, 2333-9691},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-linguistics-031720-103907},
	doi = {10.1146/annurev-linguistics-031720-103907},
	abstract = {Why is breathing relevant for linguistics? In this review we approach this question from diﬀerent perspectives. The most popular view is that breathing adapts to speech, because respiratory behaviour has astonishing ﬂexibility. Among others, we review studies showing that breathing pauses occur mostly at meaningful places, breathing adapts to cognitive load during speech perception, and breathing adapts to the communicative needs in dialogue. However, speech may also adapt to breathing: e.g. the larynx can compensate for air loss, breathing can partially aﬀect f0 declination. Enhanced breathing control may have played a role for vocalisation and language evolution. Both views are not exclusive but reveal that speech production and breathing have an interwoven relationship which depends on communicative and physical constraints. We suggest that breathing should become an important topic for diﬀerent linguistic areas and that future work should investigate the interaction between breathing and speech in diﬀerent situational contexts.},
	pages = {13--30},
	number = {1},
	journaltitle = {Annu. Rev. Linguist.},
	author = {Fuchs, Susanne and Rochet-Capellan, Amélie},
	urldate = {2023-03-07},
	date = {2021-01-14},
	langid = {english},
	file = {Fuchs and Rochet-Capellan - 2021 - The Respiratory Foundations of Spoken Language.pdf:C\:\\Users\\nicol\\Zotero\\storage\\Y3NDSARZ\\Fuchs and Rochet-Capellan - 2021 - The Respiratory Foundations of Spoken Language.pdf:application/pdf},
}

@misc{jia_transfer_2019,
	title = {Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis},
	url = {http://arxiv.org/abs/1806.04558},
	abstract = {We describe a neural network-based system for text-to-speech ({TTS}) synthesis that is able to generate speech audio in the voice of different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker veriﬁcation task using an independent dataset of noisy speech without transcripts from thousands of speakers, to generate a ﬁxed-dimensional embedding vector from only seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2 that generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive {WaveNet}-based vocoder network that converts the mel spectrogram into time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the multispeaker {TTS} task, and is able to synthesize natural speech from speakers unseen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.},
	number = {{arXiv}:1806.04558},
	publisher = {{arXiv}},
	author = {Jia, Ye and Zhang, Yu and Weiss, Ron J. and Wang, Quan and Shen, Jonathan and Ren, Fei and Chen, Zhifeng and Nguyen, Patrick and Pang, Ruoming and Moreno, Ignacio Lopez and Wu, Yonghui},
	urldate = {2023-03-07},
	date = {2019-01-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1806.04558 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound},
	file = {Jia et al. - 2019 - Transfer Learning from Speaker Verification to Mul.pdf:C\:\\Users\\nicol\\Zotero\\storage\\TP2RP6LX\\deepfake.pdf:application/pdf},
}

@article{hoit_influence_2000,
	title = {Influence of Continuous Speaking on Ventilation},
	volume = {43},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jslhr.4305.1240},
	doi = {10.1044/jslhr.4305.1240},
	abstract = {National Center for Neurogenic Communication Disorders and Department of Speech and Hearing Sciences The University of Arizona Tucson This study was conducted to explore the influence of speaking on ventilation. Twenty healthy young men were studied during periods of quiet breathing and prolonged speaking using noninvasive methods to measure chest wall surface motions and expired gas composition. Results indicated that all subjects ventilated more during speaking than during quiet breathing, usually by augmenting both tidal volume and breathing frequency. Ventilation did not change across repeated speaking trials. Quiet breathing was altered from its usual behavior following speaking, often for several minutes. Speaking-related increases in ventilation were found to be strongly correlated with lung volume expenditures per syllable. These findings have clinical implications for the respiratory care practitioner and the speech-language pathologist.},
	pages = {1240--1251},
	number = {5},
	journaltitle = {J Speech Lang Hear Res},
	author = {Hoit, Jeannette D. and Lohmeier, Heather L.},
	urldate = {2023-03-07},
	date = {2000-10},
	langid = {english},
	file = {Hoit and Lohmeier - 2000 - Influence of Continuous Speaking on Ventilation.pdf:C\:\\Users\\nicol\\Zotero\\storage\\LIPZPQIL\\Hoit and Lohmeier - 2000 - Influence of Continuous Speaking on Ventilation.pdf:application/pdf},
}

@article{wang_breath_2010,
	title = {Breath Group Analysis for Reading and Spontaneous Speech in Healthy Adults},
	volume = {62},
	issn = {1021-7762, 1421-9972},
	url = {https://www.karger.com/Article/FullText/316976},
	doi = {10.1159/000316976},
	abstract = {\textit{Aims:} The breath group can serve as a functional unit to define temporal and fundamental frequency (f\&\#8320;) features in continuous speech. These features of the breath group are determined by the physiologic, linguistic, and cognitive demands of communication. Reading and spontaneous speech are two speaking tasks that vary in these demands and are commonly used to evaluate speech performance for research and clinical applications. The purpose of this study is to examine differences between reading and spontaneous speech in the temporal and f\&\#8320; aspects of their breath groups. \textit{Methods:} Sixteen participants read two passages and answered six questions while wearing a circumferentially vented mask connected to a pneumotach. The aerodynamic signal was used to identify inspiratory locations. The audio signal was used to analyze task differences in breath group structure, including temporal and f\&\#8320; components. \textit{Results:} The main findings were that spontaneous speech task exhibited significantly more grammatically inappropriate breath group locations and longer breath group duration than did the passage reading task. \textit{Conclusion:} The task differences in the percentage of grammatically inadequate breath group locations and in breath group duration for healthy adult speakers partly explain the differences in cognitive-linguistic load between the passage reading and spontaneous speech.},
	pages = {297--302},
	number = {6},
	journaltitle = {Folia Phoniatr Logop},
	author = {Wang, Yu-Tsai and Green, Jordan R. and Nip, Ignatius S.B. and Kent, Ray D. and Kent, Jane Finley},
	urldate = {2023-03-07},
	date = {2010},
	langid = {english},
	file = {Wang et al. - 2010 - Breath Group Analysis for Reading and Spontaneous .pdf:C\:\\Users\\nicol\\Zotero\\storage\\7PTI4D5D\\Wang et al. - 2010 - Breath Group Analysis for Reading and Spontaneous .pdf:application/pdf},
}

@article{hodge_characteristics_1989,
	title = {Characteristics of Speech Breathing in Young Women},
	volume = {32},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/jshr.3203.466},
	doi = {10.1044/jshr.3203.466},
	abstract = {Chest wall kinematic records were obtained from 10 healthy young women in the upright, seated position during resting breathing, conversation, and reading aloud. Breathing frequency, lung volume levels relative to resting end-expiratory level, and relative volume displacements of the rib cage and abdomen were measured. Compared to conversation, group results for reading revealed three differences: an increase in syllables spoken per breath, an absence of filled pauses, and a slight upward shift in end-inspiratory and end-expiratory lung volume levels. Compared to resting breathing, group results for speech revealed four differences: a background chest wall configuration characterized by a relatively larger rib cage and smaller abdomen, slight increases in breathing frequency and in lung volume expenditure, and a slight decrease in rib cage contribution to lung volume displacement. The physical characteristic most strongly associated with rib cage contribution to lung volume displacement in resting breathing was height (r = .76). In comparing the relationship between the same respiratory behavior during resting breathing and speech, a correlation of .83 was obtained for rib cage contribution to volume displacement in the two conditions and of.60 for end-inspiratory volume level in the two conditions. Somewhat weaker positive correlations were obtained for lung volume expenditure and for breathing frequency in the two conditions. Comparison of the present findings for women to those recently reported for comparable men (Holt \& Hixon, 1987) revealed no remarkable differences in speech breathing characteristics. Results suggest that certain physical characteristics and task variables may have greater functional importance than gender in determining normative speech breathing behaviors.},
	pages = {466--480},
	number = {3},
	journaltitle = {J Speech Lang Hear Res},
	author = {Hodge, Megan M. and Rochet, Anne Putnam},
	urldate = {2023-03-08},
	date = {1989-09},
	langid = {english},
	file = {Hodge and Rochet - 1989 - Characteristics of Speech Breathing in Young Women.pdf:C\:\\Users\\nicol\\Zotero\\storage\\6FBVKPQB\\Hodge and Rochet - 1989 - Characteristics of Speech Breathing in Young Women.pdf:application/pdf},
}

@article{berger_explorations_1975,
	title = {Some Explorations in Initial Interaction and Beyond: Toward a Developmental Theory of Interpersonal Communication},
	volume = {1},
	issn = {0360-3989},
	url = {https://doi.org/10.1111/j.1468-2958.1975.tb00258.x},
	doi = {10.1111/j.1468-2958.1975.tb00258.x},
	shorttitle = {Some Explorations in Initial Interaction and Beyond},
	abstract = {This paper provides a theoretical perspective for dealing with the initial entry stage of interpersonal interaction. The seven axioms and 21 theorems presented suggest a set of research priorities for studying the development of interpersonal relationships. The paper concludes with a discussion of some of the problems to be considered if the theory is to be extended beyond the initial stages of interaction.},
	pages = {99--112},
	number = {2},
	journaltitle = {Human Communication Research},
	author = {Berger, Charles R. and Calabrese, Richard J.},
	urldate = {2023-03-09},
	date = {1975-12-01},
	file = {Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\RH8IMUAT\\4637500.html:text/html},
}

@online{noauthor_prosody_nodate,
	title = {Prosody {\textbar} Definition, Examples, Elements, \& Facts {\textbar} Britannica},
	url = {https://www.britannica.com/art/prosody},
	abstract = {prosody, the study of all the elements of language that contribute toward acoustic and rhythmic effects, chiefly in poetry but also in prose. The term derived from an ancient Greek word that originally meant a song accompanied by music or the particular tone or accent given to an individual syllable. Greek and Latin literary critics generally regarded prosody as part of grammar; it concerned itself with the rules determining the length or shortness of a syllable, with syllabic quantity, and with how the various combinations of short and long syllables formed the metres (i.e., the rhythmic patterns) of Greek and},
	urldate = {2023-03-22},
	langid = {english},
	file = {Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\D533WYCM\\prosody.html:text/html},
}

@article{ohala_ethological_1984,
	title = {An Ethological Perspective on Common Cross-Language Utilization of F₀ of Voice},
	volume = {41},
	url = {https://doi.org/10.1159/000261706},
	doi = {10.1159/000261706},
	series = {Phonetica},
	pages = {1--16},
	number = {1},
	author = {Ohala, John J.},
	urldate = {2023-03-22},
	date = {1984},
	file = {ohala1984.pdf:C\:\\Users\\nicol\\Zotero\\storage\\EDHENEQI\\ohala1984.pdf:application/pdf},
}

@article{xu_survey_2022,
	title = {A Survey of Cross-lingual Sentiment Analysis: Methodologies, Models and Evaluations},
	volume = {7},
	issn = {2364-1541},
	url = {https://doi.org/10.1007/s41019-022-00187-3},
	doi = {10.1007/s41019-022-00187-3},
	shorttitle = {A Survey of Cross-lingual Sentiment Analysis},
	abstract = {Cross-lingual sentiment analysis ({CLSA}) leverages one or several source languages to help the low-resource languages to perform sentiment analysis. Therefore, the problem of lack of annotated corpora in many non-English languages can be alleviated. Along with the development of economic globalization, {CLSA} has attracted much attention in the field of sentiment analysis and the last decade has seen a surge of researches in this area. Numerous methods, datasets and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art {CLSA} approaches from 2004 to the present. This paper teases out the research context of cross-lingual sentiment analysis and elaborates the following methods in detail: (1) The early main methods of {CLSA}, including those based on Machine Translation and its improved variants, parallel corpora or bilingual sentiment lexicon; (2) {CLSA} based on cross-lingual word embedding; (3) {CLSA} based on multi-{BERT} and other pre-trained models. We further analyze their main ideas, methodologies, shortcomings, etc., and attempt to reach a conclusion on the coverage of languages, datasets and their performance. Finally, we look into the future development of {CLSA} and the challenges facing the research area.},
	pages = {279--299},
	number = {3},
	journaltitle = {Data Sci. Eng.},
	author = {Xu, Yuemei and Cao, Han and Du, Wanze and Wang, Wenqing},
	urldate = {2023-03-22},
	date = {2022-09-01},
	langid = {english},
	keywords = {Bilingual word embedding, Cross-lingual, Sentiment analysis},
	file = {Full Text PDF:C\:\\Users\\nicol\\Zotero\\storage\\7XFLZMXR\\Xu et al. - 2022 - A Survey of Cross-lingual Sentiment Analysis Meth.pdf:application/pdf},
}

@misc{chen_cross-lingual_2023,
	title = {Cross-lingual Alzheimer's Disease detection based on paralinguistic and pre-trained features},
	url = {http://arxiv.org/abs/2303.07650},
	abstract = {We present our submission to the {ICASSP}-{SPGC}-2023 {ADReSS}-M Challenge Task, which aims to investigate which acoustic features can be generalized and transferred across languages for Alzheimer's Disease ({AD}) prediction. The challenge consists of two tasks: one is to classify the speech of {AD} patients and healthy individuals, and the other is to infer Mini Mental State Examination ({MMSE}) score based on speech only. The difficulty is mainly embodied in the mismatch of the dataset, in which the training set is in English while the test set is in Greek. We extract paralinguistic features using {openSmile} toolkit and acoustic features using {XLSR}-53. In addition, we extract linguistic features after transcribing the speech into text. These features are used as indicators for {AD} detection in our method. Our method achieves an accuracy of 69.6\% on the classification task and a root mean squared error ({RMSE}) of 4.788 on the regression task. The results show that our proposed method is expected to achieve automatic multilingual Alzheimer's Disease detection through spontaneous speech.},
	number = {{arXiv}:2303.07650},
	publisher = {{arXiv}},
	author = {Chen, Xuchu and Pu, Yu and Li, Jinpeng and Zhang, Wei-Qiang},
	urldate = {2023-03-22},
	date = {2023-03-14},
	eprinttype = {arxiv},
	eprint = {2303.07650 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Sound},
	file = {arXiv Fulltext PDF:C\:\\Users\\nicol\\Zotero\\storage\\FMCE9GQI\\Chen et al. - 2023 - Cross-lingual Alzheimer's Disease detection based .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\86SNTWIS\\2303.html:text/html},
}

@article{luz_multilingual_2023,
	title = {Multilingual Alzheimer's Dementia Recognition through Spontaneous Speech: a Signal Processing Grand Challenge},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2301.05562},
	doi = {10.48550/ARXIV.2301.05562},
	shorttitle = {Multilingual Alzheimer's Dementia Recognition through Spontaneous Speech},
	abstract = {This Signal Processing Grand Challenge ({SPGC}) targets a difficult automatic prediction problem of societal and medical relevance, namely, the detection of Alzheimer's Dementia ({AD}). Participants were invited to employ signal processing and machine learning methods to create predictive models based on spontaneous speech data. The Challenge has been designed to assess the extent to which predictive models built based on speech in one language (English) generalise to another language (Greek). To the best of our knowledge no work has investigated acoustic features of the speech signal in multilingual {AD} detection. Our baseline system used conventional machine learning algorithms with Active Data Representation of acoustic features, achieving accuracy of 73.91\% on {AD} detection, and 4.95 root mean squared error on cognitive score prediction.},
	author = {Luz, Saturnino and Haider, Fasih and Fromm, Davida and Lazarou, Ioulietta and Kompatsiaris, Ioannis and {MacWhinney}, Brian},
	urldate = {2023-03-22},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {68T10 (Primary) 92C55 (Secondary), Artificial Intelligence (cs.{AI}), Audio and Speech Processing (eess.{AS}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, {FOS}: Electrical engineering, electronic engineering, information engineering, J.3; I.2.6; I.5.1, Machine Learning (cs.{LG})},
}

@incollection{messinger_affective_2015,
	location = {New York, {NY}, {US}},
	title = {Affective computing, emotional development, and autism},
	isbn = {978-0-19-994223-7},
	series = {Oxford library of psychology},
	abstract = {Affective computing can illuminate early emotional dynamics and provide tools for intervention in disordered emotional functioning. This chapter reviews affective computing approaches to understanding emotional communication in typically developing children and children with an autism spectrum disorder ({ASD}). It covers the application of automated measurement of the dynamics of emotional expression and discusses advances in the modeling of infant and parent interactions based on insights from time-series analysis, machine learning, and recurrence theory. The authors discuss progress in the automated measurement of vocalization in infants and children and new methods for the efficient measurement of sympathetic activation and its application in children with {ASD}. They conclude by presenting translational applications of affective computing to children with {ASD}, including the use of embodied conversational agents ({ECAs}) to understand and influence the affective dynamics of learning, and the use of robots to improve the social and emotional functioning of children with {ASD}. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {516--536},
	booktitle = {The Oxford handbook of affective computing},
	publisher = {Oxford University Press},
	author = {Messinger, Daniel S. and Duvivier, Leticia Lobo and Warren, Zachary E. and Mahoor, Mohammad and Baker, Jason and Warlaumont, Anne and Ruvolo, Paul},
	date = {2015},
	doi = {10.1093/oxfordhb/9780199942237.013.012},
	keywords = {Machine Learning, Affective Computing, Autism Spectrum Disorders, Childhood Development, Human Computer Interaction, Robotics},
	file = {Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\IY92BDZR\\2014-43075-039.html:text/html},
}

@article{ortega-llebaria_chinese-english_2021,
	title = {Chinese-English Speakers’ Perception of Pitch in Their Non-Tonal Language: Reinterpreting English as a Tonal-Like Language},
	volume = {64},
	issn = {0023-8309, 1756-6053},
	url = {http://journals.sagepub.com/doi/10.1177/0023830919894606},
	doi = {10.1177/0023830919894606},
	shorttitle = {Chinese-English Speakers’ Perception of Pitch in Their Non-Tonal Language},
	abstract = {Changing the F0-contour of English words does not change their lexical meaning. However, it changes the meaning in tonal languages such as Mandarin. Given this important difference and knowing that words in the two languages of a bilingual lexicon interact, the question arises as to how {MandarinEnglish} speakers process pitch in their bilingual lexicon. The few studies that addressed this question showed that Mandarin-English speakers did not perceive pitch in English words as native English speakers did. These studies, however, used English words as stimuli failing to examine nonwords and Mandarin words. Consequently, possible pre-lexical effects and L1 transfer were not ruled out. The present study fills this gap by examining pitch perception in Mandarin and English words and nonwords by Mandarin-English speakers and a group of native English controls. Results showed the tonal experience of Chinese-English speakers modulated their perception of pitch in their non-tonal language at both pre-lexical and lexical levels. In comparison to native English controls, tonal speakers were more sensitive to the acoustic salience of F0-contours in the pre-lexical processing due to top-down feedback. At the lexical level, Mandarin-English speakers organized words in their two languages according to similarity criteria based on both F0 and segmental information, whereas only the segmental information was relevant to the control group. These results in perception together with consistently reported production patterns in previous literature suggest that Mandarin-English speakers process pitch in English as if it was a one-tone language.},
	pages = {467--487},
	number = {2},
	journaltitle = {Lang Speech},
	author = {Ortega-Llebaria, Marta and Wu, Zhaohong},
	urldate = {2023-03-22},
	date = {2021-06},
	langid = {english},
	file = {Ortega-Llebaria and Wu - 2021 - Chinese-English Speakers’ Perception of Pitch in T.pdf:C\:\\Users\\nicol\\Zotero\\storage\\YJ95QF57\\Ortega-Llebaria and Wu - 2021 - Chinese-English Speakers’ Perception of Pitch in T.pdf:application/pdf},
}

@article{kano_end--end_2018,
	title = {An end-to-end model for cross-lingual transformation of paralinguistic information},
	volume = {32},
	issn = {1573-0573},
	url = {https://doi.org/10.1007/s10590-018-9217-7},
	doi = {10.1007/s10590-018-9217-7},
	abstract = {Speech translation is a technology that helps people communicate across different languages. The most commonly used speech translation model is composed of automatic speech recognition, machine translation and text-to-speech synthesis components, which share information only at the text level. However, spoken communication is different from written communication in that it uses rich acoustic cues such as prosody in order to transmit more information through non-verbal channels. This paper is concerned with speech-to-speech translation that is sensitive to this paralinguistic information. Our long-term goal is to make a system that allows users to speak a foreign language with the same expressiveness as if they were speaking in their own language. Our method works by reconstructing input acoustic features in the target language. From the many different possible paralinguistic features to handle, in this paper we choose duration and power as a first step, proposing a method that can translate these features from input speech to the output speech in continuous space. This is done in a simple and language-independent fashion by training an end-to-end model that maps source-language duration and power information into the target language. Two approaches are investigated: linear regression and neural network models. We evaluate the proposed methods and show that paralinguistic information in the input speech of the source language can be reflected in the output speech of the target language.},
	pages = {353--368},
	number = {4},
	journaltitle = {Machine Translation},
	author = {Kano, Takatomo and Takamichi, Shinnosuke and Sakti, Sakriani and Neubig, Graham and Toda, Tomoki and Nakamura, Satoshi},
	urldate = {2023-03-22},
	date = {2018-12-01},
	langid = {english},
	keywords = {★, Automatic speech recognition, Machine translation, Paralinguistic information, Speech to speech translation, Text to speech synthesis},
	file = {Full Text PDF:C\:\\Users\\nicol\\Zotero\\storage\\W3HUAYK4\\Kano et al. - 2018 - An end-to-end model for cross-lingual transformati.pdf:application/pdf},
}

@online{mcwhorter_worlds_2015,
	title = {The World's Most Musical Languages},
	url = {https://www.theatlantic.com/international/archive/2015/11/tonal-languages-linguistics-mandarin/415701/},
	abstract = {Why one syllable spoken at different pitches can have seven meanings},
	titleaddon = {The Atlantic},
	author = {{McWhorter}, John},
	urldate = {2023-03-22},
	date = {2015-11-13},
	langid = {english},
	note = {Section: Global},
	file = {Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\DR4MQ5Z3\\415701.html:text/html},
}

@article{wynn_rhythm_2022,
	title = {Rhythm Perception, Speaking Rate Entrainment, and Conversational Quality: A Mediated Model},
	volume = {65},
	issn = {1092-4388, 1558-9102},
	url = {http://pubs.asha.org/doi/10.1044/2022_JSLHR-21-00293},
	doi = {10.1044/2022_JSLHR-21-00293},
	shorttitle = {Rhythm Perception, Speaking Rate Entrainment, and Conversational Quality},
	abstract = {Purpose: Acoustic–prosodic entrainment, defined as the tendency for individuals to modify their speech behaviors to more closely align with the behaviors of their conversation partner, plays an important role in successful interaction. From a mechanistic perspective, acoustic–prosodic entrainment is, by its very nature, a rhythmic activity. Accordingly, it is highly plausible that an individual’s rhythm perception abilities play a role in their ability to successfully entrain. Here, we examine the impact of rhythm perception in speaking rate entrainment and subsequent conversational quality.
Method: A round-robin paradigm was used to collect 90 dialogues from neurotypical adults. Additional assessments determined participants’ rhythm perception abilities, social competence, and partner familiarity (i.e., whether the conversation partners knew each other prior to the interaction. Mediation analysis was used to examine the relationships between rhythm perception scores, speaking rate entrainment (using a measure of static local synchrony), and a measure of conversational success (i.e., conversational quality) based on thirdparty listener observations. Findings were compared to the same analysis with three additional predictive factors: participant gender, partner familiarity, and social competence.
Results: Results revealed a relationship between rhythm perception and speaking rate entrainment. In unfamiliar conversation partners, there was a relationship between speaking rate entrainment and conversational quality. The relationships between entrainment and each of the three additional factors (i.e., gender, partner familiarity, and social competence) were nonsignificant.
Conclusions: In unfamiliar conversation partners, better rhythm perception abilities were indicative of increased conversational quality mediated by higher levels of speaking rate entrainment. These results support theoretical postulations specifying rhythm perception abilities as a component of acoustic–prosodic entrainment, which, in turn, facilitates conversational success. Knowledge of this relationship contributes to the development of a causal framework for considering a mechanism by which rhythm perception deficits in clinical populations may impact conversational success.},
	pages = {2187--2203},
	number = {6},
	journaltitle = {J Speech Lang Hear Res},
	author = {Wynn, Camille J. and Barrett, Tyson S. and Borrie, Stephanie A.},
	urldate = {2023-03-23},
	date = {2022-06-08},
	langid = {english},
	file = {Wynn et al. - 2022 - Rhythm Perception, Speaking Rate Entrainment, and .pdf:C\:\\Users\\nicol\\Zotero\\storage\\53SCS4FV\\Wynn et al. - 2022 - Rhythm Perception, Speaking Rate Entrainment, and .pdf:application/pdf},
}

@article{fernald_intonation_1989,
	title = {Intonation and Communicative Intent in Mothers' Speech to Infants: Is the Melody the Message?},
	volume = {60},
	issn = {00093920},
	url = {https://www.jstor.org/stable/1130938?origin=crossref},
	doi = {10.2307/1130938},
	shorttitle = {Intonation and Communicative Intent in Mothers' Speech to Infants},
	pages = {1497},
	number = {6},
	journaltitle = {Child Development},
	author = {Fernald, Anne},
	urldate = {2023-03-23},
	date = {1989-12},
	langid = {english},
	file = {Fernald - 1989 - Intonation and Communicative Intent in Mothers' Sp.pdf:C\:\\Users\\nicol\\Zotero\\storage\\3UUD9UV2\\Fernald - 1989 - Intonation and Communicative Intent in Mothers' Sp.pdf:application/pdf},
}

@article{fernald_cross-language_1989,
	title = {A cross-language study of prosodic modifications in mothers' and fathers' speech to preverbal infants},
	volume = {16},
	issn = {0305-0009, 1469-7602},
	url = {https://www.cambridge.org/core/product/identifier/S0305000900010679/type/journal_article},
	doi = {10.1017/S0305000900010679},
	abstract = {{ABSTRACT}
            
              This study compares the prosodie modifications in mothers' and fathers' speech to preverbal infants in French, Italian, German, Japanese, British English, and American English. At every stage of data collection and analysis, standardized procedures were used to enhance the comparability across data sets that is essential for valid cross-language comparison of the prosodie features of parental speech. In each of the six language groups, five mothers and five fathers were recorded in semi-structured home observations while speaking to their infant aged 0; 10–1;2 and to an adult. Speech samples were instrumentally analysed to measure seven prosodic parameters: mean fundamental frequency (f
              0
              ), f
              0
              -minimum, f
              0
              -maximum, f
              0
              -range, f
              0
              -variability, utterance duration, and pause duration. Results showed cross-language consistency in the patterns of prosodic modification used in parental speech to infants. Across languages, both mothers and fathers used higher mean-f
              0
              , f
              0
              -minimum, and f
              0
              -maximum, greater f
              0
              -variability, shorter utterances, and longer pauses in infant-directed speech than in adult-directed speech. Mothers, but not fathers, used a wider f
              0
              -range in speech to infants. American English parents showed the most extreme prosodic modifications, differing from the other language groups in the extent of intonational exaggeration in Speech to infants. These results reveal common patterns in caretaker's use of intonation across languages, which may function developmentally to regulate infant arousal and attention, to communicate affect, and to facilitate speech perception and language comprehension. In addition to providing evidence for possibly universal prosodic features of speech to infants, these results suggest that language-specific variations are also important, and that the findings of the numerous studies of early language input based on American English are not necessarily generalisable to other cultures.},
	pages = {477--501},
	number = {3},
	journaltitle = {J. Child Lang.},
	author = {Fernald, Anne and Taeschner, Traute and Dunn, Judy and Papousek, Mechthild and de Boysson-Bardies, Bénédicte and Fukui, Ikuko},
	urldate = {2023-03-23},
	date = {1989-10},
	langid = {english},
	file = {Fernald et al. - 1989 - A cross-language study of prosodic modifications i.pdf:C\:\\Users\\nicol\\Zotero\\storage\\UUG4BZNF\\Fernald et al. - 1989 - A cross-language study of prosodic modifications i.pdf:application/pdf},
}

@article{wynn_rhythm_2022-1,
	title = {Rhythm Perception, Speaking Rate Entrainment, and Conversational Quality: A Mediated Model},
	volume = {65},
	doi = {10.1044/2022_JSLHR-21-00293},
	pages = {1--17},
	journaltitle = {Journal of Speech, Language, and Hearing Research},
	author = {Wynn, Camille and Barrett, Tyson and Borrie, Stephanie},
	date = {2022-05-26},
}

@incollection{cameron_perception_2020,
	title = {Perception of Rhythm},
	isbn = {978-1-108-49292-8},
	pages = {20--38},
	author = {Cameron, Daniel and Grahn, Jessica},
	date = {2020-09-24},
	doi = {10.1017/9781108631730.004},
	keywords = {★},
	file = {Full Text PDF:C\:\\Users\\nicol\\Zotero\\storage\\V4BNM9FQ\\Cameron and Grahn - 2020 - Perception of Rhythm.pdf:application/pdf},
}

@article{bolton_rhythm_1894,
	title = {Rhythm},
	volume = {6},
	issn = {1939-8298},
	doi = {10.2307/1410948},
	abstract = {Determined what the mind did with a series of simple auditory impressions in which there was absolutely no change of intensity, pitch, quality or time-interval; and what values these properties of sound have in forming a rhythmical series. Some of the manifestations of rhythm are: rhythms in nature; physiological rhythms; attention and periodicity; rhythmic speech; time-relations; intensity and quality of sounds; emotional effects of rhythm upon savages and children; and the place of rhythm in music and poetry. The apparatus were an electric telephone, and a chronograph. Introspections of 30 Ss were collected. Results show that a given number of auditory impressions within certain time limits, when presented with a kind of subordination among them with respect to time, intensity pitch or quality or with respect to any two or more of the properties, always stood as a unit in consciousness. ({PsycINFO} Database Record (c) 2017 {APA}, all rights reserved)},
	pages = {145--238},
	journaltitle = {The American Journal of Psychology},
	author = {Bolton, Thaddeus L.},
	date = {1894},
	note = {Place: {US}
Publisher: Univ of Illinois Press},
	file = {Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\JDEM4PVE\\1926-00084-001.html:text/html},
}

@article{ververidis_emotional_2006,
	title = {Emotional speech recognition: Resources, features, and methods},
	volume = {48},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639306000422},
	doi = {10.1016/j.specom.2006.04.003},
	shorttitle = {Emotional speech recognition},
	abstract = {In this paper we overview emotional speech recognition having in mind three goals. The ﬁrst goal is to provide an up-todate record of the available emotional speech data collections. The number of emotional states, the language, the number of speakers, and the kind of speech are brieﬂy addressed. The second goal is to present the most frequent acoustic features used for emotional speech recognition and to assess how the emotion aﬀects them. Typical features are the pitch, the formants, the vocal tract cross-section areas, the mel-frequency cepstral coeﬃcients, the Teager energy operator-based features, the intensity of the speech signal, and the speech rate. The third goal is to review appropriate techniques in order to classify speech into emotional states. We examine separately classiﬁcation techniques that exploit timing information from which that ignore it. Classiﬁcation techniques based on hidden Markov models, artiﬁcial neural networks, linear discriminant analysis, k-nearest neighbors, support vector machines are reviewed.},
	pages = {1162--1181},
	number = {9},
	journaltitle = {Speech Communication},
	author = {Ververidis, Dimitrios and Kotropoulos, Constantine},
	urldate = {2023-03-23},
	date = {2006-09},
	langid = {english},
	file = {Ververidis and Kotropoulos - 2006 - Emotional speech recognition Resources, features,.pdf:C\:\\Users\\nicol\\Zotero\\storage\\MM8RCBB9\\Ververidis and Kotropoulos - 2006 - Emotional speech recognition Resources, features,.pdf:application/pdf},
}

@article{paiva_empathy_2011,
	title = {Empathy in Social Agents},
	volume = {10},
	issn = {1081-1451},
	url = {https://ijvr.eu/article/view/2794},
	doi = {10.20870/IJVR.2011.10.1.2794},
	abstract = {Empathy is seen as the capacity to perceive, understand and experience others' emotions. This notion is often considered as one of the major elements in social interactions between humans. As such, when creating social agents, that are believable and able to engage users in social interactions, empathy needs to be addressed. Indeed, for the past few years, many researchers have been looking at this problem, not only in trying to find ways to perceive the user's emotions, but also to adapt to them, and react in an empathic way. This paper provides a small overview of this new challenging area of research, by analyzing empathy in the social relations established between humans and social agents, and providing a concrete model for the creation of empathic social agents.},
	pages = {1--4},
	number = {1},
	journaltitle = {{IJVR}},
	author = {Paiva, Ana},
	urldate = {2023-03-29},
	date = {2011-01-01},
	langid = {english},
	file = {Paiva - 2011 - Empathy in Social Agents.pdf:C\:\\Users\\nicol\\Zotero\\storage\\5AXM8EY3\\Paiva - 2011 - Empathy in Social Agents.pdf:application/pdf},
}

@incollection{guthier_affective_2016,
	location = {Cham},
	title = {Affective Computing in Games},
	isbn = {978-3-319-46152-6},
	url = {https://doi.org/10.1007/978-3-319-46152-6_16},
	abstract = {Being able to automatically recognize and interpret the affective state of the player can have various benefits in a Serious Game. The difficulty and pace of a learning game could be adapted, or the quality of the interaction between the player and the game could be improved – just to name two examples. This Chapter aims to give an introduction to Affective Computing with the goal of helping developers to incorporate the player’s affective data into the games. Suitable psychological models of emotion and personality are described, and a multitude of sensors as well as methods to recognize affect are discussed in detail. The Chapter ends with a number examples where human affect is utilized in Serious Games.},
	pages = {402--441},
	booktitle = {Entertainment Computing and Serious Games: International {GI}-Dagstuhl Seminar 15283, Dagstuhl Castle, Germany, July 5-10, 2015, Revised Selected Papers},
	publisher = {Springer International Publishing},
	author = {Guthier, Benjamin and Dörner, Ralf and Martinez, Hector P.},
	editor = {Dörner, Ralf and Göbel, Stefan and Kickmeier-Rust, Michael and Masuch, Maic and Zweig, Katharina},
	date = {2016},
	doi = {10.1007/978-3-319-46152-6_16},
}

@article{brave_computers_2005,
	title = {Computers that care: investigating the effects of orientation of emotion exhibited by an embodied computer agent},
	volume = {62},
	issn = {1071-5819},
	url = {https://www.sciencedirect.com/science/article/pii/S1071581904001284},
	doi = {10.1016/j.ijhcs.2004.11.002},
	abstract = {Embodied computer agents are becoming an increasingly popular human–computer interaction technique. Often, these agents are programmed with the capacity for emotional expression. This paper investigates the psychological effects of emotion in agents upon users. In particular, two types of emotion were evaluated: self-oriented emotion and other-oriented, empathic emotion. In a 2 (self-oriented emotion: absent vs. present) by 2 (empathic emotion: absent vs. present) by 2 (gender dyad: male vs. female) between-subjects experiment (N=96), empathic emotion was found to lead to more positive ratings of the agent by users, including greater likeability and trustworthiness, as well as greater perceived caring and felt support. No such effect was found for the presence of self-oriented emotion. Implications for the design of embodied computer agents are discussed and directions for future research suggested.},
	pages = {161--178},
	number = {2},
	journaltitle = {International Journal of Human-Computer Studies},
	author = {Brave, Scott and Nass, Clifford and Hutchinson, Kevin},
	date = {2005-02-01},
	keywords = {Affective computing, Empathy, Characters, Embodied agents, Emotion, Empirical studies, Social interfaces},
}

@article{paiva_caring_2004,
	title = {Caring for Agents and Agents that Care: Building Empathic Relations with Synthetic Agents},
	volume = {1},
	issn = {0-7695-2092-8},
	doi = {10.1109/AAMAS.2004.82},
	shorttitle = {Caring for Agents and Agents that Care},
	abstract = {When building agents and synthetic characters, and in order to achieve believability, we must consider the emotional relations established between users and characters, that is, we must consider the issue of "empathy". Defined in broad terms as "An observer reacting emotionally because he perceives that another is experiencing or about to experience an emotion", empathy is an important element to consider in the creation of relations between humans and agents. In this paper we will focus on the role of empathy in the construction of synthetic characters, providing some requirements for such construction and illustrating the presented concepts with a specific system called {FearNot}!. {FearNot}! was developed to address the difficult and often devastating problem of bullying in schools. By using role playing and empathic synthetic characters in a 3D environment, {FearNot}! allows children from 8 to 12 to experience a virtual scenario where they can witness (in a third-person perspective) bullying situations. To build empathy into {FearNot}! we have considered the following components: agent?s architecture; the characters? embodiment and emotional expression; proximity with the user and emotionally charged situations.We will describe how these were implemented in {FearNot}! and report on the preliminary results we have with it.},
	pages = {194--201},
	journaltitle = {Autonomous Agents and Multiagent Systems, International Joint Conference on},
	author = {Paiva, Ana and Dias, João and Sobral, Daniel and Aylett, Ruth and Sobreperez, Polly and Woods, Sarah and Zoll, Carsten and Hall, Lynne},
	date = {2004-01-01},
	file = {Full Text PDF:C\:\\Users\\nicol\\Zotero\\storage\\XEB7VEVL\\Paiva et al. - 2004 - Caring for Agents and Agents that Care Building E.pdf:application/pdf},
}

@online{noauthor_short_2022,
	title = {A Short History Of Text-to-Speech {\textbar} Speechify},
	url = {https://speechify.com/blog/history-of-text-to-speech/},
	abstract = {Text-to-speech technology has changed the way many people read and digest content. Learn more about how automated text voice readers have evolved over time.},
	urldate = {2023-04-03},
	date = {2022-06-27},
	langid = {american},
	note = {Section: Learning},
	file = {Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\66KVUG5R\\history-of-text-to-speech.html:text/html},
}

@misc{hsu_hierarchical_2018,
	title = {Hierarchical Generative Modeling for Controllable Speech Synthesis},
	url = {http://arxiv.org/abs/1810.07217},
	abstract = {This paper proposes a neural sequence-to-sequence text-to-speech ({TTS}) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model based on the variational autoencoder ({VAE}) framework, with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model ({GMM}) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, we train a high-quality controllable {TTS} model on real found data, which is capable of inferring speaker and style attributes from a noisy utterance and use it to synthesize clean speech with controllable speaking style.},
	number = {{arXiv}:1810.07217},
	publisher = {{arXiv}},
	author = {Hsu, Wei-Ning and Zhang, Yu and Weiss, Ron J. and Zen, Heiga and Wu, Yonghui and Wang, Yuxuan and Cao, Yuan and Jia, Ye and Chen, Zhifeng and Shen, Jonathan and Nguyen, Patrick and Pang, Ruoming},
	urldate = {2023-04-03},
	date = {2018-12-27},
	eprinttype = {arxiv},
	eprint = {1810.07217 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:C\:\\Users\\nicol\\Zotero\\storage\\TGFC3XBQ\\Hsu et al. - 2018 - Hierarchical Generative Modeling for Controllable .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\PRJF62L4\\1810.html:text/html},
}

@misc{um_emotional_2019,
	title = {Emotional speech synthesis with rich and granularized control},
	url = {http://arxiv.org/abs/1911.01635},
	abstract = {This paper proposes an effective emotion control method for an end-to-end text-to-speech ({TTS}) system. To flexibly control the distinct characteristic of a target emotion category, it is essential to determine embedding vectors representing the {TTS} input. We introduce an inter-to-intra emotional distance ratio algorithm to the embedding vectors that can minimize the distance to the target emotion category while maximizing its distance to the other emotion categories. To further enhance the expressiveness of a target speech, we also introduce an effective interpolation technique that enables the intensity of a target emotion to be gradually changed to that of neutral speech. Subjective evaluation results in terms of emotional expressiveness and controllability show the superiority of the proposed algorithm to the conventional methods.},
	number = {{arXiv}:1911.01635},
	publisher = {{arXiv}},
	author = {Um, Se-Yun and Oh, Sangshin and Byun, Kyungguen and Jang, Inseon and Ahn, Chunghyun and Kang, Hong-Goo},
	urldate = {2023-04-04},
	date = {2019-11-05},
	eprinttype = {arxiv},
	eprint = {1911.01635 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
	file = {arXiv Fulltext PDF:C\:\\Users\\nicol\\Zotero\\storage\\YJD7Y53T\\Um et al. - 2019 - Emotional speech synthesis with rich and granulari.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\2UTG3GPD\\1911.html:text/html},
}

@misc{kwon_effective_2019,
	title = {Effective parameter estimation methods for an {ExcitNet} model in generative text-to-speech systems},
	url = {http://arxiv.org/abs/1905.08486},
	doi = {10.48550/arXiv.1905.08486},
	abstract = {In this paper, we propose a high-quality generative text-to-speech ({TTS}) system using an effective spectrum and excitation estimation method. Our previous research verified the effectiveness of the {ExcitNet}-based speech generation model in a parametric {TTS} framework. However, the challenge remains to build a high-quality speech synthesis system because auxiliary conditional features estimated by a simple deep neural network often contain large prediction errors, and the errors are inevitably propagated throughout the autoregressive generation process of the {ExcitNet} vocoder. To generate more natural speech signals, we exploited a sequence-to-sequence (seq2seq) acoustic model with an attention-based generative network (e.g., Tacotron 2) to estimate the condition parameters of the {ExcitNet} vocoder. Because the seq2seq acoustic model accurately estimates spectral parameters, and because the {ExcitNet} model effectively generates the corresponding time-domain excitation signals, combining these two models can synthesize natural speech signals. Furthermore, we verified the merit of the proposed method in producing expressive speech segments by adopting a global style token-based emotion embedding method. The experimental results confirmed that the proposed system significantly outperforms the systems with a similarly configured conventional {WaveNet} vocoder and our best prior parametric {TTS} counterpart.},
	number = {{arXiv}:1905.08486},
	publisher = {{arXiv}},
	author = {Kwon, Ohsung and Song, Eunwoo and Kim, Jae-Min and Kang, Hong-Goo},
	urldate = {2023-04-04},
	date = {2019-05-21},
	eprinttype = {arxiv},
	eprint = {1905.08486 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:C\:\\Users\\nicol\\Zotero\\storage\\HP4WFKVK\\Kwon et al. - 2019 - Effective parameter estimation methods for an Exci.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\EVJHWQK2\\1905.html:text/html},
}

@misc{lee_emotional_2017,
	title = {Emotional End-to-End Neural Speech Synthesizer},
	url = {http://arxiv.org/abs/1711.05447},
	doi = {10.48550/arXiv.1711.05447},
	abstract = {In this paper, we introduce an emotional speech synthesizer based on the recent end-to-end neural model, named Tacotron. Despite its benefits, we found that the original Tacotron suffers from the exposure bias problem and irregularity of the attention alignment. Later, we address the problem by utilization of context vector and residual connection at recurrent neural networks ({RNNs}). Our experiments showed that the model could successfully train and generate speech for given emotion labels.},
	number = {{arXiv}:1711.05447},
	publisher = {{arXiv}},
	author = {Lee, Younggun and Rabiee, Azam and Lee, Soo-Young},
	urldate = {2023-04-04},
	date = {2017-11-27},
	eprinttype = {arxiv},
	eprint = {1711.05447 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Sound},
	file = {arXiv Fulltext PDF:C\:\\Users\\nicol\\Zotero\\storage\\GFD5BJ9G\\Lee et al. - 2017 - Emotional End-to-End Neural Speech Synthesizer.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\WM7XUAFI\\1711.html:text/html},
}

@misc{kim_conditional_2021,
	title = {Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech},
	url = {http://arxiv.org/abs/2106.06103},
	abstract = {Several recent end-to-end text-to-speech ({TTS}) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage {TTS} systems. In this work, we present a parallel end-to-end {TTS} method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or {MOS}) on the {LJ} Speech, a single speaker dataset, shows that our method outperforms the best publicly available {TTS} systems and achieves a {MOS} comparable to ground truth.},
	number = {{arXiv}:2106.06103},
	publisher = {{arXiv}},
	author = {Kim, Jaehyeon and Kong, Jungil and Son, Juhee},
	urldate = {2023-04-04},
	date = {2021-06-10},
	eprinttype = {arxiv},
	eprint = {2106.06103 [cs, eess]},
	keywords = {★, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound, {VITS}},
	file = {arXiv Fulltext PDF:C\:\\Users\\nicol\\Zotero\\storage\\WCKZTS7N\\Kim et al. - 2021 - Conditional Variational Autoencoder with Adversari.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\5JYQR3ZM\\2106.html:text/html},
}

@online{hughes_azure_2021,
	title = {Azure {AI} milestone: New Neural Text-to-Speech models more closely mirror natural speech},
	url = {https://www.microsoft.com/en-us/research/blog/azure-ai-milestone-new-neural-text-to-speech-models-more-closely-mirror-natural-speech/},
	shorttitle = {Azure {AI} milestone},
	abstract = {Neural Text-to-Speech—along with recent milestones in computer vision and question answering—is part of a larger Azure {AI} mission to provide relevant, meaningful {AI} solutions and services that work better for people because they better capture how people learn and work—with improved vision, knowledge understanding, and speech capabilities. At the center of these efforts is {XYZ}-code, […]},
	titleaddon = {Microsoft Research},
	author = {Hughes, Alyssa},
	urldate = {2023-04-05},
	date = {2021-12-17},
	langid = {american},
	file = {Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\LV6T394B\\azure-ai-milestone-new-neural-text-to-speech-models-more-closely-mirror-natural-speech.html:text/html},
}

@inproceedings{chu_objective_2001,
	title = {An objective measure for estimating {MOS} of synthesized speech},
	url = {https://www.isca-speech.org/archive/eurospeech_2001/chu01b_eurospeech.html},
	doi = {10.21437/Eurospeech.2001-492},
	abstract = {This paper proposes an average concatenative cost function as the objective measure for naturalness of synthesized speech. All its seven component-costs can be derived directly from the input text and the scripts of speech database. A formal Mean Opinion Score ({MOS}) experiment shows that the average concatenative cost and its seven components are all highly correlated with {MOS} obtained subjectively. The correlation coefficient between the objective measure and subjective measure is –0.872. The mean of errors in {MOS} estimation for individual waveforms is 0.32 with 0.40 {RMSE}. When estimating the overall {MOS} for {TTS} systems, the mean error is smaller than 0.05. With the proposed objective measure, it becomes possible and easy for us to track the performance in naturalness regularly. The proposed cost function could also serve as criteria for optimizing the algorithms for unit selecting and speech database pruning.},
	eventtitle = {7th European Conference on Speech Communication and Technology (Eurospeech 2001)},
	pages = {2087--2090},
	booktitle = {7th European Conference on Speech Communication and Technology (Eurospeech 2001)},
	publisher = {{ISCA}},
	author = {Chu, Min and Peng, Hu},
	urldate = {2023-04-05},
	date = {2001-09-03},
	langid = {english},
	file = {Chu and Peng - 2001 - An objective measure for estimating MOS of synthes.pdf:C\:\\Users\\nicol\\Zotero\\storage\\W2T47NZA\\Chu and Peng - 2001 - An objective measure for estimating MOS of synthes.pdf:application/pdf},
}

@online{ito_lj_nodate,
	title = {The {LJ} Speech Dataset},
	url = {https://keithito.com/LJ-Speech-Dataset},
	abstract = {A public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip.},
	author = {Ito, Keith and Johnson, Linda},
	urldate = {2023-04-05},
	file = {Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\I2ER3NFZ\\LJ-Speech-Dataset.html:text/html},
}

@online{karagiannakos_speech_2021,
	title = {Speech synthesis: A review of the best text to speech architectures with Deep Learning},
	url = {https://theaisummer.com/text-to-speech/},
	shorttitle = {Speech synthesis},
	abstract = {Explore the most popular deep learning models to perform text to speech ({TTS}) synthesis},
	titleaddon = {{AI} Summer},
	author = {Karagiannakos, Sergios},
	urldate = {2023-04-05},
	date = {2021-05-13},
	langid = {english},
	file = {Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\TSABEJBR\\text-to-speech.html:text/html},
}

@online{noauthor_papers_2023,
	title = {Papers with Code - {LJSpeech} Benchmark (Text-To-Speech Synthesis)},
	url = {https://paperswithcode.com/sota/text-to-speech-synthesis-on-ljspeech},
	abstract = {The current state-of-the-art on {LJSpeech} is {NaturalSpeech}. See a full comparison of 14 papers with code.},
	urldate = {2023-04-05},
	date = {2023},
	langid = {english},
	file = {Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\7K4CXU6M\\text-to-speech-synthesis-on-ljspeech.html:text/html},
}

@article{viswanathan_measuring_2005,
	title = {Measuring speech quality for text-to-speech systems: development and assessment of a modified mean opinion score ({MOS}) scale},
	volume = {19},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230803000676},
	doi = {10.1016/j.csl.2003.12.001},
	shorttitle = {Measuring speech quality for text-to-speech systems},
	pages = {55--83},
	number = {1},
	journaltitle = {Computer Speech \& Language},
	author = {Viswanathan, Mahesh and Viswanathan, Madhubalan},
	urldate = {2023-04-05},
	date = {2005-01},
	langid = {english},
}

@incollection{neumann_measures_2015,
	title = {Measures of Empathy},
	isbn = {978-0-12-386915-9},
	abstract = {This chapter reviews the major approaches to measure empathy, considers their validity and reliability, and the relative advantages and disadvantages of each approach. Empathy is a complex multifaceted construct that is important for interpersonal relationships and social functioning in normal and pathological populations. This complexity is also reflected in the approaches used to measure empathy. The approaches can be categorized as self-report questionnaires, behavioral measures, and neuroscientific measures. Some measurement approaches focus more on the affective components of empathy, others focus more on the cognitive components, and some take a multidimensional perspective. Measures also vary according to whether they are used in clinical or medical contexts or with younger age groups. While self-report measures are the most commonly used and well-validated, behavioral and neuroscientific measures are becoming increasingly popular in research and practice.},
	pages = {257--289},
	author = {Neumann, David and Chan, Raymond and Boyle, Gregory J. and Wang, Yi and Westbury, Rae},
	date = {2015-12-31},
	doi = {10.1016/B978-0-12-386915-9.00010-3},
	file = {Full Text PDF:C\:\\Users\\nicol\\Zotero\\storage\\LSATPQ5E\\Neumann et al. - 2015 - Measures of Empathy.pdf:application/pdf},
}

@unpublished{mehrabian_manual_1996,
	location = {1130 Alta Mesa Road, Monterey, {CA}, {USA} 93940},
	title = {Manual for the Balanced Emotional Empathy Scale ({BEES}).},
	author = {Mehrabian, Albert},
	date = {1996},
}

@article{spreng_toronto_2009,
	title = {The Toronto Empathy Questionnaire: Scale development and initial validation of a factor-analytic solution to multiple empathy measures},
	volume = {91},
	issn = {1532-7752},
	doi = {10.1080/00223890802484381},
	shorttitle = {The Toronto Empathy Questionnaire},
	abstract = {To formulate a parsimonious tool to assess empathy, we used factor analysis on a combination of self-report measures to examine consensus and developed a brief self-report measure of this common factor. The Toronto Empathy Questionnaire ({TEQ}) represents empathy as a primarily emotional process. In 3 studies, the {TEQ} demonstrated strong convergent validity, correlating positively with behavioral measures of social decoding, self-report measures of empathy, and negatively with a measure of Autism symptomatology. Moreover, it exhibited good internal consistency and high test-retest reliability. The {TEQ} is a brief, reliable, and valid instrument for the assessment of empathy. ({PsycINFO} Database Record (c) 2018 {APA}, all rights reserved)},
	pages = {62--71},
	journaltitle = {Journal of Personality Assessment},
	author = {Spreng, R. Nathan and {McKinnon}, Margaret C. and Mar, Raymond A. and Levine, Brian},
	date = {2009},
	note = {Place: United Kingdom
Publisher: Taylor \& Francis},
	keywords = {Empathy, Questionnaires, Self-Report, Test Construction, Test Validity},
	file = {Accepted Version:C\:\\Users\\nicol\\Zotero\\storage\\Q7RS2PSL\\Spreng et al. - 2009 - The Toronto Empathy Questionnaire Scale developme.pdf:application/pdf;Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\LV36253S\\2009-09333-011.html:text/html},
}

@article{reniers_qcae_2011,
	title = {The {QCAE}: a Questionnaire of Cognitive and Affective Empathy},
	volume = {93},
	issn = {1532-7752},
	doi = {10.1080/00223891.2010.528484},
	shorttitle = {The {QCAE}},
	abstract = {Empathy has been inconsistently defined and inadequately measured. This research aimed to produce a new and rigorously developed questionnaire. Exploratory (n₁ = 640) and confirmatory (n₂ = 318) factor analyses were employed to develop the Questionnaire of Cognitive and Affective Empathy ({QCAE}). Principal components analysis revealed 5 factors (31 items). Confirmatory factor analysis confirmed this structure in an independent sample. The hypothesized 2-factor structure (cognitive and affective empathy) was tested and provided the best and most parsimonious fit to the data. Gender differences, convergent validity, and construct validity were examined. The {QCAE} is a valid tool for assessing cognitive and affective empathy.},
	pages = {84--95},
	number = {1},
	journaltitle = {J Pers Assess},
	author = {Reniers, Renate L. E. P. and Corcoran, Rhiannon and Drake, Richard and Shryane, Nick M. and Völlm, Birgit A.},
	date = {2011-01},
	pmid = {21184334},
	keywords = {Empathy, Adolescent, Adult, Aged, Anger, Cognition, Factor Analysis, Statistical, Female, Humans, Interpersonal Relations, Male, Middle Aged, Mood Disorders, Personality Inventory, Self-Assessment, Sex Distribution, Students, Surveys and Questionnaires, United Kingdom, Universities, Young Adult},
}

@inproceedings{schuller_interspeech_2020,
	title = {The {INTERSPEECH} 2020 Computational Paralinguistics Challenge: Elderly Emotion, Breathing \& Masks},
	url = {https://www.isca-speech.org/archive/interspeech_2020/schuller20_interspeech.html},
	doi = {10.21437/Interspeech.2020-32},
	shorttitle = {The {INTERSPEECH} 2020 Computational Paralinguistics Challenge},
	abstract = {The {INTERSPEECH} 2020 Computational Paralinguistics Challenge addresses three different problems for the ﬁrst time in a research competition under well-deﬁned conditions: In the Elderly Emotion Sub-Challenge, arousal and valence in the speech of elderly individuals have to be modelled as a 3-class problem; in the Breathing Sub-Challenge, breathing has to be assessed as a regression problem; and in the Mask Sub-Challenge, speech without and with a surgical mask has to be told apart. We describe the Sub-Challenges, baseline feature extraction, and classiﬁers based on the ‘usual’ {COMPARE} and {BoAW} features as well as deep unsupervised representation learning using the {AUDEEP} toolkit, and deep feature extraction from pre-trained {CNNs} using the {DEEP} {SPECTRUM} toolkit; in addition, we partially add deep end-to-end sequential modelling, and, for the ﬁrst time in the challenge, linguistic analysis.},
	eventtitle = {Interspeech 2020},
	pages = {2042--2046},
	booktitle = {Interspeech 2020},
	publisher = {{ISCA}},
	author = {Schuller, Björn W. and Batliner, Anton and Bergler, Christian and Messner, Eva-Maria and Hamilton, Antonia and Amiriparian, Shahin and Baird, Alice and Rizos, Georgios and Schmitt, Maximilian and Stappen, Lukas and Baumeister, Harald and {MacIntyre}, Alexis Deighton and Hantke, Simone},
	urldate = {2023-04-12},
	date = {2020-10-25},
	langid = {english},
	file = {Schuller et al. - 2020 - The INTERSPEECH 2020 Computational Paralinguistics.pdf:C\:\\Users\\nicol\\Zotero\\storage\\NYKYHA69\\Schuller et al. - 2020 - The INTERSPEECH 2020 Computational Paralinguistics.pdf:application/pdf},
}

@article{zhou_emotional_2022,
	title = {Emotional voice conversion: Theory, databases and {ESD}},
	volume = {137},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639321001308},
	doi = {10.1016/j.specom.2021.11.006},
	shorttitle = {Emotional voice conversion},
	abstract = {In this paper, we first provide a review of the state-of-the-art emotional voice conversion research, and the existing emotional speech databases. We then motivate the development of a novel emotional speech database ({ESD}) that addresses the increasing research need. With this paper, the {ESD} database1 is now made available to the research community. The {ESD} database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 h of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies. As case studies, we implement several state-of-the-art emotional voice conversion systems on the {ESD} database. This paper provides a reference study on {ESD} in conjunction with its release.},
	pages = {1--18},
	journaltitle = {Speech Communication},
	author = {Zhou, Kun and Sisman, Berrak and Liu, Rui and Li, Haizhou},
	urldate = {2023-04-12},
	date = {2022-02},
	langid = {english},
	file = {Zhou et al. - 2022 - Emotional voice conversion Theory, databases and .pdf:C\:\\Users\\nicol\\Zotero\\storage\\MDCHVARC\\Zhou et al. - 2022 - Emotional voice conversion Theory, databases and .pdf:application/pdf},
}

@article{yamagishi_cstr_2019,
	title = {{CSTR} {VCTK} Corpus: English Multi-speaker Corpus for {CSTR} Voice Cloning Toolkit (version 0.92)},
	rights = {Creative Commons Attribution 4.0 International Public License},
	url = {https://datashare.ed.ac.uk/handle/10283/3443},
	doi = {10.7488/ds/2645},
	shorttitle = {{CSTR} {VCTK} Corpus},
	abstract = {This {CSTR} {VCTK} Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. 
 
The newspaper texts were taken from Herald Glasgow, with permission from Herald \& Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage.  
 
The details of the text selection algorithms are described in the following paper:  
C. Veaux, J. Yamagishi and S. King,  
"The voice bank corpus: Design, collection and data analysis of  
a large regional accent speech database,"  
https://doi.org/10.1109/{ICSDA}.2013.6709856 
 
The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/{\textasciitilde}idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at 
http://www.ualberta.ca/{\textasciitilde}aacl2009/{PDFs}/{WeinbergerKunath}2009AACL.pdf 
 
All speech data was recorded using an identical recording setup: an omni-directional microphone ({DPA} 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser {MKH} 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using {MKH} 800). All recordings were converted into 16 bits, were downsampled to 48 {kHz}, and were manually end-pointed. 
 
This corpus was originally aimed for {HMM}-based text-to-speech synthesis systems, especially for speaker-adaptive {HMM}-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for {DNN}-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. 
 
The dataset was was referenced in the Google {DeepMind} work on {WaveNet}: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.},
	journaltitle = {The Rainbow Passage which the speakers read out can be found in the International Dialects of English Archive: (http://web.ku.edu/{\textasciitilde}idea/readings/rainbow.htm).},
	author = {Yamagishi, Junichi and Veaux, Christophe and {MacDonald}, Kirsten},
	urldate = {2023-04-12},
	date = {2019-11-13},
	note = {Accepted: 2019-11-13T17:09:33Z
Publisher: University of Edinburgh. The Centre for Speech Technology Research ({CSTR})},
}

@misc{livingstone_ryerson_2018-1,
	title = {The Ryerson Audio-Visual Database of Emotional Speech and Song ({RAVDESS})},
	url = {https://zenodo.org/record/1188976},
	doi = {10.5281/zenodo.1188976},
	abstract = {Citing the {RAVDESS} The {RAVDESS} is released under a Creative Commons Attribution license, so please cite the {RAVDESS} if it is used in your work in any form.  Published academic papers should use the academic paper citation for our {PLoS}1 paper.  Personal works, such as machine learning projects/blog posts, should provide a {URL} to this Zenodo page, though a reference to our {PLoS}1 paper would also be appreciated. Academic paper citation Livingstone {SR}, Russo {FA} (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song ({RAVDESS}): A dynamic, multimodal set of facial and vocal expressions in North American English. {PLoS} {ONE} 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. Personal use citation Include a link to this Zenodo page - https://zenodo.org/record/1188976 Commercial Licenses Commercial licenses for the {RAVDESS} can be purchased.  For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Contact Information If you would like further information about the {RAVDESS}, to purchase a commercial license, or if you experience any issues downloading files, please contact us at ravdess@gmail.com. Example Videos Watch a sample of the {RAVDESS} speech and song videos. Emotion Classification Users If you're interested in using machine learning to classify emotional expressions with the {RAVDESS}, please see our new {RAVDESS} Facial Landmark Tracking data set [Zenodo project page]. Construction and Validation Full details on the construction and perceptual validation of the {RAVDESS} are described in our {PLoS} {ONE} paper - https://doi.org/10.1371/journal.pone.0196391. The {RAVDESS} contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from {PLoS} {ONE}. Description The Ryerson Audio-Visual Database of Emotional Speech and Song ({RAVDESS}) contains 7356 files (total size: 24.8 {GB}). The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, {AAC} 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor\_18. Audio-only files Audio-only files of all actors (01-24) are available as two separate zip files ({\textasciitilde}200 {MB} each): Speech file (Audio\_Speech\_Actors\_01-24.zip, 215 {MB}) contains 1440 files: 60 trials per actor x 24 actors = 1440.  Song file (Audio\_Song\_Actors\_01-24.zip, 198 {MB}) contains 1012 files: 44 trials per actor x 23 actors = 1012. Audio-Visual and Video-only files Video files are provided as separate zip downloads for each actor (01-24, {\textasciitilde}500 {MB} each), and are split into separate speech and song downloads: Speech files (Video\_Speech\_Actor\_01.zip to Video\_Speech\_Actor\_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities ({AV}, {VO}) x 24 actors = 2880. Song files (Video\_Song\_Actor\_01.zip to Video\_Song\_Actor\_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities ({AV}, {VO}) x 23 actors = 2024. File Summary In total, the {RAVDESS} collection includes 7356 files (2880+2024+1440+1012 files). File naming convention Each of the 7356 {RAVDESS} files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:  Filename identifiers  Modality (01 = full-{AV}, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). {NOTE}: There is no strong intensity for the 'neutral' emotion. Statement (01 = "Kids are talking by the door", 02 = "Dogs are sitting by the door"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female). Filename example: 02-01-06-01-02-01-12.mp4  Video-only (02) Speech (01) Fearful (06) Normal intensity (01) Statement "dogs" (02) 1st Repetition (01) 12th Actor (12) Female, as the actor {ID} number is even. License information The {RAVDESS} is released under a Creative Commons Attribution-{NonCommercial}-{ShareAlike} 4.0 International License, {CC} {BY}-{NC}-{SA} 4.0  Commercial licenses for the {RAVDESS} can also be purchased.  For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Related Data sets {RAVDESS} Facial Landmark Tracking data set [Zenodo project page].},
	publisher = {Zenodo},
	author = {Livingstone, Steven R. and Russo, Frank A.},
	urldate = {2023-04-12},
	date = {2018-04-05},
	keywords = {emotion, emotion classification, emotion database, emotion expression, emotion perception, face, facial expressions, multimodal communication, {RAVDESS}, stimulus validation, vocal expressions, voice},
	file = {Zenodo Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\GW8TJXBF\\1188976.html:text/html},
}

@online{noauthor_text_nodate,
	title = {Text to Speech Software – Amazon Polly – Amazon Web Services},
	url = {https://aws.amazon.com/polly/},
	abstract = {Amazon Polly turns text into lifelike speech, allowing you to create applications that talk and build entirely new categories of speech-activated applications.},
	titleaddon = {Amazon Web Services, Inc.},
	urldate = {2023-04-14},
	langid = {american},
	file = {Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\DETHEYR7\\polly.html:text/html},
}

@article{looser_tipping_2010,
	title = {The Tipping Point of Animacy: How, When, and Where We Perceive Life in a Face},
	volume = {21},
	issn = {0956-7976},
	url = {https://doi.org/10.1177/0956797610388044},
	doi = {10.1177/0956797610388044},
	shorttitle = {The Tipping Point of Animacy},
	abstract = {Faces capture humans? attention; yet, beyond aesthetic appreciation, it is presumably not the face itself that interests people but the mind behind it. Minds think, feel, and act in ways that have direct consequences for well-being, but despite their importance, how minds are perceived in faces is not well understood. We investigated this mechanism by presenting participants with morphed images created from animate (human) and inanimate (mannequin) faces. Life and mind were perceived to ?appear? at a consistent location on the morph continuum, close to the human endpoint. This location constituted a categorical boundary, as evidenced by increased sensitivity to differences in image pairs that straddled this tipping point. Additionally, the impression of life was gleaned from the eyes more than from other facial features. These results suggest that human beings are highly attuned to specific facial cues, carried largely in the eyes, that gate the categorical perception of life.},
	pages = {1854--1862},
	number = {12},
	journaltitle = {Psychol Sci},
	author = {Looser, Christine E. and Wheatley, Thalia},
	urldate = {2023-04-14},
	date = {2010-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Looser and Wheatley - 2010 - The Tipping Point of Animacy How, When, and Where.pdf:C\:\\Users\\nicol\\Zotero\\storage\\AZ88FJKW\\Looser and Wheatley - 2010 - The Tipping Point of Animacy How, When, and Where.pdf:application/pdf},
}

@article{mori_uncanny_1970,
	title = {The uncanny valley.},
	volume = {7},
	pages = {33--35},
	journaltitle = {Energy},
	author = {Mori, Masahiro},
	date = {1970},
}

@article{ribeiro_crowdmos_2011,
	title = {{CROWDMOS}: An approach for crowdsourcing mean opinion score studies},
	url = {http://ieeexplore.ieee.org/document/5946971/},
	doi = {10.1109/ICASSP.2011.5946971},
	shorttitle = {{CROWDMOS}},
	abstract = {{MOS} (mean opinion score) subjective quality studies are used to evaluate many signal processing methods. Since laboratory quality studies are time consuming and expensive, researchers often run small studies with less statistical significance or use objective measures which only approximate human perception. We propose a cost-effective and convenient measure called {crowdMOS}, obtained by having internet users participate in a {MOS}-like listening study. Workers listen and rate sentences at their leisure, using their own hardware, in an environment of their choice. Since these individuals cannot be supervised, we propose methods for detecting and discarding inaccurate scores. To automate {crowdMOS} testing, we offer a set of freely distributable, open-source tools for Amazon Mechanical Turk, a platform designed to facilitate crowdsourcing. These tools implement the {MOS} testing methodology described in this paper, providing researchers with a user-friendly means of performing subjective quality evaluations without the overhead associated with laboratory studies. Finally, we demonstrate the use of {crowdMOS} using data from the Blizzard text-to-speech competition, showing that it delivers accurate and repeatable results.},
	pages = {2416--2419},
	journaltitle = {2011 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Ribeiro, Flavio and Florencio, Dinei and Zhang, Cha and Seltzer, Michael},
	urldate = {2023-04-20},
	date = {2011-05},
	note = {Conference Name: {ICASSP} 2011 - 2011 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})
{ISBN}: 9781457705380
Place: Prague, Czech Republic
Publisher: {IEEE}},
}

@online{noauthor_p800_nodate,
	title = {P.800 : Methods for subjective determination of transmission quality},
	url = {https://www.itu.int/rec/T-REC-P.800},
	urldate = {2023-04-20},
	file = {P.800 \: Methods for subjective determination of transmission quality:C\:\\Users\\nicol\\Zotero\\storage\\KKDVGDLJ\\T-REC-P.html:text/html},
}

@online{noauthor_p808_nodate,
	title = {P.808 : Subjective evaluation of speech quality with a crowdsourcing approach},
	url = {https://www.itu.int/rec/T-REC-P.808/en},
	urldate = {2023-04-20},
	file = {P.808 \: Subjective evaluation of speech quality with a crowdsourcing approach:C\:\\Users\\nicol\\Zotero\\storage\\6CX6FVKK\\en.html:text/html},
}

@inproceedings{naderi_open_2020,
	title = {An Open source Implementation of {ITU}-T Recommendation P.808 with Validation},
	url = {http://arxiv.org/abs/2005.08138},
	doi = {10.21437/Interspeech.2020-2665},
	abstract = {The {ITU}-T Recommendation P.808 provides a crowdsourcing approach for conducting a subjective assessment of speech quality using the Absolute Category Rating ({ACR}) method. We provide an open-source implementation of the {ITU}-T Rec. P.808 that runs on the Amazon Mechanical Turk platform. We extended our implementation to include Degradation Category Ratings ({DCR}) and Comparison Category Ratings ({CCR}) test methods. We also significantly speed up the test process by integrating the participant qualification step into the main rating task compared to a two-stage qualification and rating solution. We provide program scripts for creating and executing the subjective test, and data cleansing and analyzing the answers to avoid operational errors. To validate the implementation, we compare the Mean Opinion Scores ({MOS}) collected through our implementation with {MOS} values from a standard laboratory experiment conducted based on the {ITU}-T Rec. P.800. We also evaluate the reproducibility of the result of the subjective speech quality assessment through crowdsourcing using our implementation. Finally, we quantify the impact of parts of the system designed to improve the reliability: environmental tests, gold and trapping questions, rating patterns, and a headset usage test.},
	pages = {2862--2866},
	booktitle = {Interspeech 2020},
	author = {Naderi, Babak and Cutler, Ross},
	urldate = {2023-04-20},
	date = {2020-10-25},
	eprinttype = {arxiv},
	eprint = {2005.08138 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Sound},
	file = {arXiv Fulltext PDF:C\:\\Users\\nicol\\Zotero\\storage\\WF4MZE44\\Naderi and Cutler - 2020 - An Open source Implementation of ITU-T Recommendat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nicol\\Zotero\\storage\\WGATV4BF\\2005.html:text/html},
}

@online{noauthor_amazon_nodate,
	title = {Amazon Mechanical Turk},
	url = {https://www.mturk.com/},
	urldate = {2023-04-20},
	file = {Amazon Mechanical Turk:C\:\\Users\\nicol\\Zotero\\storage\\AEUG7EPD\\www.mturk.com.html:text/html},
}

@inproceedings{martin_enterface_2006,
	title = {The {eNTERFACE}' 05 Audio-Visual Emotion Database},
	doi = {10.1109/ICDEW.2006.145},
	abstract = {This paper presents an audio-visual emotion database that can be used as a reference database for testing and evaluating video, audio or joint audio-visual emotion recognition algorithms. Additional uses may include the evaluation of algorithms performing other multimodal signal processing tasks, such as multimodal person identification or audio-visual speech recognition. This paper presents the difficulties involved in the construction of such a multimodal emotion database and the different protocols that have been used to cope with these difficulties. It describes the experimental setup used for the experiments and includes a section related to the segmentation and selection of the video samples, in such a way that the database contains only video sequences carrying the desired affective information. This database is made publicly available for scientific research purposes.},
	eventtitle = {22nd International Conference on Data Engineering Workshops ({ICDEW}'06)},
	pages = {8--8},
	booktitle = {22nd International Conference on Data Engineering Workshops ({ICDEW}'06)},
	author = {Martin, O. and Kotsia, I. and Macq, B. and Pitas, I.},
	date = {2006-04},
	keywords = {Humans, Audio databases, Emotion recognition, Image databases, Informatics, Protocols, Signal processing algorithms, Spatial databases, Speech analysis, Visual databases},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\nicol\\Zotero\\storage\\HMCVURM4\\1623803.html:text/html},
}

@article{busso_msp-improv_2017,
	title = {{MSP}-{IMPROV}: An Acted Corpus of Dyadic Interactions to Study Emotion Perception},
	volume = {8},
	issn = {1949-3045},
	doi = {10.1109/TAFFC.2016.2515617},
	shorttitle = {{MSP}-{IMPROV}},
	abstract = {We present the {MSP}-{IMPROV} corpus, a multimodal emotional database, where the goal is to have control over lexical content and emotion while also promoting naturalness in the recordings. Studies on emotion perception often require stimuli with fixed lexical content, but that convey different emotions. These stimuli can also serve as an instrument to understand how emotion modulates speech at the phoneme level, in a manner that controls for coarticulation. Such audiovisual data are not easily available from natural recordings. A common solution is to record actors reading sentences that portray different emotions, which may not produce natural behaviors. We propose an alternative approach in which we define hypothetical scenarios for each sentence that are carefully designed to elicit a particular emotion. Two actors improvise these emotion-specific situations, leading them to utter contextualized, non-read renditions of sentences that have fixed lexical content and convey different emotions. We describe the context in which this corpus was recorded, the key features of the corpus, the areas in which this corpus can be useful, and the emotional content of the recordings. The paper also provides the performance for speech and facial emotion classifiers. The analysis brings novel classification evaluations where we study the performance in terms of inter-evaluator agreement and naturalness perception, leveraging the large size of the audiovisual database.},
	pages = {67--80},
	number = {1},
	journaltitle = {{IEEE} Transactions on Affective Computing},
	author = {Busso, Carlos and Parthasarathy, Srinivas and Burmania, Alec and {AbdelWahab}, Mohammed and Sadoughi, Najmeh and Provost, Emily Mower},
	date = {2017-01},
	note = {Conference Name: {IEEE} Transactions on Affective Computing},
	keywords = {Affective computing, Emotion recognition, audiovisual emotional dataset, Computer science, Context, Databases, Emotion elicitation, emotion recognition, emotional evaluation, Speech, Videos},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\nicol\\Zotero\\storage\\66F29S7J\\7374697.html:text/html},
}

@article{busso_iemocap_2008,
	title = {{IEMOCAP}: interactive emotional dyadic motion capture database},
	volume = {42},
	issn = {1574-020X, 1574-0218},
	url = {http://link.springer.com/10.1007/s10579-008-9076-6},
	doi = {10.1007/s10579-008-9076-6},
	shorttitle = {{IEMOCAP}},
	abstract = {Since emotions are expressed through a combination of verbal and non-verbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the “interactive emotional dyadic motion capture database” ({IEMOCAP}), collected by the Speech Analysis and Interpretation Laboratory ({SAIL}) at the University of Southern California ({USC}). This database was recorded from ten actors in dyadic sessions with markers on the face, head, and hands, which provide detailed information about their facial expression and hand movements during scripted and spontaneous spoken communication scenarios. The actors performed selected emotional scripts and also improvised hypothetical scenarios designed to elicit speciﬁc types of emotions (happiness, anger, sadness, frustration and neutral state). The corpus contains approximately twelve hours of data. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication.},
	pages = {335--359},
	number = {4},
	journaltitle = {Lang Resources \& Evaluation},
	author = {Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N. and Lee, Sungbok and Narayanan, Shrikanth S.},
	urldate = {2023-06-05},
	date = {2008-12},
	langid = {english},
	file = {Busso et al. - 2008 - IEMOCAP interactive emotional dyadic motion captu.pdf:C\:\\Users\\nicol\\Zotero\\storage\\3KUSPRE6\\Busso et al. - 2008 - IEMOCAP interactive emotional dyadic motion captu.pdf:application/pdf},
}

@article{constantinescu_can_2022,
	title = {Can Robotic {AI} Systems Be Virtuous and Why Does This Matter?},
	volume = {14},
	issn = {1875-4791, 1875-4805},
	url = {https://link.springer.com/10.1007/s12369-022-00887-w},
	doi = {10.1007/s12369-022-00887-w},
	abstract = {The growing use of social robots in times of isolation refocuses ethical concerns for Human–Robot Interaction and its implications for social, emotional, and moral life. In this article we raise a virtue-ethics-based concern regarding deployment of social robots relying on deep learning {AI} and ask whether they may be endowed with ethical virtue, enabling us to speak of “virtuous robotic {AI} systems”. In answering this question, we argue that {AI} systems cannot genuinely be virtuous but can only behave in a virtuous way. To that end, we start from the philosophical understanding of the nature of virtue in the Aristotelian virtue ethics tradition, which we take to imply the ability to perform (1) the right actions (2) with the right feelings and (3) in the right way. We discuss each of the three requirements and conclude that {AI} is unable to satisfy any of them. Furthermore, we relate our claims to current research in machine ethics, technology ethics, and Human–Robot Interaction, discussing various implications, such as the possibility to develop Autonomous Artiﬁcial Moral Agents in a virtue ethics framework.},
	pages = {1547--1557},
	number = {6},
	journaltitle = {Int J of Soc Robotics},
	author = {Constantinescu, Mihaela and Crisp, Roger},
	urldate = {2023-09-08},
	date = {2022-08},
	langid = {english},
	file = {Constantinescu and Crisp - 2022 - Can Robotic AI Systems Be Virtuous and Why Does Th.pdf:C\:\\Users\\nicol\\Zotero\\storage\\A8X3D3K4\\Constantinescu and Crisp - 2022 - Can Robotic AI Systems Be Virtuous and Why Does Th.pdf:application/pdf},
}

@article{fuchs_understanding_nodate,
	title = {Understanding Sophia? On human interaction with artificial agents},
	abstract = {Advances in artificial intelligence ({AI}) create an increasing similarity between the performance of {AI} systems or {AI}-based robots and human communication. They raise the questions: (1) whether it is possible to communicate with, understand, and even empathically perceive artificial agents; (2) whether we should ascribe actual subjectivity and thus quasi-personal status to them beyond a certain level of simulation; (3) what will be the impact of an increasing dissolution of the distinction between simulated and real encounters. (1) To answer these questions, the paper argues that the precondition for actually understanding others consists in the implicit assumption of the subjectivity of our counterpart, which makes shared feelings and a „we-intentionality” possible. This assumption is ultimately based on the presupposition of a shared form of life, conceived here as „conviviality.” (2) The possibility that future artificial agents could meet these preconditions is refuted on the basis of embodied and enactive cognition, which links subjectivity and consciousness to the aliveness of an organism.},
	author = {Fuchs, Thomas},
	langid = {english},
	file = {Fuchs - Understanding Sophia On human interaction with ar.pdf:C\:\\Users\\nicol\\Zotero\\storage\\Q9796EUJ\\Fuchs - Understanding Sophia On human interaction with ar.pdf:application/pdf},
}

@article{nayak_test_2022,
	title = {Test of Prosody via Syllable Emphasis (“{TOPsy}”): Psychometric Validation of a Brief Scalable Test of Lexical Stress Perception},
	volume = {16},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2022.765945/full},
	doi = {10.3389/fnins.2022.765945},
	shorttitle = {Test of Prosody via Syllable Emphasis (“{TOPsy}”)},
	abstract = {Prosody perception is fundamental to spoken language communication as it supports comprehension, pragmatics, morphosyntactic parsing of speech streams, and phonological awareness. A particular aspect of prosody: perceptual sensitivity to speech rhythm patterns in words (i.e., lexical stress sensitivity), is also a robust predictor of reading skills, though it has received much less attention than phonological awareness in the literature. Given the importance of prosody and reading in educational outcomes, reliable and valid tools are needed to conduct large-scale health and genetic investigations of individual differences in prosody, as groundwork for investigating the biological underpinnings of the relationship between prosody and reading. Motivated by this need, we present the Test of Prosody via Syllable Emphasis (“{TOPsy}”) and highlight its merits as a phenotyping tool to measure lexical stress sensitivity in as little as 10 min, in scalable internet-based cohorts. In this 28-item speech rhythm perception test [modeled after the stress identiﬁcation test from Wade-Woolley (2016)], participants listen to multi-syllabic spoken words and are asked to identify lexical stress patterns. Psychometric analyses in a large internet-based sample shows excellent reliability, and predictive validity for self-reported difﬁculties with speech-language, reading, and musical beat synchronization. Further, items loaded onto two distinct factors corresponding to initially stressed vs. non-initially stressed words. These results are consistent with previous reports that speech rhythm perception abilities correlate with musical rhythm sensitivity and speech-language/reading skills, and are implicated in reading disorders (e.g., dyslexia). We conclude that {TOPsy} can serve as a useful tool for studying prosodic perception at large scales in a variety of different settings, and importantly can act as a validated brief phenotype for future investigations of the genetic architecture of prosodic perception, and its relationship to educational outcomes.},
	pages = {765945},
	journaltitle = {Front. Neurosci.},
	author = {Nayak, Srishti and Gustavson, Daniel E. and Wang, Youjia and Below, Jennifer E. and Gordon, Reyna L. and Magne, Cyrille L.},
	urldate = {2023-09-09},
	date = {2022-02-09},
	langid = {english},
	file = {Nayak et al. - 2022 - Test of Prosody via Syllable Emphasis (“TOPsy”) P.pdf:C\:\\Users\\nicol\\Zotero\\storage\\TUBCB84Z\\Nayak et al. - 2022 - Test of Prosody via Syllable Emphasis (“TOPsy”) P.pdf:application/pdf},
}
