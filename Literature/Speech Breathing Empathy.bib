
@article{Valle2020,
	title = {Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},
	url = {http://arxiv.org/abs/2005.05957},
	abstract = {In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from {IAF} and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores ({MOS}) show that Flowtron matches state-of-the-art {TTS} models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at https://github.com/{NVIDIA}/flowtron},
	author = {Valle, Rafael and Shih, Kevin and Prenger, Ryan and Catanzaro, Bryan},
	date = {2020-05-12},
	eprinttype = {arxiv},
	eprint = {2005.05957},
	keywords = {★, synthesis},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\P6WNQVD7\\flowtron.pdf:application/pdf},
}

@inproceedings{novick_paolachat_2018,
	title = {Paolachat: A virtual agent with naturalistic breathing},
	volume = {10909 {LNCS}},
	isbn = {978-3-319-91580-7},
	doi = {10.1007/978-3-319-91581-4_26},
	abstract = {For embodied conversational agents ({ECAs}) the relationship between gesture and rapport is an open question. To enable us to learn whether adding breathing behaviors to an agent similar to {SimSensei} would lead users interacting to perceive the agent as more natural, we built an application, called Paola Chat, in which the {ECA} could display naturalistic breathing animations. Our study had two phases. In the first phase, we determined the most natural amplitude for the agent’s breathing. In the second phase, we assessed the effect of breathing on the users’ perceptions of rapport and naturalness. The study had a within-subjects design, with breathing/not-breathing as the independent variable. Despite our expectation that increased naturalness from breathing would lead users to report greater rapport in the breathing condition than in the not-breathing condition, the study’s results suggest that the animation of breathing appears to neither increase nor decrease these perceptions.},
	pages = {351--360},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	publisher = {Springer Verlag},
	author = {Novick, David and Afravi, Mahdokht and Camacho, Adriana},
	date = {2018},
	note = {{ISSN}: 16113349},
	keywords = {Dialog system, Embodied conversational agents, Human-agent dialog},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\BEUI6SP9\\PaolaChat.pdf:application/pdf},
}

@inproceedings{berthold_interpreting_1999,
	title = {Interpreting symptoms of cognitive load in speech input},
	volume = {407},
	isbn = {978-3-211-83151-9},
	doi = {10.1007/978-3-7091-2490-1_23},
	abstract = {Users of computing devices are increasingly likely to be subject to situationally determined distractions that produce exceptionally high cognitive load. The question arises of how a system can automatically interpret symptoms of such cognitive load in the user’s behavior. This paper examines this question with respect to systems that process speech input. First, we synthesize results of previous experimental studies of the ways in which a speaker’s cognitive load is reflected in features of speech. Then we present a conceptualization of these relationships in terms of Bayesian networks. For two examples of such symptoms-sentence fragments and articulation rate-we present results concerning the distribution of the symptoms in realistic assistance dialogs. Finally. using artificial data generated in accordance with the preceding analyses. we examine the ability of a Bayesian network to assess a user’s cognitive load on the basis of limited observations involving these two symptoms.},
	pages = {235--244},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	publisher = {Springer Verlag},
	author = {Berthold, Andre and Jameson, Anthony},
	date = {1999},
	note = {{ISSN}: 16113349},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\4USE2X75\\cognitive_load_speech.pdf:application/pdf},
}

@report{heim_emotion_1968,
	title = {Emotion, breathing and speech},
	author = {Heim, Edgar and Knapp, Peter H and Vachon, Louis and Globus, Gordon G and Nemetz, S Joseph},
	date = {1968},
	note = {Publication Title: Jourmti of Psychosomatic Reaearch
Volume: 12},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\M887QJRH\\emotion_breathing_speech.pdf:application/pdf},
}

@inproceedings{bernardet_dynamic_2017,
	title = {A dynamic speech breathing system for virtual characters},
	volume = {10498 {LNAI}},
	isbn = {978-3-319-67400-1},
	doi = {10.1007/978-3-319-67401-8_5},
	abstract = {Human speech production requires the dynamic regulation of air through the vocal system. While virtual character systems commonly are capable of speech output, they rarely take breathing during speaking – speech breathing – into account. We believe that integrating dynamic speech breathing systems in virtual characters can significantly contribute to augmenting their realism. Here, we present a novel control architecture aimed at generating speech breathing in virtual characters. This architecture is informed by behavioral, linguistic and anatomical knowledge of human speech breathing. Based on textual input and controlled by a set of low- and high-level parameters, the system produces dynamic signals in real-time that control the virtual character’s anatomy (thorax, abdomen, head, nostrils, and mouth) and sound production (speech and breathing). The system is implemented in Python, offers a graphical user interface for easy parameter control, and simultaneously controls the visual and auditory aspects of speech breathing through the integration of the character animation system {SmartBody} [16] and the audio synthesis platform {SuperCollider} [12]. Beyond contributing to realism, the presented system allows for a flexible generation of a wide range of speech breathing behaviors that can convey information about the speaker such as mood, age, and health.},
	pages = {43--52},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	publisher = {Springer Verlag},
	author = {Bernardet, Ulysses and Kang, Sin hwa and Feng, Andrew and {DiPaola}, Steve and Shapiro, Ari},
	date = {2017},
	note = {{ISSN}: 16113349},
	keywords = {synthesis, Animation, Breathing, Speaking, Speech breathing, Virtual character},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\YQ4GBL8T\\A_Dynamic_Speech_Breathing_System_for_Virtual_Char(1).pdf:application/pdf},
}

@thesis{kroes_k_empathizing_2022,
	title = {Empathizing with virtual agents: the effect of personification and general empathic tendencies},
	type = {phdthesis},
	author = {{Kroes K}},
	date = {2022},
	keywords = {empathy, thesis},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\YK94HNN6\\Empathizing with virtual agents.pdf:application/pdf},
}

@article{paiva_empathy_2017,
	title = {Empathy in virtual agents and robots: A survey},
	volume = {7},
	issn = {21606463},
	doi = {10.1145/2912150},
	abstract = {This article surveys the area of computational empathy, analysing different ways by which artificial agents can simulate and trigger empathy in their interactions with humans. Empathic agents can be seen as agents that have the capacity to place themselves into the position of a user's or another agent's emotional situation and respond appropriately. We also survey artificial agents that, by their design and behaviour, can lead users to respond emotionally as if they were experiencing the agent's situation. In the course of this survey, we present the research conducted to date on empathic agents in light of the principles and mechanisms of empathy found in humans. We end by discussing some of the main challenges that this exciting area will be facing in the future. Copyright is held by the owner/author(s).},
	number = {3},
	journaltitle = {{ACM} Transactions on Interactive Intelligent Systems},
	author = {Paiva, Ana and Leite, Iolanda and Boukricha, Hana and Wachsmuth, Ipke},
	date = {2017-09-01},
	note = {Publisher: Association for Computing Machinery},
	keywords = {★, Affective computing, Empathy, Human-computer interaction, Human-robot interaction, Social robots, Virtual agents},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\IE9MJCHX\\empathy_in_agents.pdf:application/pdf},
}

@inproceedings{roes_emotional_2022,
	title = {An Emotional Respiration Speech Dataset},
	isbn = {978-1-4503-9389-8},
	doi = {10.1145/3536220.3558803},
	pages = {70--78},
	publisher = {Association for Computing Machinery ({ACM})},
	author = {Roes, Rozemarijn Hannah and Pessanha, Francisca and Akdag Salah, Almila},
	date = {2022-11-07},
	keywords = {★},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\GPNYJ8CY\\Revised_GENEA_Emotional_Respiration_Dataset(2).pdf:application/pdf},
}

@thesis{wiersema_perception_2022,
	title = {Perception study: The difference in lighting perception on overall mood in rendered video compared to Virtual Reality environment},
	type = {phdthesis},
	author = {Wiersema, L J},
	date = {2022},
	keywords = {thesis},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\GFRD99MI\\Thesis_LauraWiersema.pdf:application/pdf},
}

@thesis{francois_blom_automated_2022,
	title = {Automated detection of depression},
	type = {phdthesis},
	author = {{François Blom}},
	date = {2022},
	keywords = {thesis},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\QW9HQVAR\\MSc_Thesis__Fran_ois___Phase_2__Thesis_Report(1).pdf:application/pdf},
}

@article{le_emotional_2023,
	title = {Emotional Vietnamese Speech Synthesis Using Style-Transfer Learning},
	volume = {44},
	issn = {02676192},
	doi = {10.32604/csse.2023.026234},
	abstract = {In recent years, speech synthesis systems have allowed for the production of very high-quality voices. Therefore, research in this domain is now turning to the problem of integrating emotions into speech. However, the method of constructing a speech synthesizer for each emotion has some limitations. First, this method often requires an emotional-speech data set with many sentences. Such data sets are very time-intensive and labor-intensive to complete. Second, training each of these models requires computers with large computational capabilities and a lot of effort and time for model tuning. In addition, each model for each emotion failed to take advantage of data sets of other emotions. In this paper, we propose a new method to synthesize emotional speech in which the latent expressions of emotions are learned from a small data set of professional actors through a Flowtron model. In addition, we provide a new method to build a speech corpus that is scalable and whose quality is easy to control. Next, to produce a high-quality speech synthesis model, we used this data set to train the Tacotron 2 model. We used it as a pre-trained model to train the Flowtron model. We applied this method to synthesize Vietnamese speech with sadness and happiness. Mean opinion score ({MOS}) assessment results show that {MOS} is 3.61 for sadness and 3.95 for happiness. In conclusion, the proposed method proves to be more effective for a high degree of automation and fast emotional sentence generation, using a small emotional-speech data set.},
	pages = {1263--1278},
	number = {2},
	journaltitle = {Computer Systems Science and Engineering},
	author = {Le, Thanh X. and Le, An T. and Nguyen, Quang H.},
	date = {2023},
	note = {Publisher: Tech Science Press},
	keywords = {Emotional speech synthesis, flowtron, speech synthesis, style transfer, vietnamese speech},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\XNL5TDI9\\vietnamese_flowtron.pdf:application/pdf},
}

@article{liu_reinforcement_2021,
	title = {Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability},
	url = {http://arxiv.org/abs/2104.01408},
	abstract = {Emotional text-to-speech synthesis ({ETTS}) has seen much progress in recent years. However, the generated voice is often not perceptually identifiable by its intended emotion category. To address this problem, we propose a new interactive training paradigm for {ETTS}, denoted as i-{ETTS}, which seeks to directly improve the emotion discriminability by interacting with a speech emotion recognition ({SER}) model. Moreover, we formulate an iterative training strategy with reinforcement learning to ensure the quality of i-{ETTS} optimization. Experimental results demonstrate that the proposed i-{ETTS} outperforms the state-of-the-art baselines by rendering speech with more accurate emotion style. To our best knowledge, this is the first study of reinforcement learning in emotional text-to-speech synthesis.},
	author = {Liu, Rui and Sisman, Berrak and Li, Haizhou},
	date = {2021-04-03},
	eprinttype = {arxiv},
	eprint = {2104.01408},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\TRGX8W5L\\RL_speech_synth.pdf:application/pdf},
}

@article{salah_sound_2021,
	title = {The sound of silence: Breathing analysis for finding traces of trauma and depression in oral history archives},
	volume = {36},
	issn = {2055-7671},
	doi = {10.1093/llc/fqaa056},
	abstract = {Many people experience a traumatic event during their lifetime. In some extraordinary situations, such as natural disasters, war, massacres, terrorism, or mass migration, the traumatic event is shared by a community and the effects go beyond those directly affected. Today, thanks to recorded interviews and testimonials, many archives and collections exist that are open to researchers of trauma studies, holocaust studies, and historians, among others. These archives act as vital testimonials for oral history, politics, and human rights. As such, they are usually either transcribed or meticulously indexed. In this work, we propose to look at the nonverbal signals emitted by victims of various traumatic events when they describe the trauma and we seek to render these for novel representations without taking into account the explicit verbal content. Our preliminary paralinguistic analysis on a manually annotated collection of testimonials from different archives, as well as on a corpus prepared for depression and post-traumatic stress disorder detection indicates a tentative connection between breathing and emotional states of speakers, which opens up new possibilities of exploring oral history archives.},
	pages = {ii2--ii8},
	issue = {Supplement\_2},
	journaltitle = {Digital Scholarship in the Humanities},
	author = {Salah, Almila Akdag and Salah, Albert Ali and Kaya, Heysem and Doyran, Metehan and Kavcar, Evrim},
	date = {2021-11-05},
	note = {Publisher: Oxford University Press ({OUP})},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\G66X3MAU\\the_sound_of_silence.pdf:application/pdf},
}

@article{livingstone_ryerson_2018,
	title = {The Ryerson Audio-Visual Database of Emotional Speech and Song ({RAVDESS}): A dynamic, multimodal set of facial and vocal expressions in North American English},
	url = {https://www.},
	doi = {10.5281/zenodo.1188976},
	abstract = {The {RAVDESS} is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/ zenodo.1188976.},
	author = {Livingstone, Steven R and Russo, Frank A},
	urldate = {2022-12-30},
	date = {2018},
	note = {{ISBN}: 1111111111},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\H8EJDY3N\\full-text.pdf:application/pdf},
}

@article{Lancucki2020,
	title = {{FastPitch}: Parallel Text-to-speech with Pitch Prediction},
	url = {http://arxiv.org/abs/2006.06873},
	abstract = {We present {FastPitch}, a fully-parallel text-to-speech model based on {FastSpeech}, conditioned on fundamental frequency contours. The model predicts pitch contours during inference. By altering these predictions, the generated speech can be more expressive, better match the semantic of the utterance, and in the end more engaging to the listener. Uniformly increasing or decreasing pitch with {FastPitch} generates speech that resembles the voluntary modulation of voice. Conditioning on frequency contours improves the overall quality of synthesized speech, making it comparable to state-of-the-art. It does not introduce an overhead, and {FastPitch} retains the favorable, fully-parallel Transformer architecture, with over 900x real-time factor for mel-spectrogram synthesis of a typical utterance.},
	author = {Łańcucki, Adrian},
	date = {2020-06-11},
	eprinttype = {arxiv},
	eprint = {2006.06873},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\8ZSNDBD5\\fastpitch.pdf:application/pdf},
}

@incollection{Szekely2020,
	title = {Breathing and speech planning in spontaneous speech synthesis},
	isbn = {978-1-5090-6631-5},
	abstract = {Breathing and speech planning in spontaneous speech are coordinated processes, often exhibiting disfluent patterns. While synthetic speech is not subject to respiratory needs, integrating breath into synthesis has advantages for naturalness and recall. At the same time, a synthetic voice reproducing disfluent breathing patterns learned from the data can be problematic. To address this, we first propose training stochastic {TTS} on a corpus of overlapping breath-group bigrams, to take context into account. Next, we introduce an unsupervised automatic annotation of likely-disfluent breath events, through a product-of-experts model that combines the output of two breathevent predictors, each using complementary information and operating in opposite directions. This annotation enables creating an automatically-breathing spontaneous speech synthesiser with a more fluent breathing style. A subjective evaluation on two spoken genres (impromptu and rehearsed) found the proposed system to be preferred over the baseline approach treating all breath events the same.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {{Éva Székely} and {Gustav Eje Henter} and {Jonas Beskow} and {Joakim Gustafson}},
	date = {2020},
	keywords = {★},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\BMJ9P3WU\\stochastic_breath_synthesis.pdf:application/pdf},
}

@article{iannizzotto_vision_2018,
	title = {A Vision and Speech Enabled, Customizable, Virtual Assistant for Smart Environments},
	url = {https://ieeexplore.ieee.org/document/8431232/},
	doi = {10.1109/HSI.2018.8431232},
	pages = {50--56},
	journaltitle = {2018 11th International Conference on Human System Interaction ({HSI})},
	author = {Iannizzotto, Giancarlo and Bello, Lucia Lo and Nucita, Andrea and Grasso, Giorgio Mario},
	urldate = {2023-01-13},
	date = {2018-07},
	note = {Publisher: {IEEE}
{ISBN}: 978-1-5386-5024-0},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\D5TRUHYV\\non_human_VA.pdf:application/pdf},
}

@article{urgen_uncanny_2018,
	title = {Uncanny valley as a window into predictive processing in the social brain},
	volume = {114},
	issn = {18733514},
	doi = {10.1016/j.neuropsychologia.2018.04.027},
	abstract = {Uncanny valley refers to humans' negative reaction to almost-but-not-quite-human agents. Theoretical work proposes prediction violation as an explanation for uncanny valley but no empirical work has directly tested it. Here, we provide evidence that supports this theory using event-related brain potential recordings from the human scalp. Human subjects were presented images and videos of three agents as {EEG} was recorded: a real human, a mechanical robot, and a realistic robot in between. The real human and the mechanical robot had congruent appearance and motion whereas the realistic robot had incongruent appearance and motion. We hypothesize that the appearance of the agent would provide a context to predict her movement, and accordingly the perception of the realistic robot would elicit an N400 effect indicating the violation of predictions, whereas the human and the mechanical robot would not. Our data confirmed this hypothesis suggesting that uncanny valley could be explained by violation of one's predictions about human norms when encountered with realistic but artificial human forms. Importantly, our results implicate that the mechanisms underlying perception of other individuals in our environment are predictive in nature.},
	pages = {181--185},
	journaltitle = {Neuropsychologia},
	author = {Urgen, Burcu A. and Kutas, Marta and Saygin, Ayse P.},
	date = {2018-06-01},
	pmid = {29704523},
	note = {Publisher: Elsevier Ltd},
	keywords = {Action perception, N400, Predictive processing, Social neuroscience, Uncanny valley},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\YPWSGKCH\\urgen_uncanny_valley.pdf:application/pdf},
}

@inproceedings{weis_cognitive_2017,
	title = {Cognitive conflict as possible origin of the uncanny valley},
	volume = {2017-October},
	isbn = {978-0-945289-53-1},
	doi = {10.1177/1541931213601763},
	abstract = {In social robotics, the term Uncanny Valley describes the phenomenon that linear increases in human-likeness of an agent do not entail an equally linear increase in favorable reactions towards that agent. Instead, a pronounced dip or 'valley' at around 70\% human-likeness emerges. One currently popular view to explain this drop in favorable reactions is delivered by the Categorical Perception Hypothesis. It is suggested that categorization of agents with mixed human and nonhuman features is associated with additional cognitive costs and that these costs are the cause of the Uncanny Valley. However, the nature of the cognitive costs is still matter of debate. The current study explores whether the cognitive costs associated with stimulus categorization around the Uncanny Valley could be due to cognitive conflict as evoked by simultaneous activation of two categories. Using the mouse tracking technique, we show that cognitive conflict indeed peaks around the Uncanny Valley region of human-likeness. Our findings lay the foundation for investigating the effects of cognitive conflict on positive affect towards agents of around 70\% human-likeness, possibly leading to the unraveling of the origins of the Uncanny Valley.},
	pages = {1599--1603},
	booktitle = {Proceedings of the Human Factors and Ergonomics Society},
	publisher = {Human Factors an Ergonomics Society Inc.},
	author = {Weis, Patrick P. and Wiese, Eva},
	date = {2017},
	note = {{ISSN}: 10711813},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\CYTJG4DR\\uncanny_valley.pdf:application/pdf},
}

@inproceedings{wang_exploring_2019,
	title = {Exploring virtual agents for augmented reality},
	isbn = {978-1-4503-5970-2},
	doi = {10.1145/3290605.3300511},
	abstract = {Prior work has shown that embodiment can benefit virtual agents, such as increasing rapport and conveying nonverbal information. However, it is unclear if users prefer an embodied to a speech-only agent for augmented reality ({AR}) headsets that are designed to assist users in completing real-world tasks. We conducted a study to examine users’ perceptions and behaviors when interacting with virtual agents in {AR}. We asked 24 adults to wear the Microsoft {HoloLens} and find objects in a hidden object game while interacting with an agent that would offer assistance. We presented participants with four different agents: voice-only, non-human, full-size embodied, and a miniature embodied agent. Overall, users preferred the miniature embodied agent due to the novelty of his size and reduced uncanniness as opposed to the larger agent. From our results, we draw conclusions about how agent representation matters and derive guidelines on designing agents for {AR} headsets.},
	booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
	publisher = {Association for Computing Machinery},
	author = {Wang, Isaac and Smith, Jesse and Ruiz, Jaime},
	date = {2019-05-02},
	keywords = {Embodied conversational agents, Augmented reality},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\8ETTXCTI\\AR_virtual_agents_comparison.pdf:application/pdf},
}

@report{bailenson_independent_2005,
	title = {The Independent and Interactive Effects of Embodied-Agent Appearance and Behavior on Self-Report, Cognitive, and Behavioral Markers of Copresence in Immersive Virtual Environments},
	abstract = {The current study examined how assessments of copresence in an immersive virtual environment are influenced by variations in how much an embodied agent resembles a human being in appearance and behavior. We measured the extent to which virtual representations were both perceived and treated as if they were human via self-report, behavioral, and cognitive dependent measures. Distinctive patterns of findings emerged with respect to the behavior and appearance of embodied agents depending on the definition and operationalization of copresence. Independent and interactive effects for appearance and behavior were found suggesting that assessing the impact of behavioral realism on copresence without taking into account the appearance of the embodied agent (and vice versa) can lead to misleading conclusions. Consistent with the results of previous research, copresence was lowest when there was a large mismatch between the appearance and behav-ioral realism of an embodied agent.},
	pages = {379--393},
	author = {Bailenson, Jeremy N and Swinth, Kim and Hoyt, Crystal and Persky, Susan and Dimov, Alex and Blascovich, Jim},
	date = {2005},
	note = {Publication Title: Presence
Volume: 14
Issue: 4},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\K8W3BU52\\bailenson2005.pdf:application/pdf},
}

@report{haake_look_2009,
	title = {A Look at the Roles of Look \& Roles in Embodied Pedagogical Agents-A User Preference Perspective. {LINEAGE} View project Maximizing informativeness and minimizing neglect-the next step in feedback research View project},
	url = {https://www.researchgate.net/publication/220049803},
	author = {Haake, Magnus and Gulz, Agneta},
	date = {2009},
	note = {Publication Title: Article in International Journal of Artificial Intelligence},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\ALFMNXU4\\IJAIED707Haakev7.pdf:application/pdf},
}

@inproceedings{klausen_signalling_2022,
	title = {Signalling Emotions with a Breathing Soft Robot},
	isbn = {978-1-66540-828-8},
	doi = {10.1109/RoboSoft54090.2022.9762140},
	abstract = {A novel abstract non-humanoid soft robot with four pneumatically actuated chambers was developed with the aim to signal specific emotions by altering its shape, movements, and breathing rates. Through a user study we investigated how observers perceived the robot's emotional state at different breathing rates. An online questionnaire utilizing the Self-Assessment Manikin scale was used to evaluate pleasure, arousal, and dominance. Our findings show that a slow breathing rate between 7-12.5 breaths per minute corresponds to a high level of pleasure, whereas a high breathing rate of 40 breaths per minute corresponds to a high level of arousal. Participants' gender, in addition, influences the perception of pleasure and arousal at different breathing rates. The findings demonstrate the possibility of signalling emotions through breathing patterns with a non-humanoid soft robot.},
	pages = {194--200},
	booktitle = {2022 {IEEE} 5th International Conference on Soft Robotics, {RoboSoft} 2022},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Klausen, Troels Aske and Farhadi, Ulrich and Vlachos, Evgenios and Jorgensen, Jonas},
	date = {2022},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\8D3P2QHH\\Signalling_Emotions_with_a_Breathing_Soft_Robot_correct_version_.pdf:application/pdf},
}

@inproceedings{terzioglu_designing_2020,
	title = {Designing social cues for collaborative robots: The role of gaze and breathing in human-robot collaboration},
	isbn = {978-1-4503-6746-2},
	doi = {10.1145/3319502.3374829},
	abstract = {In this paper, we investigate how collaborative robots, or cobots, typically composed of a robotic arm and a gripper carrying out manipulation tasks alongside human coworkers, can be enhanced with {HRI} capabilities by applying ideas and principles from character animation. To this end, we modified the appearance and behaviors of a cobot, with minimal impact on its functionality and performance, and studied the extent to which these modifications improved its communication with and perceptions by human collaborators. Specifically, we aimed to improve the Appeal of the robot by manipulating its physical appearance, posture, and gaze, creating an animal-like character with a head-on-neck morphology; to utilize Arcs by generating smooth trajectories for the robot arm; and to increase the lifelikeness of the robot through Secondary Action by adding breathing motions to the robot. In two user studies, we investigated the effects of these cues on collaborator perceptions of the robot. Findings from our first study showed breathing to have a positive effect on most measures of robot perception and reveal nuanced interactions among the other factors. Data from our second study showed that, using gaze cues alone, a robot arm can improve metrics such as likeability and perceived sociability.},
	pages = {343--357},
	booktitle = {{ACM}/{IEEE} International Conference on Human-Robot Interaction},
	publisher = {{IEEE} Computer Society},
	author = {Terzioglu, Yunus and Mutlu, Bilge and Sahin, Erol},
	date = {2020-03-09},
	note = {{ISSN}: 21672148},
	keywords = {Animation principles, Character design, Collaborative robots, Human-robot collaboration, Robot motion, Social cues},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\4EL7AVYV\\breathing_robot_human_collaboration.pdf:application/pdf},
}

@inproceedings{Szekely2019,
	title = {How to train your fillers: uh and um in spontaneous speech synthesis},
	doi = {10.21437/ssw.2019-44},
	abstract = {Using spontaneous conversational speech for {TTS} raises questions on how disfluencies such as filled pauses ({FPs}) should be approached. Detailed annotation of {FPs} in training data enables precise control at synthesis time; coarse or nonexistent {FP} annotation, when combined with stochastic attention-based neural {TTS}, leads to synthesisers that insert these phenomena into fluent prompts on their own accord. In this study we investigate, objectively and subjectively, the effects of {FP} annotation and the impact of relinquishing control over {FPs} in a Tacotron {TTS} system. The training corpus comprised 9 hours of singlespeaker breath groups extracted from a conversational podcast. Systems trained with no or location-only {FP} annotation were found to reproduce {FP} locations and types (uh/um) in a pattern broadly similar to that of the corpus. We also studied the effect of {FPs} on natural and synthetic speech rate and the interchangeability of {FP} types. Interestingly, subjective tests indicate that synthesiser-predicted {FP} types from location-only annotation often were preferred over specifying the ground-truth type. In contrast, a more precise annotation, allowing us to focus training on the most fluent parts of the corpus, improved rated naturalness when synthesising fluent speech.},
	pages = {245--250},
	publisher = {International Speech Communication Association},
	author = {Székely, Éva and Eje Henter, Gustav and Beskow, Jonas and Gustafson, Joakim},
	date = {2019-09-14},
	keywords = {★},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\BCTKFAS6\\how_to_train_your_fillers.pdf:application/pdf},
}

@report{liu_beat_2022,
	title = {{BEAT}: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis},
	abstract = {Fig. 1. Overview. {BEAT} is a large-scale, multi-modal mo-cap human gestures dataset with semantic, emotional annotations, diverse speakers and multiple languages. Abstract. Achieving realistic, vivid, and human-like synthesized conversational gestures conditioned on multi-modal data is still an unsolved problem, due to the lack of available datasets, models and standard evaluation metrics. To address this, we build Body-Expression-Audio-Text dataset, {BEAT}, which has i) 76 hours, high-quality, multi-modal data captured from 30 speakers talking with eight different emotions and in four different languages, ii) 32 millions frame-level emotion and semantic relevance annotations. Our statistical analysis on {BEAT} demonstrates the correlation of conversational gestures with facial expressions, emotions, and semantics, in addition to the known correlation with audio , text, and speaker identity. Based on this observation, we propose a baseline model, Cascaded Motion Network ({CaMN}), which consists of above six modalities modeled in a cascaded architecture for gesture synthesis. To evaluate the diversity of synthesized gestures, we introduce a metric, Semantic Relevance Gesture Recall ({SRGR}). Qualitative and quantitative experiments demonstrate metrics' validness, ground truth data quality, and baseline's state-of-the-art performance. To the best of our knowledge, {BEAT} is the largest motion capture dataset for investigating human gestures, which may contribute to a number of different research fields including controllable gesture synthesis, cross-modality analysis, emotional gesture recognition. The data, code and model will be released for research. 2 H. Liu et al.},
	author = {Liu, Haiyang and Zhu, Zihao and Iwamoto, Naoya and Peng, Yichen and Li, Zhengqing and Zhou, You and Bozkurt, Elif and Zheng, Bo},
	date = {2022},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\HFCUJIC8\\BEAT_A_Large-Scale_Semantic_and_Emotional_Multi-Mo.pdf:application/pdf},
}

@report{pfeifer_should_2009,
	title = {Should Agents Speak Like, um, Humans? The Use of Conversational Fillers by Virtual Agents},
	abstract = {We describe the design and evaluation of an agent that uses the fillers um and uh in its speech. We describe an empirical study of human-human dialogue , analyzing gaze behavior during the production of fillers and use this data to develop a model of agent-based gaze behavior. We find that speakers are significantly more likely to gaze away from their dialogue partner while uttering fillers, especially if the filler occurs at the beginning of a speaking turn. This model is evaluated in a preliminary experiment. Results indicate mixed attitudes towards an agent that uses conversational fillers in its speech.},
	pages = {460--466},
	author = {Pfeifer, Laura M and Bickmore, Timothy},
	date = {2009},
	note = {Publication Title: {LNAI}
Volume: 5773},
	keywords = {embodied conversational agent, filled pause, fillers, gaze},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\5ZWIVAMS\\should_agents_speak_with_fillers.pdf:application/pdf},
}

@report{cassell_beat_2004,
	title = {{BEAT}: the Behavior Expression Animation Toolkit},
	abstract = {The Behavior Expression Animation Toolkit ({BEAT}) allows animators to input typed text that they wish to be spoken by an animated human figure, and to obtain as output appropriate and synchronized nonverbal behaviors and synthesized speech in a form that can be sent to a number of different animation systems. The nonverbal behaviors are assigned on the basis of actual linguistic and contextual analysis of the typed text, relying on rules derived from extensive research into human conversational behavior. The toolkit is extensible, so that new rules can be quickly added. It is designed to plug into larger systems that may also assign personality profiles, motion characteristics, scene constraints, or the animation styles of particular animators.},
	author = {Cassell, Justine and Vilhjálmsson, Hannes Högni and Bickmore, Timothy},
	date = {2004},
	keywords = {Animation Systems, Facial Animation, Gesture, Speech Synthesis},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\HPNXAK4P\\beat-behavorial-animation-toolkit.pdf:application/pdf},
}

@report{bergmann_increasing_2009,
	title = {Increasing the Expressiveness of Virtual Agents-Autonomous Generation of Speech and Gesture for Spatial Description Tasks},
	url = {www.ifaamas.org},
	abstract = {Embodied conversational agents are required to be able to express themselves convincingly and autonomously. Based on an empirial study on spatial descriptions of landmarks in direction-giving, we present a model that allows virtual agents to automatically generate, i.e., select the content and derive the form of coordinated language and iconic gestures. Our model simulates the interplay between these two modes of expressiveness on two levels. First, two kinds of knowledge representation (propositional and imagistic) are utilized to capture the modality-specific contents and processes of content planning. Second, specific planners are integrated to carry out the formulation of concrete verbal and gestural behavior. A probabilistic approach to gesture formulation is presented that incorporates multiple contextual factors as well as idiosyncratic patterns in the mapping of visuo-spatial referent properties onto gesture morphology. Results from a prototype implementation are described.},
	author = {Bergmann, Kirsten and Kopp, Stefan},
	date = {2009},
	keywords = {D22 [Software Engineering]: Design Tools and Techniques-User Interfaces General Terms Design, Experimentation, Theory Keywords Gesture, language, expressiveness, multimodal output, em-bodied conversational agents, I20 [Artificial Intelligence]: General-Cognitive Simula-tion, I21 [Artificial Intelligence]: Applications and Ex-pert Systems-Natural Language Interfaces, I211 [Artificial Intelligence]: Distributed Artificial Intelligence-Intelli-gent Agents},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\P7RUMPIS\\Increasing_the_expressiveness_of_virtual.pdf:application/pdf},
}

@report{batliner_you_2004,
	title = {"You stupid tin box"-children interacting with the {AIBO} robot: A cross-linguistic emotional speech corpus},
	url = {http://pfstar.itc.it/},
	abstract = {This paper deals with databases that combine different aspects: children's speech, emotional speech, human-robot communication, cross-linguistics, and read vs. spontaneous speech: in a Wizard-of-Oz scenario, German and English children had to instruct Sony's {AIBO} robot to fulfil specific tasks. In one experimental condition, strictly parallel for German and English, the {AIBO} behaved 'disobedient' by following it's own script irrespective of the child's commands. By that, reactions of different children to the same sequence of {AIBO}'s actions could be obtained. In addition, both the German and the English children were recorded reading texts. The data are transliterated orthographically; emotional user states and some other phenomena will be annotated. We report preliminary word recognition rates and classification results.},
	author = {Batliner, A and Hacker, C and Steidl, S and Nöth, E and D'arcy, S and Russell, M and Wong, M},
	date = {2004},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\I8HBCAMY\\Batliner04-YST.pdf:application/pdf},
}

@report{stiefelhagen_natural_2004,
	title = {Natural Human-Robot Interaction using Speech, Head Pose and Gestures},
	abstract = {Absfmcf-In this paper we present our ongoing work in building technologis for natural multimodal human-mbot interaction. We present our systems for spontmeous speech r d t i o n , multimodal dialogue processing and visual pcr-ception of a user, which Includes the recognition of pointing gestures as well as the recognition ora person's head orients-tion. Each of the components are described in the paper and experimental resultr are presented. In order to demonstrate and measure the usefulness of such technologies for human-robot interaction, all components have been integrated on B mobile mho1 platform and have been used for real-time human-robot interaction in a kitchen scenario. 1. {\textasciitilde}{NTRODUCTION} bring certain objects or to obtain suggested recipes from the robot. The current components of our system include. a speech recognizer,. 3D face-and hand-tracking, pointing gesture recognition,. recognition of head pose,. a dialogue component, speech synthesis, .' a mobile platfom,. a stereo camera system, including pan-tilt, unit mnmred on the olatform. In the upcoming field of humanoid and buman-friendb robots, the ability of the robot for simple, unconstrained and natural interaction with its users is of central impor-lance [I], [21. The basis for appropriate action ofthe robot must be a comprehensive model of the Curtent {SUrro}{\textasciitilde}nding Figure 1.a) shows a picture of our system and a person interacting with it. Part of the risual tracking components have aheady heen integrated in A R M , 131, a humanoid robot with two arms and 23 degrees of freedom. This robot is depicted in Figure lb).-and in panicular of the humans involved in interaction. To facilitate natural interaction, robots should be able to perceive and understand all the modalities used by humans during face-to-face interaction. Besides speech, as the probably most prominent modality used by humans, these modalities also include pointing gestures, facial expressions , head pose, gaze, eye-contact and body language for example. In our rehearch labs 31 thc U I {\textasciitilde} I V \& {\textasciitilde} I Kxlzruhe {ITHJ} and at Camegie hlellon Uruseriity. uc are de{\textbackslash}clopinp technologies for the undcr4anding o l 1hr.w human {\textasciitilde}nlcrartion mdalities. In particular in the frmeu,ork of 3 German research project on humanoid robots (Sonderforschungs-bereich Humanoide Roboter, {SFB} 588) we have been working using and improving such technologies to provide for natural interaction between a humanoid robot and its users. In this paper we present our work in this area. We have developed components for speech recognition, multi-modal dialogue processing, visual detection and modeling of users, including head pose estimation and pointing gesture recognition. All components have been integrated on a mobile robot platform and can he used for real-time multimodal interaction with a robot. The target scenario we addressed is a household situation , in which a human can ask the robot questions related to the kitchen (such as 'What's in the fridge ?"), ask the robot to set the table, to switch certain lights on or off, to a) b) Fig. I {FIG}.1 A) {INTERACTION} {WITH} {OUR} {DEVELOPMENT} {SYSTEM}.},
	author = {Stiefelhagen, R and Fugen, C and Gieselmann, P and Holzapfel, H and Nickel, K and Waibel, A},
	date = {2004},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\Z7KZJSIX\\iros.2004.1389771.pdf:application/pdf},
}

@report{bernardet_speech_2019,
	title = {Speech Breathing in Virtual Humans: An Interactive Model and Empirical Study},
	abstract = {Figure 1: A model for simulating speech breathing. Breaths are added to synthetic speech based on lung capacity, speaking loudness, mouth or nose breathing and other factors. {ABSTRACT} Human speech production requires the dynamic regulation of air through the vocal system. While virtual character systems commonly are capable of speech output, they rarely take breathing during speaking-speech breathing-into account. We believe that integrating dynamic speech breathing systems in virtual characters can significantly contribute to augmenting their realism. Here, we present a novel control architecture aimed at generating speech breathing in virtual characters. This architecture is informed by behavioral, linguistic and anatomical knowledge of human speech breathing. Based on textual input and controlled by a set of low-and high-level parameters, the system produces dynamic signals in real-time that control the virtual character's anatomy (thorax, abdomen , head, nostrils, and mouth) and sound production (speech and breathing). In addition, we perform a study to determine the effects of including breathing-motivated speech movements, such as head tilts and chest expansions during dialogue on a virtual character, as well as breathing sounds. This study includes speech that is generated both from a text-to-speech engine as well as from recorded voice.},
	author = {Bernardet, Ulysses and Feng, Andrew and Dipaola, Steve and Shapiro, Ari},
	date = {2019},
	keywords = {Index Terms, Human-centered computing, Virtual reality, Procedural animation},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\DDENUGSA\\bernardet2019.pdf:application/pdf},
}

@article{yan_adaspeech_2021,
	title = {{AdaSpeech} 3: Adaptive Text to Speech for Spontaneous Style},
	url = {http://arxiv.org/abs/2107.02530},
	abstract = {While recent text to speech ({TTS}) models perform very well in synthesizing reading-style (e.g., audiobook) speech, it is still challenging to synthesize spontaneous-style speech (e.g., podcast or conversation), mainly because of two reasons: 1) the lack of training data for spontaneous speech; 2) the difficulty in modeling the filled pauses (um and uh) and diverse rhythms in spontaneous speech. In this paper, we develop {AdaSpeech} 3, an adaptive {TTS} system that fine-tunes a well-trained reading-style {TTS} model for spontaneous-style speech. Specifically, 1) to insert filled pauses ({FP}) in the text sequence appropriately, we introduce an {FP} predictor to the {TTS} model; 2) to model the varying rhythms, we introduce a duration predictor based on mixture of experts ({MoE}), which contains three experts responsible for the generation of fast, medium and slow speech respectively, and fine-tune it as well as the pitch predictor for rhythm adaptation; 3) to adapt to other speaker timbre, we fine-tune some parameters in the decoder with few speech data. To address the challenge of lack of training data, we mine a spontaneous speech dataset to support our research this work and facilitate future research on spontaneous {TTS}. Experiments show that {AdaSpeech} 3 synthesizes speech with natural {FP} and rhythms in spontaneous styles, and achieves much better {MOS} and {SMOS} scores than previous adaptive {TTS} systems.},
	author = {Yan, Yuzi and Tan, Xu and Li, Bohan and Zhang, Guangyan and Qin, Tao and Zhao, Sheng and Shen, Yuan and Zhang, Wei-Qiang and Liu, Tie-Yan},
	date = {2021-07-06},
	eprinttype = {arxiv},
	eprint = {2107.02530},
	keywords = {★},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\67FUBFJ7\\AdaSpeech_3_Adaptive_Text_to_Speech_for_Spontaneou.pdf:application/pdf},
}

@inproceedings{kirkland_wheres_2022,
	title = {Where's the uh, hesitation? The interplay between filled pause location, speech rate and fundamental frequency in perception of confidence},
	volume = {2022-September},
	doi = {10.21437/Interspeech.2022-10973},
	abstract = {Much of the research investigating the perception of speaker certainty has relied on either attempting to elicit prosodic features in read speech, or artificial manipulation of recorded audio. Our novel method of controlling prosody in synthesized spontaneous speech provides a powerful tool for studying speech perception and can provide better insight into the interacting effects of prosodic features on perception while also paving the way for conversational systems which are more effectively able to engage in and respond to social behaviors. Here we have used this method to examine the combined impact of filled pause location, speech rate and f0 on the perception of speaker confidence. We found an additive effect of all three features. The most confident-sounding utterances had no filler, low f0 and high speech rate, while the least confident-sounding utterances had a medial filled pause, high f0 and low speech rate. Insertion of filled pauses had the strongest influence, but pitch and speaking rate could be used to more finely control the uncertainty cues in spontaneous speech synthesis.},
	pages = {4990--4994},
	booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, {INTERSPEECH}},
	publisher = {International Speech Communication Association},
	author = {Kirkland, Ambika and Lameris, Harm and Székely, Éva and Gustafson, Joakim},
	date = {2022},
	note = {{ISSN}: 19909772},
	keywords = {speech synthesis, expressive speech synthesis, paralinguistics, speech perception},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\WQUFFBJM\\kirkland22_interspeech.pdf:application/pdf},
}

@article{drolet_authenticity_2012,
	title = {Authenticity affects the recognition of emotions in speech: Behavioral and {fMRI} evidence},
	volume = {12},
	issn = {15307026},
	doi = {10.3758/s13415-011-0069-3},
	abstract = {The aim of the present study was to determine how authenticity of emotion expression in speech modulates activity in the neuronal substrates involved in emotion recognition. Within an {fMRI} paradigm, participants judged either the authenticity (authentic or play acted) or emotional content (anger, fear, joy, or sadness) of recordings of spontaneous emotions and reenactments by professional actors. When contrasting between task types, active judgment of authenticity, more than active judgment of emotion, indicated potential involvement of the theory of mind ({ToM}) network (medial prefrontal cortex, temporoparietal cortex, retrosplenium) as well as areas involved in working memory and decision making ({BA} 47). Subsequently, trials with authentic recordings were contrasted with those of reenactments to determine the modulatory effects of authenticity. Authentic recordings were found to enhance activity in part of the {ToM} network (medial prefrontal cortex). This effect of authenticity suggests that individuals integrate recollections of their own experiences more for judgments involving authentic stimuli than for those involving play-acted stimuli. The behavioral and functional results show that authenticity of emotional prosody is an important property influencing human responses to such stimuli, with implications for studies using play-acted emotions. © The Author(s) 2011.},
	pages = {140--150},
	number = {1},
	journaltitle = {Cognitive, Affective and Behavioral Neuroscience},
	author = {Drolet, Matthis and Schubotz, Ricarda I. and Fischer, Julia},
	date = {2012-03},
	pmid = {22038706},
	keywords = {Affect, Episodic memory, Intonation, Prosodym mentalizing, Theory of mind},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\5K5CYKM2\\s13415-011-0069-3.pdf:application/pdf},
}

@inproceedings{kirkland_perception_2021,
	title = {Perception of smiling voice in spontaneous speech synthesis},
	doi = {10.21437/ssw.2021-19},
	pages = {108--112},
	publisher = {International Speech Communication Association},
	author = {Kirkland, Ambika and Włodarczak, Marcin and Gustafson, Joakim and Szekely, Eva},
	date = {2021-08-24},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\IG4NNNKX\\kirkland21_ssw.pdf:application/pdf},
}

@article{moullec_multi-sensory_2022,
	title = {Multi-sensory display of self-avatar's physiological state: virtual breathing and heart beating can increase sensation of effort in {VR}},
	volume = {2022},
	issn = {1077-2626},
	url = {https://www.ieee.org/publications/rights/index.html},
	doi = {10.1109/TVCG.2022.3203120ï},
	abstract = {sensory display of self-avatar's physiological state: virtual breathing and heart beating can increase sensation of effort in {VR}. elanie {CognéCogn}´Cogné and Anatole L ´ ecuyer Fig. 1. We propose a novel approach to increase the connection with a self-avatar in virtual reality (A), by displaying its physiological state and physical exertion. It is based on a multi-sensory setup (B) involving visual, auditory and haptic displays. It includes visual effects such as a periphery overlay (C) simulating heart beating ; or haptic stimulation delivered with a piezoelectric actuator (D) and a novel compression belt (E) which exerts pressure on the abdomen to simulate a virtual breathing. Abstract-In this paper we explore the multi-sensory display of self-avatars' physiological state in Virtual Reality ({VR}), as a means to enhance the connection between the users and their avatar. Our approach consists in designing and combining a coherent set of visual, auditory and haptic cues to represent the avatar's cardiac and respiratory activity. These sensory cues are modulated depending on the avatar's simulated physical exertion. We notably introduce a novel haptic technique to represent respiratory activity using a compression belt simulating abdominal movements that occur during a breathing cycle. A series of experiments was conducted to evaluate the influence of our multi-sensory rendering techniques on various aspects of the {VR} user experience, including the sense of virtual embodiment and the sensation of effort during a walking simulation. A first study (N=30) that focused on displaying cardiac activity showed that combining sensory modalities significantly enhances the sensation of effort. A second study (N=20) that focused on respiratory activity showed that combining sensory modalities significantly enhances the sensation of effort as well as two sub-components of the sense of embodiment. Interestingly, the user's actual breathing tended to synchronize with the simulated breathing, especially with the multi-sensory and haptic displays. A third study (N=18) that focused on the combination of cardiac and respiratory activity showed that combining both rendering techniques significantly enhances the sensation of effort. Taken together, our results promote the use of our novel breathing display technique and multi-sensory rendering of physiological parameters in {VR} applications where effort sensations are prominent, such as for rehabilitation, sport training, or exergames.},
	pages = {3596},
	number = {11},
	author = {Moullec, Yann and Saint-Aubert, Justine and Manson, Julien and Cogne, Melanie and Lécuyer, Anatole and Lécuyer Multi-, Anatole},
	date = {2022},
	keywords = {Avatar, cardiac, effort sensation, embodiment, haptic, multi-sensory display, physiological computing, respiration},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\U9HHY49U\\Multi-sensory_display_of_self-avatars_physiological_state_virtual_breathing_and_heart_beating_can_increase_sensation_of_effort_in_VR-3.pdf:application/pdf},
}

@report{guo_research_2011,
	title = {Research on Lhasa Tibetan Prosodic Model of Journalese Based on Respiratory Signal},
	abstract = {In accordance with the actual development for Tibetan speech synthesis, the paper has taken news text as training corpora, analyzed the speech and prosodic features of Tibetan Lhasa dialect and confirmed the respiratory signal parameters with prosodic features. It has confirmed 6 classes of 39 dimensions context feature parameters in terms of previous prosodic structure analysis results. It uses {RBF} neural network to establish prosodic model and output 10 dimensions prosodic control parameters, and testing to know the predictable nature of the established model.},
	author = {Guo, Shuni and Gao, Lu and Yu, Hongzhi},
	date = {2011},
	keywords = {Prosodic model, {RBF}, respiratory signal, Tibetan Lhasa dialect},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\UPJ659TU\\iasp.2011.6108991.pdf:application/pdf},
}

@inproceedings{szekely_casting_2019,
	title = {Casting to Corpus: Segmenting and Selecting Spontaneous Dialogue for Tts with a Cnn-lstm Speaker-dependent Breath Detector},
	volume = {2019-May},
	isbn = {978-1-4799-8131-1},
	doi = {10.1109/ICASSP.2019.8683846},
	abstract = {This paper considers utilising breaths to create improved spontaneous-speech corpora for conversational text-to-speech from found audio recordings such as dialogue podcasts. Breaths are of interest since they relate to prosody and speech planning and are independent of language and transcription. Specifically, we propose a semi-supervised approach where a fraction of coarsely annotated data is used to train a convolutional and recurrent speaker-specific breath detector operating on spectrograms and zero-crossing rate. The classifier output is used to find target-speaker breath groups (audio segments delineated by breaths) and subsequently select those that constitute clean utterances appropriate for a synthesis corpus. An application to 11 hours of raw podcast audio extracts 1969 utterances (106 minutes), 87\% of which are clean and correctly segmented. This outperforms a baseline that performs integrated {VAD} and speaker attribution without accounting for breaths.},
	pages = {6925--6929},
	booktitle = {{ICASSP}, {IEEE} International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Székely, Éva and Henter, Gustav Eje and Gustafson, Joakim},
	date = {2019-05-01},
	note = {{ISSN}: 15206149},
	keywords = {Speech Synthesis, Spontaneous speech, Computational Paralinguistics, Breath detection, Found Data},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\RXXV5A6X\\szekely2019casting.pdf:application/pdf},
}

@report{szekely_spontaneous_2019,
	title = {Spontaneous conversational speech synthesis from found data},
	url = {https://prolific.ac/},
	abstract = {Synthesising spontaneous speech is a difficult task due to disflu-encies, high variability and syntactic conventions different from those of written language. Using found data, as opposed to lab-recorded conversations, for speech synthesis adds to these challenges because of overlapping speech and the lack of control over recording conditions. In this paper we address these challenges by using a speaker-dependent {CNN}-{LSTM} breath detector to separate continuous recordings into utterances, which we here apply to extract nine hours of clean single-speaker breath groups from a conversational podcast. The resulting corpus is transcribed automatically (both lexical items and filler tokens) and used to build several voices on a Tacotron 2 architecture. Listening tests show: i) pronunciation accuracy improved with phonetic input and transfer learning; ii) it is possible to create a more fluent conversational voice by training on data without filled pauses; and iii) the presence of filled pauses improved perceived speaker authenticity. Another listening test showed the found podcast voice to be more appropriate for prompts from both public speeches and casual conversations , compared to synthesis from found read speech and from a manually transcribed lab-recorded spontaneous conversation.},
	author = {Székely, Éva and Henter, Gustav Eje and Beskow, Jonas and Gustafson, Joakim},
	date = {2019},
	keywords = {found data, conversational speech, disfluencies, hesitations, Index Terms: Speech synthesis, spon-taneous speech},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\IWDBZAGS\\szekely2019spontaneous.pdf:application/pdf},
}

@report{mostaani_breathing_2022,
	title = {On Breathing Pattern Information in Synthetic Speech},
	abstract = {The respiratory system is an integral part of human speech production. As a consequence, there is a close relation between respiration and speech signal, and the produced speech signal carries breathing pattern related information. Speech can also be generated using speech synthesis systems. In this paper , we investigate whether synthetic speech carries breathing pattern related information in the same way as natural human speech. We address this research question in the framework of logical-access presentation attack detection using embeddings extracted from neural networks pre-trained for speech breathing pattern estimation. Our studies on {ASVSpoof} 2019 challenge data show that there is a clear distinction between the extracted breathing pattern embedding of natural human speech and synthesized speech, indicating that speech synthesis systems tend to not carry breathing pattern related information in the same way as human speech. Whilst, this is not the case with voice conversion of natural human speech.},
	author = {Mostaani, Zohreh and Doss, Mathew Magimai -},
	date = {2022},
	keywords = {Index Terms: Breathing pattern estimation, Neural network, Presentation attack detection, Synthetic speech},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\K5DGYUTC\\Mostaani_INTERSPEECH_2022.pdf:application/pdf},
}

@inproceedings{schuller_interspeech_2020,
	title = {The {INTERSPEECH} 2020 computational paralinguistics challenge: Elderly emotion, breathing \& masks},
	volume = {2020-October},
	doi = {10.21437/Interspeech.2020-32},
	abstract = {The {INTERSPEECH} 2020 Computational Paralinguistics Challenge addresses three different problems for the first time in a research competition under well-defined conditions: In the Elderly Emotion Sub-Challenge, arousal and valence in the speech of elderly individuals have to be modelled as a 3-class problem; in the Breathing Sub-Challenge, breathing has to be assessed as a regression problem; and in the Mask Sub-Challenge, speech without and with a surgical mask has to be told apart. We describe the Sub-Challenges, baseline feature extraction, and classifiers based on the 'usual' {COMPARE} and {BoAW} features as well as deep unsupervised representation learning using the {AUDEEP} toolkit, and deep feature extraction from pre-trained {CNNs} using the {DEEP} {SPECTRUM} toolkit; in addition, we partially add deep end-to-end sequential modelling, and, for the first time in the challenge, linguistic analysis.},
	pages = {2042--2046},
	booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, {INTERSPEECH}},
	publisher = {International Speech Communication Association},
	author = {Schuller, Björn W. and Batliner, Anton and Bergler, Christian and Messner, Eva Maria and Hamilton, Antonia and Amiriparian, Shahin and Baird, Alice and Rizos, Georgios and Schmitt, Maximilian and Stappen, Lukas and Baumeister, Harald and {MacIntyre}, Alexis Deighton and Hantke, Simone},
	date = {2020},
	note = {{ISSN}: 19909772},
	keywords = {Breathing, Challenge, Computational Paralinguistics, Elderly Emotion, Speech under Mask},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\Z3GQ77ME\\schuller20_interspeech.pdf:application/pdf},
}

@article{mehta_neural_2021,
	title = {Neural {HMMs} are all you need (for high-quality attention-free {TTS})},
	url = {http://arxiv.org/abs/2108.13320},
	doi = {10.1109/ICASSP43922.2022.9746686},
	abstract = {Neural sequence-to-sequence {TTS} has achieved significantly better output quality than statistical speech synthesis using {HMMs}. However, neural {TTS} is generally not probabilistic and uses non-monotonic attention. Attention failures increase training time and can make synthesis babble incoherently. This paper describes how the old and new paradigms can be combined to obtain the advantages of both worlds, by replacing attention in neural {TTS} with an autoregressive left-right no-skip hidden Markov model defined by a neural network. Based on this proposal, we modify Tacotron 2 to obtain an {HMM}-based neural {TTS} model with monotonic alignment, trained to maximise the full sequence likelihood without approximation. We also describe how to combine ideas from classical and contemporary {TTS} for best results. The resulting example system is smaller and simpler than Tacotron 2, and learns to speak with fewer iterations and less data, whilst achieving comparable naturalness prior to the post-net. Our approach also allows easy control over speaking rate.},
	author = {Mehta, Shivam and Székely, Éva and Beskow, Jonas and Henter, Gustav Eje},
	date = {2021-08-30},
	eprinttype = {arxiv},
	eprint = {2108.13320},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\S3P3FE6V\\2108.13320.pdf:application/pdf},
}

@article{kong_hifi-gan_2020,
	title = {{HiFi}-{GAN}: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis},
	url = {http://arxiv.org/abs/2010.05646},
	abstract = {Several recent work on speech synthesis have employed generative adversarial networks ({GANs}) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose {HiFi}-{GAN}, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, {MOS}) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 {kHz} high-fidelity audio 167.9 times faster than real-time on a single V100 {GPU}. We further show the generality of {HiFi}-{GAN} to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of {HiFi}-{GAN} generates samples 13.4 times faster than real-time on {CPU} with comparable quality to an autoregressive counterpart.},
	author = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
	date = {2020-10-12},
	eprinttype = {arxiv},
	eprint = {2010.05646},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\S5TCL3W4\\2010.05646.pdf:application/pdf},
}

@article{tan_naturalspeech_2022,
	title = {{NaturalSpeech}: End-to-End Text to Speech Synthesis with Human-Level Quality},
	url = {http://arxiv.org/abs/2205.04421},
	abstract = {Text to speech ({TTS}) has made rapid progress in both academia and industry in recent years. Some questions naturally arise that whether a {TTS} system can achieve human-level quality, how to define/judge that quality and how to achieve it. In this paper, we answer these questions by first defining the human-level quality based on the statistical significance of subjective measure and introducing appropriate guidelines to judge it, and then developing a {TTS} system called {NaturalSpeech} that achieves human-level quality on a benchmark dataset. Specifically, we leverage a variational autoencoder ({VAE}) for end-to-end text to waveform generation, with several key modules to enhance the capacity of the prior from text and reduce the complexity of the posterior from speech, including phoneme pre-training, differentiable duration modeling, bidirectional prior/posterior modeling, and a memory mechanism in {VAE}. Experiment evaluations on popular {LJSpeech} dataset show that our proposed {NaturalSpeech} achieves -0.01 {CMOS} (comparative mean opinion score) to human recordings at the sentence level, with Wilcoxon signed rank test at p-level p {\textgreater}{\textgreater} 0.05, which demonstrates no statistically significant difference from human recordings for the first time on this dataset.},
	author = {Tan, Xu and Chen, Jiawei and Liu, Haohe and Cong, Jian and Zhang, Chen and Liu, Yanqing and Wang, Xi and Leng, Yichong and Yi, Yuanhao and He, Lei and Soong, Frank and Qin, Tao and Zhao, Sheng and Liu, Tie-Yan},
	date = {2022-05-09},
	eprinttype = {arxiv},
	eprint = {2205.04421},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\LG874ZI2\\2205.04421.pdf:application/pdf},
}

@article{tan_survey_2021,
	title = {A Survey on Neural Speech Synthesis},
	url = {http://arxiv.org/abs/2106.15561},
	abstract = {Text to speech ({TTS}), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based {TTS} has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural {TTS}, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural {TTS}, including text analysis, acoustic models and vocoders, and several advanced topics, including fast {TTS}, low-resource {TTS}, robust {TTS}, expressive {TTS}, and adaptive {TTS}, etc. We further summarize resources related to {TTS} (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on {TTS}.},
	author = {Tan, Xu and Qin, Tao and Soong, Frank and Liu, Tie-Yan},
	date = {2021-06-29},
	eprinttype = {arxiv},
	eprint = {2106.15561},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\DHCXV2G6\\TTS_survey.pdf:application/pdf},
}

@article{binkowski_high_2019,
	title = {High Fidelity Speech Synthesis with Adversarial Networks},
	url = {http://arxiv.org/abs/1909.11646},
	abstract = {Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as {WaveNet}, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce {GAN}-{TTS}, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced. To measure the performance of {GAN}-{TTS}, we employ both subjective human evaluation ({MOS} - Mean Opinion Score), as well as novel quantitative metrics (Fr{\textbackslash}'echet {DeepSpeech} Distance and Kernel {DeepSpeech} Distance), which we find to be well correlated with {MOS}. We show that {GAN}-{TTS} is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to {GAN}-{TTS} reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav.},
	author = {Bińkowski, Mikołaj and Donahue, Jeff and Dieleman, Sander and Clark, Aidan and Elsen, Erich and Casagrande, Norman and Cobo, Luis C. and Simonyan, Karen},
	date = {2019-09-25},
	eprinttype = {arxiv},
	eprint = {1909.11646},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\S98CGIU6\\gan-tts.pdf:application/pdf},
}

@article{ren_fastspeech_2020,
	title = {{FastSpeech} 2: Fast and High-Quality End-to-End Text to Speech},
	url = {http://arxiv.org/abs/2006.04558},
	abstract = {Non-autoregressive text to speech ({TTS}) models such as {FastSpeech} can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of {FastSpeech} model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in {TTS}. However, {FastSpeech} has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose {FastSpeech} 2, which addresses the issues in {FastSpeech} and better solves the one-to-many mapping problem in {TTS} by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design {FastSpeech} 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) {FastSpeech} 2 achieves a 3x training speed-up over {FastSpeech}, and {FastSpeech} 2s enjoys even faster inference speed; 2) {FastSpeech} 2 and 2s outperform {FastSpeech} in voice quality, and {FastSpeech} 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.},
	author = {Ren, Yi and Hu, Chenxu and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
	date = {2020-06-08},
	eprinttype = {arxiv},
	eprint = {2006.04558},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\XU7Z777W\\fastspeech2.pdf:application/pdf},
}

@article{shen_natural_2017,
	title = {Natural {TTS} Synthesis by Conditioning {WaveNet} on Mel Spectrogram Predictions},
	url = {http://arxiv.org/abs/1712.05884},
	abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified {WaveNet} model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score ({MOS}) of \$4.53\$ comparable to a {MOS} of \$4.58\$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to {WaveNet} instead of linguistic, duration, and \$F\_0\$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the {WaveNet} architecture.},
	author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerry-Ryan, {RJ} and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
	date = {2017-12-15},
	eprinttype = {arxiv},
	eprint = {1712.05884},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\FF6N399B\\tacotron2.pdf:application/pdf},
}

@article{chen_adaspeech_2021,
	title = {{AdaSpeech}: Adaptive Text to Speech for Custom Voice},
	url = {http://arxiv.org/abs/2103.00993},
	abstract = {Custom voice, a specific text to speech ({TTS}) service in commercial speech platforms, aims to adapt a source {TTS} model to synthesize personal voice for a target speaker using few speech data. Custom voice presents two unique challenges for {TTS} adaptation: 1) to support diverse customers, the adaptation model needs to handle diverse acoustic conditions that could be very different from source speech data, and 2) to support a large number of customers, the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality. In this work, we propose {AdaSpeech}, an adaptive {TTS} system for high-quality and efficient customization of new voices. We design several techniques in {AdaSpeech} to address the two challenges in custom voice: 1) To handle different acoustic conditions, we use two acoustic encoders to extract an utterance-level vector and a sequence of phoneme-level vectors from the target speech during training; in inference, we extract the utterance-level vector from a reference speech and use an acoustic predictor to predict the phoneme-level vectors. 2) To better trade off the adaptation parameters and voice quality, we introduce conditional layer normalization in the mel-spectrogram decoder of {AdaSpeech}, and fine-tune this part in addition to speaker embedding for adaptation. We pre-train the source {TTS} model on {LibriTTS} datasets and fine-tune it on {VCTK} and {LJSpeech} datasets (with different acoustic conditions from {LibriTTS}) with few adaptation data, e.g., 20 sentences, about 1 minute speech. Experiment results show that {AdaSpeech} achieves much better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice. Audio samples are available at https://speechresearch.github.io/adaspeech/.},
	author = {Chen, Mingjian and Tan, Xu and Li, Bohan and Liu, Yanqing and Qin, Tao and Zhao, Sheng and Liu, Tie-Yan},
	date = {2021-03-01},
	eprinttype = {arxiv},
	eprint = {2103.00993},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\I2MJXZNI\\ada.pdf:application/pdf},
}

@article{nallanthighal_deep_2021,
	title = {Deep learning architectures for estimating breathing signal and respiratory parameters from speech recordings},
	volume = {141},
	issn = {18792782},
	doi = {10.1016/j.neunet.2021.03.029},
	abstract = {Respiration is an essential and primary mechanism for speech production. We first inhale and then produce speech while exhaling. When we run out of breath, we stop speaking and inhale. Though this process is involuntary, speech production involves a systematic outflow of air during exhalation characterized by linguistic content and prosodic factors of the utterance. Thus speech and respiration are closely related, and modeling this relationship makes sensing respiratory dynamics directly from the speech plausible, however is not well explored. In this article, we conduct a comprehensive study to explore techniques for sensing breathing signal and breathing parameters from speech using deep learning architectures and address the challenges involved in establishing the practical purpose of this technology. Estimating the breathing pattern from the speech would give us information about the respiratory parameters, thus enabling us to understand the respiratory health using one's speech.},
	pages = {211--224},
	journaltitle = {Neural Networks},
	author = {Nallanthighal, Venkata Srikanth and Mostaani, Zohreh and Härmä, Aki and Strik, Helmer and Magimai-Doss, Mathew},
	date = {2021-09-01},
	pmid = {33915446},
	note = {Publisher: Elsevier Ltd},
	keywords = {Speech breathing, Deep neural networks, Respiratory parameters, Signal processing, Speech technology},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\22A5ZR66\\1-s2.0-S0893608021001179-main.pdf:application/pdf},
}

@article{elias_parallel_2020,
	title = {Parallel Tacotron: Non-Autoregressive and Controllable {TTS}},
	url = {http://arxiv.org/abs/2010.11439},
	abstract = {Although neural end-to-end text-to-speech models can synthesize highly natural speech, there is still room for improvements to its efficiency and naturalness. This paper proposes a non-autoregressive neural text-to-speech model augmented with a variational autoencoder-based residual encoder. This model, called {\textbackslash}emph\{Parallel Tacotron\}, is highly parallelizable during both training and inference, allowing efficient synthesis on modern parallel hardware. The use of the variational autoencoder relaxes the one-to-many mapping nature of the text-to-speech problem and improves naturalness. To further improve the naturalness, we use lightweight convolutions, which can efficiently capture local contexts, and introduce an iterative spectrogram loss inspired by iterative refinement. Experimental results show that Parallel Tacotron matches a strong autoregressive baseline in subjective evaluations with significantly decreased inference time.},
	author = {Elias, Isaac and Zen, Heiga and Shen, Jonathan and Zhang, Yu and Jia, Ye and Weiss, Ron and Wu, Yonghui},
	date = {2020-10-22},
	eprinttype = {arxiv},
	eprint = {2010.11439},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\RDT4H6ME\\parallel_tacotron.pdf:application/pdf},
}

@article{zhang_learning_2018,
	title = {Learning latent representations for style control and transfer in end-to-end speech synthesis},
	url = {http://arxiv.org/abs/1812.04342},
	abstract = {In this paper, we introduce the Variational Autoencoder ({VAE}) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through {VAE} shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of {VAE}, then feeding it into {TTS} network to guide the style in synthesizing speech. To avoid Kullback-Leibler ({KL}) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token ({GST}) model in {ABX} preference tests on style transfer.},
	author = {Zhang, Ya-Jie and Pan, Shifeng and He, Lei and Ling, Zhen-Hua},
	date = {2018-12-11},
	eprinttype = {arxiv},
	eprint = {1812.04342},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\SVHE9SF3\\1812.04342.pdf:application/pdf},
}

@inproceedings{akuzawa_expressive_2018,
	title = {Expressive speech synthesis via modeling expressions with variational autoencoder},
	volume = {2018-September},
	doi = {10.21437/Interspeech.2018-1113},
	abstract = {Recent advances in neural autoregressive models have improve the performance of speech synthesis ({SS}). However, as they lack the ability to model global characteristics of speech (such as speaker individualities or speaking styles), particularly when these characteristics have not been labeled, making neural autoregressive {SS} systems more expressive is still an open issue. In this paper, we propose to combine {VoiceLoop}, an autoregressive {SS} model, with Variational Autoencoder ({VAE}). This approach, unlike traditional autoregressive {SS} systems, uses {VAE} to model the global characteristics explicitly, enabling the expressiveness of the synthesized speech to be controlled in an unsupervised manner. Experiments using the {VCTK} and Bliz-zard2012 datasets show the {VAE} helps {VoiceLoop} to generate higher quality speech and to control the experssions in its synthesized speech by incorporating global characteristics into the speech generating process.},
	pages = {3067--3071},
	booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, {INTERSPEECH}},
	publisher = {International Speech Communication Association},
	author = {Akuzawa, Kei and Iwasawa, Yusuke and Matsuo, Yutaka},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1804.02135},
	note = {{ISSN}: 19909772},
	keywords = {Autoregressive model, Expressive speech synthesis, Variational autoencoder},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\74TLNG9I\\1113.pdf:application/pdf},
}

@inproceedings{markitantov_ensembling_2020,
	title = {Ensembling end-to-end deep models for computational paralinguistics tasks: {ComParE} 2020 mask and breathing sub-challenges},
	volume = {2020-October},
	doi = {10.21437/Interspeech.2020-2666},
	abstract = {This paper describes deep learning approaches for the Mask and Breathing Sub-Challenges ({SCs}), which are addressed by the {INTERSPEECH} 2020 Computational Paralinguistics Challenge. Motivated by outstanding performance of state-of-the-art end-to-end (E2E) approaches, we explore and compare effectiveness of different deep Convolutional Neural Network ({CNN}) architectures on raw data, log Mel-spectrograms, and Mel-Frequency Cepstral Coefficients. We apply a transfer learning approach to improve model's efficiency and convergence speed. In the Mask {SC}, we conduct experiments with several pretrained {CNN} architectures on log-Mel spectrograms, as well as Support Vector Machines on baseline features. For the Breathing {SC}, we propose an ensemble deep learning system that exploits E2E learning and sequence prediction. The E2E model is based on 1D {CNN} operating on raw speech signals and is coupled with Long Short-Term Memory layers for sequence modeling. The second model works with log-Mel features and is based on a pretrained 2D {CNN} model stacked to Gated Recurrent Unit layers. To increase performance of our models in both {SCs}, we use ensembles of the best deep neural models obtained from N-fold cross-validation on combined challenge training and development datasets. Our results markedly outperform the challenge test set baselines in both {SCs}.},
	pages = {2072--2076},
	booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, {INTERSPEECH}},
	publisher = {International Speech Communication Association},
	author = {Markitantov, Maxim and Dresvyanskiy, Denis and Mamontov, Danila and Kaya, Heysem and Minker, Wolfgang and Karpov, Alexey},
	date = {2020},
	note = {{ISSN}: 19909772},
	keywords = {Computational paralinguistics, End-to-end models, Information fusion, Neural networks, Transfer learning},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\NAUKV5CZ\\markitantov20_interspeech.pdf:application/pdf},
}

@article{yu_durian_2019,
	title = {{DurIAN}: Duration Informed Attention Network For Multimodal Synthesis},
	url = {http://arxiv.org/abs/1909.01700},
	abstract = {In this paper, we present a generic and robust multimodal synthesis system that produces highly natural speech and facial expression simultaneously. The key component of this system is the Duration Informed Attention Network ({DurIAN}), an autoregressive model in which the alignments between the input text and the output acoustic features are inferred from a duration model. This is different from the end-to-end attention mechanism used, and accounts for various unavoidable artifacts, in existing end-to-end speech synthesis systems such as Tacotron. Furthermore, {DurIAN} can be used to generate high quality facial expression which can be synchronized with generated speech with/without parallel speech and face data. To improve the efficiency of speech generation, we also propose a multi-band parallel generation strategy on top of the {WaveRNN} model. The proposed Multi-band {WaveRNN} effectively reduces the total computational complexity from 9.8 to 5.5 {GFLOPS}, and is able to generate audio that is 6 times faster than real time on a single {CPU} core. We show that {DurIAN} could generate highly natural speech that is on par with current state of the art end-to-end systems, while at the same time avoid word skipping/repeating errors in those systems. Finally, a simple yet effective approach for fine-grained control of expressiveness of speech and facial expression is introduced.},
	author = {Yu, Chengzhu and Lu, Heng and Hu, Na and Yu, Meng and Weng, Chao and Xu, Kun and Liu, Peng and Tuo, Deyi and Kang, Shiyin and Lei, Guangzhi and Su, Dan and Yu, Dong},
	date = {2019-09-04},
	eprinttype = {arxiv},
	eprint = {1909.01700},
	keywords = {to-read},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\7W3P3G4Z\\durian.pdf:application/pdf},
}

@report{karlapati_prosodic_2020,
	title = {Prosodic representation learning and contextual sampling for neural text-to-speech},
	abstract = {In this paper, we introduce Kathaka, a model trained with a novel two-stage training process for neural speech synthesis with contextually appropriate prosody. In Stage I, we learn a prosodic distribution at the sentence level from mel-spectrograms available during training. In Stage {II}, we propose a novel method to sample from this learnt prosodic distribution using the contextual information available in text. To do this, we use {BERT} on text, and graph-attention networks on parse trees extracted from text. We show a statistically significant relative improvement of 13.2\% in naturalness over a strong baseline when compared to recordings. We also conduct an ablation study on variations of our sampling technique , and show a statistically significant improvement over the baseline in each case.},
	author = {Karlapati, Sri and Abbas, Ammar and Hodari, Zack and Moinet, Alexis and Joly, Arnaud and Karanasou, Penny and Drugman, Thomas},
	date = {2020},
	keywords = {contextual prosody, Index Terms-{TTS}, prosody modelling},
	file = {PDF:C\:\\Users\\nicol\\Zotero\\storage\\VXIDMSU5\\prosodic-representation-learning-and-contextual-sampling-for-neural-text-to-speech.pdf:application/pdf},
}

@misc{lameris_prosody-controllable_2022,
	title = {Prosody-controllable spontaneous {TTS} with neural {HMMs}},
	url = {http://arxiv.org/abs/2211.13533},
	abstract = {Spontaneous speech has many affective and pragmatic functions that are interesting and challenging to model in {TTS} (text-to-speech). However, the presence of reduced articulation, ﬁllers, repetitions, and other disﬂuencies mean that text and acoustics are less well aligned than in read speech. This is problematic for attention-based {TTS}. We propose a {TTS} architecture that is particularly suited for rapidly learning to speak from irregular and small datasets while also reproducing the diversity of expressive phenomena present in spontaneous speech. Speciﬁcally, we modify an existing neural {HMM}-based {TTS} system, which is capable of stable, monotonic alignments for spontaneous speech, and add utterancelevel prosody control, so that the system can represent the wide range of natural variability in a spontaneous speech corpus. We objectively evaluate control accuracy and perform a subjective listening test to compare to a system without prosody control. To exemplify the power of combining midlevel prosody control and ecologically valid data for reproducing intricate spontaneous speech phenomena, we evaluate the system’s capability of synthesizing two types of creaky phonation.},
	number = {{arXiv}:2211.13533},
	publisher = {{arXiv}},
	author = {Lameris, Harm and Mehta, Shivam and Henter, Gustav Eje and Gustafson, Joakim and Székely, Éva},
	urldate = {2023-02-17},
	date = {2022-11-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2211.13533 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Human-Computer Interaction, Machine Learning, Sound},
	file = {Lameris et al. - 2022 - Prosody-controllable spontaneous TTS with neural H.pdf:C\:\\Users\\nicol\\Zotero\\storage\\BA9K9U9W\\Lameris et al. - 2022 - Prosody-controllable spontaneous TTS with neural H.pdf:application/pdf},
}

@misc{raitio_hierarchical_2022,
	title = {Hierarchical prosody modeling and control in non-autoregressive parallel neural {TTS}},
	url = {http://arxiv.org/abs/2110.02952},
	abstract = {Neural text-to-speech ({TTS}) synthesis can generate speech that is indistinguishable from natural speech. However, the synthetic speech often represents the average prosodic style of the database instead of having more versatile prosodic variation. Moreover, many models lack the ability to control the output prosody, which does not allow for different styles for the same text input. In this work, we train a non-autoregressive parallel neural {TTS} front-end model hierarchically conditioned on both coarse and ﬁne-grained acoustic speech features to learn a latent prosody space with intuitive and meaningful dimensions. Experiments show that a non-autoregressive {TTS} model hierarchically conditioned on utterance-wise pitch, pitch range, duration, energy, and spectral tilt can effectively control each prosodic dimension, generate a wide variety of speaking styles, and provide word-wise emphasis control, while maintaining equal or better quality to the baseline model.},
	number = {{arXiv}:2110.02952},
	publisher = {{arXiv}},
	author = {Raitio, Tuomo and Li, Jiangchuan and Seshadri, Shreyas},
	urldate = {2023-02-17},
	date = {2022-03-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2110.02952 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computation and Language, Apple},
	file = {Raitio et al. - 2022 - Hierarchical prosody modeling and control in non-a.pdf:C\:\\Users\\nicol\\Zotero\\storage\\QPU73IEK\\Raitio et al. - 2022 - Hierarchical prosody modeling and control in non-a.pdf:application/pdf},
}
